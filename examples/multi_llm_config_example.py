#!/usr/bin/env python3
"""
å¤šLLMé…ç½®ç¤ºä¾‹
å±•ç¤ºå¦‚ä½•ä¸ºä¸åŒä¸šåŠ¡åŠŸèƒ½é…ç½®ä¸åŒçš„LLMæ¨¡å‹
"""

import asyncio
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from aienhance.core.multi_llm_config import (
    MultiLLMConfigManager,
    LLMModelConfig,
    BusinessFunctionLLMConfig,
    create_config_from_dict,
    DOMAIN_INFERENCE_OPTIMIZED_CONFIG,
    CREATIVE_TASK_CONFIG,
    ANALYTICAL_TASK_CONFIG
)
from aienhance.core.domain_inference import (
    DomainInferenceConfig,
    DomainInferenceManager,
    LLMDomainInferenceProvider
)


# ç¤ºä¾‹é…ç½®å­—å…¸ - å¯ä»¥ä»é…ç½®æ–‡ä»¶åŠ è½½
EXAMPLE_MULTI_LLM_CONFIG = {
    "default": {
        "provider": "ollama",
        "model_name": "qwen3:8b",
        "base_url": "http://localhost:11434",
        "temperature": 0.7,
        "max_tokens": 800,
        "timeout": 30
    },
    "business_functions": {
        "domain_inference": {
            "primary": {
                "provider": "ollama",
                "model_name": "qwen3:8b",
                "base_url": "http://localhost:11434", 
                "temperature": 0.1,  # ä½æ¸©åº¦ç¡®ä¿åˆ†ç±»ä¸€è‡´æ€§
                "max_tokens": 300,   # è¾ƒå°‘tokenç”¨äºåˆ†ç±»
                "timeout": 10,
                "custom_params": {
                    "system_prompt": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é¢†åŸŸåˆ†æä¸“å®¶"
                }
            },
            "fallback": {
                "provider": "ollama", 
                "model_name": "qwen3:4b",  # æ›´å°çš„å¤‡é€‰æ¨¡å‹
                "base_url": "http://localhost:11434",
                "temperature": 0.1,
                "max_tokens": 300,
                "timeout": 8
            },
            "enabled": True,
            "custom_config": {
                "fallback_to_keywords": True,
                "custom_domains": ["technology", "science", "education", "business", "art", "health"]
            }
        },
        "creative_generation": {
            "primary": {
                "provider": "ollama",
                "model_name": "qwen3:32b",  # å¤§æ¨¡å‹ç”¨äºåˆ›æ„ä»»åŠ¡
                "base_url": "http://localhost:11434",
                "temperature": 0.8,  # é«˜æ¸©åº¦å¢åŠ åˆ›é€ æ€§
                "max_tokens": 1200,
                "timeout": 45
            },
            "enabled": True
        },
        "analytical_reasoning": {
            "primary": {
                "provider": "ollama",
                "model_name": "qwen3:14b",  # ä¸­ç­‰æ¨¡å‹ç”¨äºåˆ†æ
                "base_url": "http://localhost:11434", 
                "temperature": 0.3,  # ä¸­ç­‰æ¸©åº¦å¹³è¡¡å‡†ç¡®æ€§å’Œçµæ´»æ€§
                "max_tokens": 1000,
                "timeout": 30
            },
            "enabled": True
        },
        "user_modeling": {
            "primary": {
                "provider": "ollama",
                "model_name": "qwen3:8b",  # é»˜è®¤æ¨¡å‹
                "base_url": "http://localhost:11434",
                "temperature": 0.5,
                "max_tokens": 600,
                "timeout": 20
            },
            "enabled": False  # æš‚æ—¶ç¦ç”¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
        }
    }
}


async def demonstrate_multi_llm_config():
    """æ¼”ç¤ºå¤šLLMé…ç½®ç®¡ç†"""
    print("ğŸ”§ å¤šLLMé…ç½®ç®¡ç†æ¼”ç¤º")
    print("=" * 50)
    
    # ä»å­—å…¸åˆ›å»ºé…ç½®ç®¡ç†å™¨
    config_manager = create_config_from_dict(EXAMPLE_MULTI_LLM_CONFIG)
    
    print("\n1ï¸âƒ£ é…ç½®æ¦‚è§ˆ:")
    print(f"   å·²é…ç½®ä¸šåŠ¡åŠŸèƒ½: {config_manager.list_configured_functions()}")
    
    # å±•ç¤ºä¸åŒä¸šåŠ¡åŠŸèƒ½çš„é…ç½®
    functions_to_test = ["domain_inference", "creative_generation", "analytical_reasoning", "user_modeling"]
    
    print("\n2ï¸âƒ£ ä¸šåŠ¡åŠŸèƒ½LLMé…ç½®:")
    for func_name in functions_to_test:
        config = config_manager.get_config_for_function(func_name)
        fallback = config_manager.get_fallback_config_for_function(func_name)
        
        print(f"\n   ğŸ“Œ {func_name}:")
        if config:
            print(f"      ä¸»è¦æ¨¡å‹: {config.provider}/{config.model_name}")
            print(f"      æ¸©åº¦: {config.temperature}, æœ€å¤§token: {config.max_tokens}")
        if fallback:
            print(f"      å¤‡é€‰æ¨¡å‹: {fallback.provider}/{fallback.model_name}")
        if not config and not fallback:
            print("      ä½¿ç”¨é»˜è®¤é…ç½®")
    
    return config_manager


async def test_domain_inference_with_config(config_manager: MultiLLMConfigManager):
    """æµ‹è¯•ä½¿ç”¨é…ç½®çš„é¢†åŸŸæ¨æ–­åŠŸèƒ½"""
    print("\n" + "=" * 50)
    print("ğŸ§ª é¢†åŸŸæ¨æ–­åŠŸèƒ½æµ‹è¯•")
    print("=" * 50)
    
    # è·å–é¢†åŸŸæ¨æ–­çš„é…ç½®
    domain_config = config_manager.get_config_for_function("domain_inference")
    if not domain_config:
        print("   âŒ æœªæ‰¾åˆ°é¢†åŸŸæ¨æ–­é…ç½®")
        return
    
    # æ¨¡æ‹ŸLLMæä¾›å•† (åœ¨å®é™…ä½¿ç”¨ä¸­è¿™åº”è¯¥æ˜¯çœŸæ­£çš„LLMå®ä¾‹)
    class MockLLMProvider:
        def __init__(self, config: LLMModelConfig):
            self.config = config
        
        async def generate_async(self, messages, **kwargs):
            # æ¨¡æ‹ŸLLMå“åº”
            query_content = messages[0]["content"]
            if "ç¼–ç¨‹" in query_content or "AI" in query_content:
                return {
                    "content": '''
                    {
                        "primary_domains": ["technology"],
                        "secondary_domains": ["education"],
                        "confidence_scores": {"technology": 0.9, "education": 0.6},
                        "interdisciplinary": true,
                        "reasoning": "æŸ¥è¯¢æ¶‰åŠç¼–ç¨‹å’ŒAIæŠ€æœ¯ï¼Œä¸»è¦å±äºæŠ€æœ¯é¢†åŸŸï¼ŒåŒæ—¶æ¶‰åŠå­¦ä¹ ç›¸å…³å†…å®¹ã€‚"
                    }
                    '''
                }
            else:
                return {
                    "content": '''
                    {
                        "primary_domains": ["general"],
                        "secondary_domains": [],
                        "confidence_scores": {"general": 0.7},
                        "interdisciplinary": false,
                        "reasoning": "æŸ¥è¯¢å†…å®¹è¾ƒä¸ºé€šç”¨ï¼Œæœªæ£€æµ‹åˆ°ç‰¹å®šä¸“ä¸šé¢†åŸŸç‰¹å¾ã€‚"
                    }
                    '''
                }
    
    try:
        # åˆ›å»ºæ¨¡æ‹Ÿçš„LLMæä¾›å•†
        mock_llm = MockLLMProvider(domain_config)
        
        # åˆ›å»ºé¢†åŸŸæ¨æ–­é…ç½®
        inference_config = DomainInferenceConfig(
            llm_provider=mock_llm,
            model_name=domain_config.model_name,
            temperature=domain_config.temperature,
            max_tokens=domain_config.max_tokens,
            timeout=domain_config.timeout
        )
        
        # åˆ›å»ºé¢†åŸŸæ¨æ–­æä¾›å•†
        provider = LLMDomainInferenceProvider(inference_config)
        await provider.initialize()
        
        # æµ‹è¯•æŸ¥è¯¢
        test_queries = [
            "å¦‚ä½•å­¦ä¹ Pythonç¼–ç¨‹å’Œæœºå™¨å­¦ä¹ ï¼Ÿ",
            "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ",
            "å¦‚ä½•è®¾è®¡ä¸€ä¸ªé«˜æ•ˆçš„ç®—æ³•ï¼Ÿ"
        ]
        
        print(f"\n   ä½¿ç”¨æ¨¡å‹: {domain_config.provider}/{domain_config.model_name}")
        print(f"   æ¸©åº¦: {domain_config.temperature}, æœ€å¤§token: {domain_config.max_tokens}")
        
        for i, query in enumerate(test_queries, 1):
            print(f"\n   æµ‹è¯•{i}: {query}")
            try:
                result = await provider.infer_domains(query)
                print(f"   ç»“æœ: ä¸»è¦é¢†åŸŸ = {result.primary_domains}")
                print(f"         æ¬¡è¦é¢†åŸŸ = {result.secondary_domains}")
                print(f"         è·¨å­¦ç§‘ = {result.interdisciplinary}")
                print(f"         æ¨ç† = {result.reasoning}")
            except Exception as e:
                print(f"   âŒ æ¨æ–­å¤±è´¥: {e}")
        
        await provider.cleanup()
        
    except Exception as e:
        print(f"   âŒ æµ‹è¯•å¤±è´¥: {e}")


async def demonstrate_configuration_flexibility():
    """æ¼”ç¤ºé…ç½®çš„çµæ´»æ€§"""
    print("\n" + "=" * 50)
    print("âš™ï¸  é…ç½®çµæ´»æ€§æ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºé…ç½®ç®¡ç†å™¨
    manager = MultiLLMConfigManager()
    
    # åŠ¨æ€æ·»åŠ é…ç½®
    print("\n1ï¸âƒ£ åŠ¨æ€é…ç½®ç®¡ç†:")
    
    # è®¾ç½®é»˜è®¤é…ç½®
    default_config = LLMModelConfig(
        provider="ollama",
        model_name="qwen3:8b",
        base_url="http://localhost:11434",
        temperature=0.7,
        max_tokens=800
    )
    manager.set_default_config(default_config)
    print("   âœ… è®¾ç½®é»˜è®¤é…ç½®")
    
    # æ·»åŠ ä¸“é—¨çš„ä¸šåŠ¡åŠŸèƒ½é…ç½®
    specialized_configs = [
        BusinessFunctionLLMConfig(
            function_name="sentiment_analysis",
            primary_model=LLMModelConfig(
                provider="ollama",
                model_name="qwen3:4b",
                temperature=0.2,  # ä½æ¸©åº¦ç”¨äºæƒ…æ„Ÿåˆ†æ
                max_tokens=200
            )
        ),
        BusinessFunctionLLMConfig(
            function_name="text_generation", 
            primary_model=LLMModelConfig(
                provider="ollama",
                model_name="qwen3:32b",
                temperature=0.9,  # é«˜æ¸©åº¦ç”¨äºæ–‡æœ¬ç”Ÿæˆ
                max_tokens=1500
            ),
            fallback_model=LLMModelConfig(
                provider="ollama", 
                model_name="qwen3:14b",
                temperature=0.9,
                max_tokens=1200
            )
        )
    ]
    
    for config in specialized_configs:
        manager.register_business_function(config)
        print(f"   âœ… æ³¨å†Œä¸šåŠ¡åŠŸèƒ½: {config.function_name}")
    
    print(f"\n2ï¸âƒ£ é…ç½®çŠ¶æ€:")
    print(f"   å·²é…ç½®åŠŸèƒ½: {manager.list_configured_functions()}")
    
    # æ¼”ç¤ºé…ç½®è·å–
    print(f"\n3ï¸âƒ£ é…ç½®è·å–æ¼”ç¤º:")
    for func_name in ["sentiment_analysis", "text_generation", "undefined_function"]:
        config = manager.get_config_for_function(func_name)
        if config:
            print(f"   {func_name}: {config.provider}/{config.model_name} (T={config.temperature})")
        else:
            print(f"   {func_name}: ä½¿ç”¨é»˜è®¤é…ç½®")
    
    # æ¼”ç¤ºåŠŸèƒ½å¯ç”¨/ç¦ç”¨
    print(f"\n4ï¸âƒ£ åŠŸèƒ½æ§åˆ¶æ¼”ç¤º:")
    print(f"   ç¦ç”¨sentiment_analysis: {manager.disable_function('sentiment_analysis')}")
    print(f"   å½“å‰å¯ç”¨åŠŸèƒ½: {manager.list_configured_functions()}")
    print(f"   é‡æ–°å¯ç”¨sentiment_analysis: {manager.enable_function('sentiment_analysis')}")
    print(f"   å½“å‰å¯ç”¨åŠŸèƒ½: {manager.list_configured_functions()}")


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¤šæ¨¡å‹ååŒè°ƒåº¦é…ç½®ç³»ç»Ÿæ¼”ç¤º")
    print("=" * 70)
    
    # åŸºæœ¬é…ç½®æ¼”ç¤º
    config_manager = await demonstrate_multi_llm_config()
    
    # é¢†åŸŸæ¨æ–­æµ‹è¯•
    await test_domain_inference_with_config(config_manager)
    
    # é…ç½®çµæ´»æ€§æ¼”ç¤º
    await demonstrate_configuration_flexibility()
    
    print("\n" + "=" * 70)
    print("âœ¨ æ¼”ç¤ºå®Œæˆ!")
    print("ğŸ’¡ è¿™ä¸ªç³»ç»Ÿæ”¯æŒ:")
    print("   â€¢ ä¸ºæ¯ä¸ªä¸šåŠ¡åŠŸèƒ½é…ç½®ç‹¬ç«‹çš„LLMæ¨¡å‹")
    print("   â€¢ ä¸»è¦æ¨¡å‹å’Œå¤‡é€‰æ¨¡å‹é…ç½®")
    print("   â€¢ è¿è¡Œæ—¶åŠ¨æ€å¯ç”¨/ç¦ç”¨åŠŸèƒ½") 
    print("   â€¢ çµæ´»çš„å‚æ•°é…ç½® (æ¸©åº¦ã€tokené™åˆ¶ç­‰)")
    print("   â€¢ ä»é…ç½®æ–‡ä»¶æˆ–å­—å…¸åŠ è½½é…ç½®")
    print("=" * 70)


if __name__ == "__main__":
    asyncio.run(main())