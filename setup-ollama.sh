#!/bin/bash

# Ollamaæ¨¡å‹å®‰è£…è„šæœ¬
# ä¸ºAiEnhanceç³»ç»Ÿå®‰è£…æ¨èçš„Ollamaæ¨¡å‹

set -e

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# æ—¥å¿—å‡½æ•°
log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# æ£€æŸ¥Ollamaæ˜¯å¦å®‰è£…
check_ollama_installed() {
    if ! command -v ollama &> /dev/null; then
        log_error "Ollamaæœªå®‰è£…ï¼"
        log_info "è¯·å…ˆå®‰è£…Ollamaï¼š"
        log_info "  macOS: brew install ollama"
        log_info "  Linux: curl -fsSL https://ollama.ai/install.sh | sh"
        log_info "  Windows: ä¸‹è½½ https://ollama.ai/download/windows"
        exit 1
    fi
    log_success "Ollamaå·²å®‰è£…"
}

# æ£€æŸ¥OllamaæœåŠ¡æ˜¯å¦è¿è¡Œ
check_ollama_running() {
    if ! curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
        log_error "OllamaæœåŠ¡æœªè¿è¡Œï¼"
        log_info "è¯·åœ¨å¦ä¸€ä¸ªç»ˆç«¯å¯åŠ¨OllamaæœåŠ¡ï¼š"
        log_info "  ollama serve"
        exit 1
    fi
    log_success "OllamaæœåŠ¡è¿è¡Œæ­£å¸¸"
}

# æ‹‰å–æ¨¡å‹
pull_model() {
    local model_name=$1
    local description=$2
    
    log_info "æ­£åœ¨æ‹‰å– $model_name ($description)..."
    
    if ollama list | grep -q "$model_name"; then
        log_warning "$model_name å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½"
        return 0
    fi
    
    if ollama pull "$model_name"; then
        log_success "$model_name æ‹‰å–æˆåŠŸ"
    else
        log_error "$model_name æ‹‰å–å¤±è´¥"
        return 1
    fi
}

# éªŒè¯æ¨¡å‹
verify_model() {
    local model_name=$1
    
    log_info "éªŒè¯æ¨¡å‹ $model_name..."
    
    if ollama list | grep -q "$model_name"; then
        log_success "$model_name éªŒè¯æˆåŠŸ"
        return 0
    else
        log_error "$model_name éªŒè¯å¤±è´¥"
        return 1
    fi
}

# ä¸»å‡½æ•°
main() {
    echo "=========================================="
    echo "ğŸš€ AiEnhance Ollamaæ¨¡å‹å®‰è£…å™¨"
    echo "=========================================="
    
    # æ£€æŸ¥ç¯å¢ƒ
    check_ollama_installed
    check_ollama_running
    
    # æ¨èæ¨¡å‹åˆ—è¡¨
    declare -A models=(
        ["qwen3:8b"]="é€šä¹‰åƒé—®3.0ï¼Œæœ€æ–°ä¸€ä»£ä¸­æ–‡å¤§è¯­è¨€æ¨¡å‹ï¼Œ8Bå‚æ•°"
        ["bge-m3"]="å¤šè¯­è¨€åµŒå…¥æ¨¡å‹ï¼Œæ”¯æŒä¸­è‹±æ–‡ï¼Œå¤šåŠŸèƒ½å¤šç²’åº¦"
    )
    
    # æ‹‰å–æ¨¡å‹
    failed_models=()
    
    for model in "${!models[@]}"; do
        if ! pull_model "$model" "${models[$model]}"; then
            failed_models+=("$model")
        fi
    done
    
    echo ""
    echo "=========================================="
    echo "ğŸ“Š å®‰è£…ç»“æœ"
    echo "=========================================="
    
    # éªŒè¯å·²å®‰è£…çš„æ¨¡å‹
    log_info "å·²å®‰è£…çš„æ¨¡å‹ï¼š"
    ollama list
    
    echo ""
    
    # éªŒè¯æ ¸å¿ƒæ¨¡å‹
    success_count=0
    total_count=${#models[@]}
    
    for model in "${!models[@]}"; do
        if verify_model "$model"; then
            ((success_count++))
        fi
    done
    
    echo ""
    echo "=========================================="
    echo "ğŸ¯ æ€»ç»“"
    echo "=========================================="
    
    if [ $success_count -eq $total_count ]; then
        log_success "æ‰€æœ‰æ¨èæ¨¡å‹å®‰è£…æˆåŠŸï¼($success_count/$total_count)"
        echo ""
        log_info "ç°åœ¨å¯ä»¥å¯åŠ¨AiEnhanceç³»ç»Ÿï¼š"
        log_info "  ./docker-start.sh"
    else
        log_warning "éƒ¨åˆ†æ¨¡å‹å®‰è£…å¤±è´¥ ($success_count/$total_count)"
        if [ ${#failed_models[@]} -gt 0 ]; then
            log_info "å¤±è´¥çš„æ¨¡å‹ï¼š"
            for model in "${failed_models[@]}"; do
                log_info "  - $model"
            done
        fi
        echo ""
        log_info "ä½ ä»ç„¶å¯ä»¥å¯åŠ¨ç³»ç»Ÿï¼Œä½†æŸäº›åŠŸèƒ½å¯èƒ½å—é™"
    fi
    
    echo ""
    log_info "æ¨¡å‹å­˜å‚¨ä½ç½®ï¼š"
    log_info "  macOS: ~/.ollama/models"
    log_info "  Linux: /usr/share/ollama/.ollama/models"
    log_info "  Windows: %USERPROFILE%\\.ollama\\models"
}

# è¿è¡Œä¸»å‡½æ•°
main "$@"