#!/usr/bin/env python3
"""
åä½œå±‚æµ‹è¯•è„šæœ¬
æµ‹è¯•è¾©è¯è§†è§’ç”Ÿæˆã€è®¤çŸ¥æŒ‘æˆ˜ç­‰åŠŸèƒ½
"""

import asyncio
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from aienhance.collaboration import (
    ChallengeRequest,
    ChallengeType,
    CognitiveChallenge,
    CollaborationContext,
    CollaborativeCoordinator,
    DialecticalPerspectiveGenerator,
    PerspectiveRequest,
    PerspectiveType,
)
from aienhance.llm import LLMProviderFactory
from aienhance.llm.interfaces import ModelConfig, ModelType


async def test_dialectical_perspective():
    """æµ‹è¯•è¾©è¯è§†è§’ç”Ÿæˆ"""
    print("=" * 60)
    print("ğŸ­ æµ‹è¯•è¾©è¯è§†è§’ç”Ÿæˆ")
    print("=" * 60)

    try:
        # åˆ›å»ºLLMæä¾›å•†ï¼ˆä½¿ç”¨ollamaä½œä¸ºæµ‹è¯•ï¼‰
        llm_config = ModelConfig(
            provider="ollama",
            model_name="qwen3:8b",
            model_type=ModelType.CHAT,
            api_base="http://localhost:11434",
            temperature=0.7
        )

        llm_provider = LLMProviderFactory.create_provider(llm_config)
        await llm_provider.initialize()

        # åˆ›å»ºè§†è§’ç”Ÿæˆå™¨
        perspective_generator = DialecticalPerspectiveGenerator(llm_provider)

        # æµ‹è¯•å†…å®¹
        test_content = "äººå·¥æ™ºèƒ½å°†ä¼šå®Œå…¨æ›¿ä»£äººç±»çš„å·¥ä½œï¼Œè¿™æ˜¯æŠ€æœ¯å‘å±•çš„å¿…ç„¶è¶‹åŠ¿ã€‚"

        # åˆ›å»ºåä½œä¸Šä¸‹æ–‡
        context = CollaborationContext(
            user_id="test_user",
            session_id="test_session"
        )

        # åˆ›å»ºè§†è§’è¯·æ±‚
        request = PerspectiveRequest(
            content=test_content,
            user_position="æ”¯æŒAIæ›¿ä»£äººç±»å·¥ä½œ",
            perspective_types=[PerspectiveType.OPPOSING, PerspectiveType.MULTI_DISCIPLINARY],
            max_perspectives=3
        )

        # ç”Ÿæˆè§†è§’
        print(f"ğŸ“ åˆ†æå†…å®¹: {test_content}")
        print("ğŸ”„ ç”Ÿæˆå¤šå…ƒè§†è§’...")

        result = await perspective_generator.generate_perspectives(request, context)

        print(f"\nğŸ¯ ç”Ÿæˆäº† {len(result.perspectives)} ä¸ªè§†è§’:")

        for i, perspective in enumerate(result.perspectives, 1):
            print(f"\n{i}. {perspective.title}")
            print(f"   ç±»å‹: {perspective.perspective_type.value}")
            print(f"   æè¿°: {perspective.description}")
            print("   å…³é”®è®ºæ®:")
            for j, arg in enumerate(perspective.key_arguments, 1):
                print(f"     {j}) {arg}")
            print(f"   ç›¸å…³æ€§è¯„åˆ†: {perspective.relevance_score:.2f}")

        if result.synthesis:
            print("\nğŸ”„ ç»¼åˆåˆ†æ:")
            print(result.synthesis)

        if result.dialectical_tensions:
            print("\nâš¡ è¾©è¯å†²çªç‚¹:")
            for tension in result.dialectical_tensions:
                print(f"   â€¢ {tension}")

        print("âœ… è¾©è¯è§†è§’ç”Ÿæˆæµ‹è¯•å®Œæˆ")
        return True

    except Exception as e:
        print(f"âŒ è¾©è¯è§†è§’ç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")
        return False


async def test_cognitive_challenge():
    """æµ‹è¯•è®¤çŸ¥æŒ‘æˆ˜"""
    print("\n" + "=" * 60)
    print("ğŸ§  æµ‹è¯•è®¤çŸ¥æŒ‘æˆ˜")
    print("=" * 60)

    try:
        # åˆ›å»ºLLMæä¾›å•†
        llm_config = ModelConfig(
            provider="ollama",
            model_name="qwen3:8b",
            model_type=ModelType.CHAT,
            api_base="http://localhost:11434",
            temperature=0.7
        )

        llm_provider = LLMProviderFactory.create_provider(llm_config)
        await llm_provider.initialize()

        # åˆ›å»ºè®¤çŸ¥æŒ‘æˆ˜å™¨
        cognitive_challenger = CognitiveChallenge(llm_provider)

        # æµ‹è¯•å†…å®¹
        test_content = "è¿œç¨‹å·¥ä½œæé«˜äº†å‘˜å·¥çš„å·¥ä½œæ•ˆç‡ï¼Œåº”è¯¥æˆä¸ºæ‰€æœ‰å…¬å¸çš„æ ‡å‡†åšæ³•ã€‚"
        user_reasoning = "å› ä¸ºå‘˜å·¥å¯ä»¥é¿å…é€šå‹¤æ—¶é—´ï¼Œæœ‰æ›´å¥½çš„å·¥ä½œç¯å¢ƒï¼Œæ‰€ä»¥æ•ˆç‡æ›´é«˜ã€‚"

        # åˆ›å»ºåä½œä¸Šä¸‹æ–‡
        context = CollaborationContext(
            user_id="test_user",
            session_id="test_session"
        )

        # åˆ›å»ºæŒ‘æˆ˜è¯·æ±‚
        request = ChallengeRequest(
            content=test_content,
            user_reasoning=user_reasoning,
            challenge_types=[
                ChallengeType.ASSUMPTION_QUESTIONING,
                ChallengeType.BLIND_SPOT_DETECTION,
                ChallengeType.COMPLEXITY_EXPANSION
            ],
            intensity_level="moderate"
        )

        # ç”ŸæˆæŒ‘æˆ˜
        print(f"ğŸ“ åˆ†æå†…å®¹: {test_content}")
        print(f"ğŸ¤” ç”¨æˆ·æ¨ç†: {user_reasoning}")
        print("ğŸ”„ ç”Ÿæˆè®¤çŸ¥æŒ‘æˆ˜...")

        result = await cognitive_challenger.generate_challenges(request, context)

        print(f"\nğŸ¯ ç”Ÿæˆäº† {len(result.challenges)} ä¸ªè®¤çŸ¥æŒ‘æˆ˜:")

        for i, challenge in enumerate(result.challenges, 1):
            print(f"\n{i}. {challenge.title}")
            print(f"   ç±»å‹: {challenge.challenge_type.value}")
            print(f"   æè¿°: {challenge.description}")
            print("   æŒ‘æˆ˜æ€§é—®é¢˜:")
            for j, question in enumerate(challenge.questions, 1):
                print(f"     {j}) {question}")

            if challenge.alternative_frameworks:
                print(f"   æ›¿ä»£æ¡†æ¶: {', '.join(challenge.alternative_frameworks)}")

        if result.meta_reflection:
            print("\nğŸ¤¯ å…ƒè®¤çŸ¥åæ€:")
            print(result.meta_reflection)

        if result.growth_opportunities:
            print("\nğŸŒ± æˆé•¿æœºä¼š:")
            for opportunity in result.growth_opportunities:
                print(f"   â€¢ {opportunity}")

        print("âœ… è®¤çŸ¥æŒ‘æˆ˜æµ‹è¯•å®Œæˆ")
        return True

    except Exception as e:
        print(f"âŒ è®¤çŸ¥æŒ‘æˆ˜æµ‹è¯•å¤±è´¥: {e}")
        return False


async def test_collaborative_coordinator():
    """æµ‹è¯•åä½œåè°ƒå™¨"""
    print("\n" + "=" * 60)
    print("ğŸ¤ æµ‹è¯•åä½œåè°ƒå™¨")
    print("=" * 60)

    try:
        # åˆ›å»ºLLMæä¾›å•†
        llm_config = ModelConfig(
            provider="ollama",
            model_name="qwen3:8b",
            model_type=ModelType.CHAT,
            api_base="http://localhost:11434",
            temperature=0.7
        )

        llm_provider = LLMProviderFactory.create_provider(llm_config)
        await llm_provider.initialize()

        # åˆ›å»ºåä½œåè°ƒå™¨
        coordinator = CollaborativeCoordinator(llm_provider)

        # æµ‹è¯•å†…å®¹
        test_content = "åŒºå—é“¾æŠ€æœ¯å°†å½»åº•æ”¹å˜é‡‘èè¡Œä¸šï¼Œä¼ ç»Ÿé“¶è¡Œå°†å¤±å»å­˜åœ¨çš„æ„ä¹‰ã€‚"

        # åˆ›å»ºåä½œä¸Šä¸‹æ–‡
        context = CollaborationContext(
            user_id="test_user",
            session_id="test_session",
            collaboration_preferences={
                "enable_perspectives": True,
                "enable_challenges": True,
                "challenge_intensity": "moderate"
            }
        )

        # ç¼–æ’åä½œè¿‡ç¨‹
        print(f"ğŸ“ åˆ†æå†…å®¹: {test_content}")
        print("ğŸ”„ ç¼–æ’åä½œè¿‡ç¨‹...")

        result = await coordinator.orchestrate_collaboration(test_content, context)

        print("\nğŸ¯ åä½œç»“æœ:")
        print(f"   æ—¶é—´æˆ³: {result.get('timestamp')}")
        print(f"   ç”¨æˆ·ID: {result.get('user_id')}")

        # æ˜¾ç¤ºè§†è§’ç»“æœ
        perspectives = result.get('perspectives')
        if perspectives and not perspectives.get('error'):
            print(f"\nğŸ­ ç”Ÿæˆè§†è§’: {len(perspectives.get('perspectives', []))} ä¸ª")
            for i, p in enumerate(perspectives.get('perspectives', []), 1):
                print(f"   {i}. {p.get('title', 'N/A')}")

        # æ˜¾ç¤ºæŒ‘æˆ˜ç»“æœ
        challenges = result.get('challenges')
        if challenges and not challenges.get('error'):
            print(f"\nğŸ§  è®¤çŸ¥æŒ‘æˆ˜: {len(challenges.get('challenges', []))} ä¸ª")
            for i, c in enumerate(challenges.get('challenges', []), 1):
                print(f"   {i}. {c.get('title', 'N/A')}")

        # æ˜¾ç¤ºåä½œæ´å¯Ÿ
        insights = result.get('collaboration_insights')
        if insights:
            print("\nğŸ’¡ åä½œæ´å¯Ÿ:")
            print(f"   åä½œæ•ˆæœ: {insights.get('collaboration_effectiveness', 'N/A')}")
            print(f"   å­¦ä¹ æœºä¼š: {insights.get('learning_opportunities', [])}")
            print(f"   ä¸ªæ€§åŒ–å»ºè®®: {insights.get('personalized_recommendations', [])}")

        # æ˜¾ç¤ºä¸‹ä¸€æ­¥å»ºè®®
        next_steps = result.get('next_collaboration_steps')
        if next_steps:
            print("\nğŸš€ ä¸‹ä¸€æ­¥å»ºè®®:")
            for step in next_steps:
                print(f"   â€¢ {step}")

        print("âœ… åä½œåè°ƒå™¨æµ‹è¯•å®Œæˆ")
        return True

    except Exception as e:
        print(f"âŒ åä½œåè°ƒå™¨æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ åä½œå±‚åŠŸèƒ½æµ‹è¯•å¼€å§‹")
    print("=" * 60)

    results = []

    # æ£€æŸ¥Ollamaæ˜¯å¦å¯ç”¨
    try:
        import httpx
        async with httpx.AsyncClient() as client:
            response = await client.get("http://localhost:11434/api/tags", timeout=5.0)
            if response.status_code != 200:
                print("âŒ OllamaæœåŠ¡ä¸å¯ç”¨ï¼Œè¯·å…ˆå¯åŠ¨Ollama")
                return
    except Exception as e:
        print(f"âŒ æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡: {e}")
        print("è¯·ç¡®ä¿Ollamaæ­£åœ¨è¿è¡Œ: docker compose up -d ollama")
        return

    # æ‰§è¡Œæµ‹è¯•
    tests = [
        ("è¾©è¯è§†è§’ç”Ÿæˆ", test_dialectical_perspective),
        ("è®¤çŸ¥æŒ‘æˆ˜", test_cognitive_challenge),
        ("åä½œåè°ƒå™¨", test_collaborative_coordinator)
    ]

    for test_name, test_func in tests:
        try:
            result = await test_func()
            results.append((test_name, result))
        except KeyboardInterrupt:
            print("\nâ¹ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
            break
        except Exception as e:
            print(f"âŒ {test_name}æµ‹è¯•å‡ºç°å¼‚å¸¸: {e}")
            results.append((test_name, False))

    # æ˜¾ç¤ºæµ‹è¯•ç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»")
    print("=" * 60)

    passed = 0
    total = len(results)

    for test_name, result in results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"{test_name}: {status}")
        if result:
            passed += 1

    print(f"\næ€»è®¡: {passed}/{total} æµ‹è¯•é€šè¿‡")

    if passed == total:
        print("ğŸ‰ æ‰€æœ‰åä½œå±‚åŠŸèƒ½æµ‹è¯•é€šè¿‡ï¼")
        print("\nğŸ’¡ åä½œå±‚å·²æˆåŠŸå®ç°ï¼ŒåŒ…æ‹¬:")
        print("   â€¢ è¾©è¯è§†è§’ç”Ÿæˆ (å¤šå­¦ç§‘è§†è§’ã€å¯¹ç«‹è§‚ç‚¹)")
        print("   â€¢ è®¤çŸ¥æŒ‘æˆ˜ (å‡è®¾è´¨ç–‘ã€ç›²ç‚¹æ£€æµ‹ã€å¤æ‚æ€§æ‰©å±•)")
        print("   â€¢ åä½œåè°ƒ (æ•´ä½“ç¼–æ’ã€ç”¨æˆ·å»ºæ¨¡ã€é€‚åº”æ€§è°ƒæ•´)")
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥å®ç°")


if __name__ == "__main__":
    asyncio.run(main())
