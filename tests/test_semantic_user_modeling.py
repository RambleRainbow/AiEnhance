#!/usr/bin/env python3
"""
æµ‹è¯•æ–°çš„åŸºäºLLMçš„è¯­ä¹‰ç”¨æˆ·å»ºæ¨¡åŠŸèƒ½
"""

import asyncio
import logging
import os
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.abspath("."))

from aienhance.enhanced_system_factory import create_layered_system
from aienhance.llm.interfaces import create_model_config
from aienhance.memory.interfaces import MemorySystemConfig

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


async def test_semantic_user_modeling():
    """æµ‹è¯•è¯­ä¹‰ç”¨æˆ·å»ºæ¨¡åŠŸèƒ½"""

    print("ğŸš€ å¯åŠ¨è¯­ä¹‰ç”¨æˆ·å»ºæ¨¡æµ‹è¯•")
    print("=" * 60)

    try:
        # 1. åˆ›å»ºç³»ç»Ÿé…ç½®
        print("1. åˆ›å»ºç³»ç»Ÿé…ç½®...")

        # LLMé…ç½® - ä½¿ç”¨æœ¬åœ°Ollama
        llm_config = create_model_config(
            provider="ollama",
            model_name="qwen3:8b",
            api_base="http://localhost:11434",
            temperature=0.3,
            max_tokens=800,
        )

        # è®°å¿†ç³»ç»Ÿé…ç½® - ä½¿ç”¨MIRIXç»Ÿä¸€æ¨¡å¼
        memory_config = MemorySystemConfig(system_type="mirix_unified")

        # åˆ›å»ºåˆ†å±‚è®¤çŸ¥ç³»ç»Ÿ
        print("2. åˆ›å»ºåˆ†å±‚è®¤çŸ¥ç³»ç»Ÿï¼ˆä½¿ç”¨è¯­ä¹‰ç”¨æˆ·å»ºæ¨¡ï¼‰...")
        system = create_layered_system(
            system_type="educational",
            memory_config=memory_config,
            llm_config=llm_config,
        )

        # åˆå§‹åŒ–ç³»ç»Ÿ
        print("3. åˆå§‹åŒ–ç³»ç»Ÿ...")
        success = await system.initialize_layers()
        if not success:
            print("âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥")
            return

        print("âœ… ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")

        # 4. å‡†å¤‡æµ‹è¯•æ•°æ®
        test_queries = [
            {
                "user_id": "test_user_1",
                "query": "è¯·è¯¦ç»†è§£é‡Šæ·±åº¦å­¦ä¹ ä¸­çš„æ³¨æ„åŠ›æœºåˆ¶åŸç†ï¼Œæˆ‘æƒ³ä»æ•°å­¦åŸç†åˆ°å®é™…åº”ç”¨éƒ½äº†è§£æ¸…æ¥šã€‚",
                "context": {"domain": "technical", "urgency": "medium"},
                "description": "æŠ€æœ¯æ·±åº¦å­¦ä¹ æŸ¥è¯¢",
            },
            {
                "user_id": "test_user_2",
                "query": "èƒ½å¦ç®€å•è¯´æ˜ä¸€ä¸‹ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿæˆ‘æ˜¯åˆå­¦è€…ã€‚",
                "context": {"domain": "educational", "urgency": "low"},
                "description": "åˆå­¦è€…æ•™è‚²æŸ¥è¯¢",
            },
            {
                "user_id": "test_user_3",
                "query": "æˆ‘æƒ³è¦è®¾è®¡ä¸€ä¸ªåˆ›æ–°çš„ç”¨æˆ·ç•Œé¢ï¼Œèƒ½å¸®æˆ‘brainstormä¸€äº›å‰æ²¿çš„äº¤äº’æ¦‚å¿µå—ï¼Ÿ",
                "context": {"domain": "creative", "urgency": "high"},
                "description": "åˆ›æ„è®¾è®¡æŸ¥è¯¢",
            },
        ]

        print("\n4. æ‰§è¡Œè¯­ä¹‰ç”¨æˆ·å»ºæ¨¡æµ‹è¯•...")
        print("-" * 60)

        for i, test_case in enumerate(test_queries, 1):
            print(f"\næµ‹è¯•æ¡ˆä¾‹ {i}: {test_case['description']}")
            print(f"æŸ¥è¯¢: {test_case['query'][:50]}...")

            try:
                # å¤„ç†æŸ¥è¯¢å¹¶ç”Ÿæˆç”¨æˆ·ç”»åƒ
                response = await system.process_through_layers(
                    query=test_case["query"],
                    user_id=test_case["user_id"],
                    context=test_case["context"],
                )

                if response.status == "success":
                    # åˆ†æç”Ÿæˆçš„ç”¨æˆ·ç”»åƒ
                    user_profile = response.perception_output.user_profile

                    print("âœ… ç”¨æˆ·ç”»åƒç”ŸæˆæˆåŠŸ:")
                    print(f"   è®¤çŸ¥ç‰¹å¾: {user_profile.cognitive_characteristics}")
                    print(f"   çŸ¥è¯†é¢†åŸŸ: {user_profile.knowledge_profile}")
                    print(f"   äº¤äº’åå¥½: {user_profile.interaction_preferences}")

                    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†è¯­ä¹‰åˆ†æ
                    perception_meta = response.perception_output.metadata
                    if "è¯­ä¹‰åˆ†æ" in str(perception_meta):
                        print("   ğŸ§  ä½¿ç”¨äº†LLMè¯­ä¹‰åˆ†æ")
                    else:
                        print("   ğŸ“ ä½¿ç”¨äº†ä¼ ç»Ÿè§„åˆ™åˆ†æ")

                    # æ˜¾ç¤ºæœ€ç»ˆå“åº”
                    print(f"   æœ€ç»ˆå“åº”: {response.final_response[:100]}...")

                else:
                    print(f"âŒ å¤„ç†å¤±è´¥: {response.error_message}")

            except Exception as e:
                print(f"âŒ æµ‹è¯•æ¡ˆä¾‹æ‰§è¡Œå¤±è´¥: {e}")

        # 5. æµ‹è¯•ç”¨æˆ·ç”»åƒæ›´æ–°åŠŸèƒ½
        print(f"\n5. æµ‹è¯•ç”¨æˆ·ç”»åƒå¢é‡æ›´æ–°...")
        print("-" * 60)

        try:
            # ä¸ºç¬¬ä¸€ä¸ªç”¨æˆ·æ·»åŠ æ›´å¤šäº¤äº’æ•°æ®
            update_query = (
                "ç°åœ¨æˆ‘æƒ³äº†è§£transformeræ¶æ„çš„å…·ä½“å®ç°ç»†èŠ‚ï¼ŒåŒ…æ‹¬å¤šå¤´æ³¨æ„åŠ›çš„è®¡ç®—è¿‡ç¨‹ã€‚"
            )

            response = await system.process_through_layers(
                query=update_query,
                user_id="test_user_1",
                context={"domain": "technical", "follow_up": True},
            )

            if response.status == "success":
                updated_profile = response.perception_output.user_profile
                print("âœ… ç”¨æˆ·ç”»åƒæ›´æ–°æˆåŠŸ")
                print(
                    f"   æ›´æ–°åè®¤çŸ¥å¤æ‚åº¦: {updated_profile.cognitive_characteristics.get('cognitive_complexity', 'N/A')}"
                )
                print(f"   æ›´æ–°åçŸ¥è¯†æ·±åº¦: {updated_profile.knowledge_profile}")
            else:
                print("âŒ ç”¨æˆ·ç”»åƒæ›´æ–°å¤±è´¥")

        except Exception as e:
            print(f"âŒ ç”¨æˆ·ç”»åƒæ›´æ–°æµ‹è¯•å¤±è´¥: {e}")

        # 6. ç³»ç»Ÿä¿¡æ¯ç»Ÿè®¡
        print(f"\n6. ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯...")
        print("-" * 60)

        try:
            system_status = system.get_system_status()
            print(
                f"å¤„ç†æŸ¥è¯¢æ€»æ•°: {system_status['processing_statistics']['total_queries']}"
            )
            print(
                f"æˆåŠŸæŸ¥è¯¢æ•°é‡: {system_status['processing_statistics']['successful_queries']}"
            )
            print(
                f"å¹³å‡å¤„ç†æ—¶é—´: {system_status['processing_statistics']['average_processing_time']:.3f}s"
            )

            # æ˜¾ç¤ºæ„ŸçŸ¥å±‚çŠ¶æ€
            perception_status = system.perception_layer.get_status()
            print(f"ç”¨æˆ·ç”»åƒæ•°é‡: {perception_status['user_profiles_count']}")

            if (
                system.perception_layer.user_modeler
                and system.perception_layer.user_modeler.modeler_type == "semantic"
            ):
                print("âœ… ä½¿ç”¨è¯­ä¹‰åˆ†æç”¨æˆ·å»ºæ¨¡")
            else:
                print("ğŸ“ ä½¿ç”¨ä¼ ç»Ÿè§„åˆ™ç”¨æˆ·å»ºæ¨¡")

        except Exception as e:
            print(f"âš ï¸ æ— æ³•è·å–ç³»ç»Ÿç»Ÿè®¡: {e}")

    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {e}")
        import traceback

        traceback.print_exc()

    finally:
        # æ¸…ç†èµ„æº
        try:
            if "system" in locals():
                await system.cleanup()
                print("\nâœ… ç³»ç»Ÿèµ„æºæ¸…ç†å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸ èµ„æºæ¸…ç†æ—¶å‡ºç°é—®é¢˜: {e}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”¬ è¯­ä¹‰ç”¨æˆ·å»ºæ¨¡ç³»ç»Ÿæµ‹è¯•")
    print("æµ‹è¯•LLMé©±åŠ¨çš„ç”¨æˆ·ç”»åƒç”Ÿæˆvsä¼ ç»Ÿè§„åˆ™åŒ¹é…æ–¹æ³•")
    print()

    # æ£€æŸ¥ç¯å¢ƒ
    if not os.path.exists("aienhance"):
        print("âŒ è¯·åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œæ­¤è„šæœ¬")
        sys.exit(1)

    # è¿è¡Œå¼‚æ­¥æµ‹è¯•
    try:
        asyncio.run(test_semantic_user_modeling())
    except KeyboardInterrupt:
        print("\nâš ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿è¡Œå¤±è´¥: {e}")


if __name__ == "__main__":
    main()
