#!/usr/bin/env python3
"""
æµ‹è¯•LLM-basedé¢†åŸŸæ¨æ–­ç³»ç»Ÿ
éªŒè¯æ–°çš„é¢†åŸŸæ¨æ–­åŠŸèƒ½æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import asyncio
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from aienhance.core.domain_inference import (
    DomainInferenceConfig,
    DomainInferenceManager,
    LLMDomainInferenceProvider,
    DomainInferenceResult
)
from aienhance.core.multi_llm_config import (
    MultiLLMConfigManager,
    LLMModelConfig,
    BusinessFunctionLLMConfig
)
from aienhance.core.perception_layer import PerceptionLayer
from aienhance.core.layer_interfaces import PerceptionInput
from aienhance.llm.adapters.ollama_adapter import OllamaLLMAdapter
from aienhance.llm.interfaces import ModelConfig


class MockLLMProvider:
    """æ¨¡æ‹ŸLLMæä¾›å•†ï¼Œç”¨äºæµ‹è¯•"""
    
    def __init__(self, model_name: str = "mock_model"):
        self.model_name = model_name
    
    async def generate_async(self, messages, **kwargs):
        """æ¨¡æ‹Ÿå¼‚æ­¥ç”Ÿæˆ"""
        # å¤„ç†ä¸åŒçš„æ¶ˆæ¯æ ¼å¼
        if isinstance(messages, list) and len(messages) > 0:
            if isinstance(messages[0], dict):
                query_content = messages[0].get("content", "")
            else:
                # å¦‚æœæ˜¯ChatMessageå¯¹è±¡ï¼Œè·å–å…¶å†…å®¹
                query_content = getattr(messages[0], 'content', str(messages[0]))
        else:
            query_content = str(messages)
        
        # ä»å®Œæ•´çš„æç¤ºä¸­æå–å®é™…çš„ç”¨æˆ·æŸ¥è¯¢
        user_query = ""
        if "ç”¨æˆ·æŸ¥è¯¢:" in query_content:
            # æå– "ç”¨æˆ·æŸ¥è¯¢:" åé¢çš„å†…å®¹
            query_start = query_content.find("ç”¨æˆ·æŸ¥è¯¢:") + len("ç”¨æˆ·æŸ¥è¯¢:")
            query_end = query_content.find("\n\n", query_start)
            if query_end == -1:
                query_end = len(query_content)
            user_query = query_content[query_start:query_end].strip()
        else:
            user_query = query_content
        
        
        # æ ¹æ®æå–çš„ç”¨æˆ·æŸ¥è¯¢è¿”å›ä¸åŒçš„é¢†åŸŸæ¨æ–­ç»“æœ
        if any(keyword in user_query.lower() for keyword in ["ç¼–ç¨‹", "python", "ai", "ç®—æ³•", "æŠ€æœ¯", "programming", "algorithm", "å¼€å‘", "äººå·¥æ™ºèƒ½", "æœºå™¨å­¦ä¹ "]):
            return {
                "content": '''
                {
                    "primary_domains": ["technology"],
                    "secondary_domains": ["education"],
                    "confidence_scores": {"technology": 0.9, "education": 0.6},
                    "interdisciplinary": true,
                    "reasoning": "æŸ¥è¯¢æ¶‰åŠæŠ€æœ¯å’Œç¼–ç¨‹å†…å®¹ï¼Œä¸»è¦å±äºæŠ€æœ¯é¢†åŸŸï¼ŒåŒæ—¶æ¶‰åŠå­¦ä¹ ç›¸å…³å†…å®¹ã€‚"
                }
                '''
            }
        elif any(keyword in user_query.lower() for keyword in ["å•†ä¸š", "è¥é”€", "ç®¡ç†", "business", "marketing", "ç»æµ"]):
            return {
                "content": '''
                {
                    "primary_domains": ["business"],
                    "secondary_domains": ["psychology", "economics"],
                    "confidence_scores": {"business": 0.85, "psychology": 0.5, "economics": 0.6},
                    "interdisciplinary": true,
                    "reasoning": "æŸ¥è¯¢æ¶‰åŠå•†ä¸šç®¡ç†ï¼Œæ˜¯å…¸å‹çš„è·¨å­¦ç§‘å†…å®¹ï¼Œç»“åˆäº†å•†ä¸šã€å¿ƒç†å­¦å’Œç»æµå­¦ã€‚"
                }
                '''
            }
        elif any(keyword in user_query.lower() for keyword in ["è‰ºæœ¯", "è®¾è®¡", "åˆ›ä½œ", "art", "design", "ä½œå“"]):
            return {
                "content": '''
                {
                    "primary_domains": ["art"],
                    "secondary_domains": ["psychology"],
                    "confidence_scores": {"art": 0.8, "psychology": 0.4},
                    "interdisciplinary": false,
                    "reasoning": "æŸ¥è¯¢ä¸»è¦æ¶‰åŠè‰ºæœ¯åˆ›ä½œç›¸å…³å†…å®¹ã€‚"
                }
                '''
            }
        elif any(keyword in user_query.lower() for keyword in ["å¤©æ°”", "weather", "ä»Šå¤©"]):
            return {
                "content": '''
                {
                    "primary_domains": ["general"],
                    "secondary_domains": [],
                    "confidence_scores": {"general": 0.7},
                    "interdisciplinary": false,
                    "reasoning": "æŸ¥è¯¢å†…å®¹è¾ƒä¸ºé€šç”¨ï¼Œå±äºæ—¥å¸¸ç”Ÿæ´»ç±»é—®é¢˜ã€‚"
                }
                '''
            }
        else:
            return {
                "content": '''
                {
                    "primary_domains": ["general"],
                    "secondary_domains": [],
                    "confidence_scores": {"general": 0.7},
                    "interdisciplinary": false,
                    "reasoning": "æŸ¥è¯¢å†…å®¹è¾ƒä¸ºé€šç”¨ï¼Œæœªæ£€æµ‹åˆ°ç‰¹å®šä¸“ä¸šé¢†åŸŸç‰¹å¾ã€‚"
                }
                '''
            }
    
    async def chat(self, messages, **kwargs):
        """æä¾›chatæ¥å£å…¼å®¹æ€§"""
        result = await self.generate_async(messages, **kwargs)
        return result["content"]


async def test_domain_inference_basic():
    """æµ‹è¯•åŸºç¡€é¢†åŸŸæ¨æ–­åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•1: åŸºç¡€é¢†åŸŸæ¨æ–­åŠŸèƒ½")
    print("-" * 40)
    
    # åˆ›å»ºæ¨¡æ‹ŸLLMæä¾›å•†
    mock_llm = MockLLMProvider()
    
    # åˆ›å»ºé¢†åŸŸæ¨æ–­é…ç½®
    config = DomainInferenceConfig(
        llm_provider=mock_llm,
        model_name="mock_model",
        temperature=0.1,
        max_tokens=300,
        timeout=10
    )
    
    # åˆ›å»ºé¢†åŸŸæ¨æ–­æä¾›å•†
    provider = LLMDomainInferenceProvider(config)
    success = await provider.initialize()
    
    if not success:
        print("   âŒ åˆå§‹åŒ–å¤±è´¥")
        return False
    
    # æµ‹è¯•æŸ¥è¯¢
    test_cases = [
        ("å¦‚ä½•å­¦ä¹ Pythonç¼–ç¨‹ï¼Ÿ", ["technology"]),
        ("å¦‚ä½•è¿›è¡Œå•†ä¸šè¥é”€ï¼Ÿ", ["business"]), 
        ("å¦‚ä½•åˆ›ä½œè‰ºæœ¯ä½œå“ï¼Ÿ", ["art"]),
        ("ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ", ["general"])
    ]
    
    all_passed = True
    
    for query, expected_domains in test_cases:
        try:
            result = await provider.infer_domains(query)
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«é¢„æœŸé¢†åŸŸ
            all_domains = result.primary_domains + result.secondary_domains
            has_expected = any(domain in all_domains for domain in expected_domains)
            
            status = "âœ…" if has_expected else "âŒ"
            print(f"   {status} æŸ¥è¯¢: {query}")
            print(f"      é¢„æœŸ: {expected_domains}")
            print(f"      ç»“æœ: {result.primary_domains} + {result.secondary_domains}")
            print(f"      æ¨ç†: {result.reasoning}")
            
            if not has_expected:
                all_passed = False
                
        except Exception as e:
            print(f"   âŒ æŸ¥è¯¢å¤±è´¥: {query} - {e}")
            all_passed = False
    
    await provider.cleanup()
    
    print(f"\n   æµ‹è¯•ç»“æœ: {'âœ… å…¨éƒ¨é€šè¿‡' if all_passed else 'âŒ éƒ¨åˆ†å¤±è´¥'}")
    return all_passed


async def test_domain_inference_manager():
    """æµ‹è¯•é¢†åŸŸæ¨æ–­ç®¡ç†å™¨"""
    print("\nğŸ§ª æµ‹è¯•2: é¢†åŸŸæ¨æ–­ç®¡ç†å™¨")
    print("-" * 40)
    
    manager = DomainInferenceManager()
    
    # æ³¨å†Œå¤šä¸ªæä¾›å•†
    providers = [
        ("primary", MockLLMProvider("primary_model")),
        ("fallback", MockLLMProvider("fallback_model"))
    ]
    
    for name, mock_llm in providers:
        config = DomainInferenceConfig(
            llm_provider=mock_llm,
            model_name=f"mock_{name}",
            temperature=0.1,
            max_tokens=300
        )
        
        provider = LLMDomainInferenceProvider(config)
        success = await manager.register_provider(name, provider)
        
        status = "âœ…" if success else "âŒ"
        print(f"   {status} æ³¨å†Œæä¾›å•†: {name}")
    
    # æµ‹è¯•ä½¿ç”¨ä¸åŒæä¾›å•†
    test_query = "å¦‚ä½•å¼€å‘AIç®—æ³•ï¼Ÿ"
    
    try:
        # ä½¿ç”¨ä¸»è¦æä¾›å•†
        result1 = await manager.infer_domains(test_query, provider_name="primary")
        print(f"   âœ… ä¸»è¦æä¾›å•†æ¨æ–­: {result1.primary_domains}")
        
        # ä½¿ç”¨å¤‡é€‰æä¾›å•†
        result2 = await manager.infer_domains(test_query, provider_name="fallback")
        print(f"   âœ… å¤‡é€‰æä¾›å•†æ¨æ–­: {result2.primary_domains}")
        
        # ä½¿ç”¨é»˜è®¤æä¾›å•†
        result3 = await manager.infer_domains(test_query)
        print(f"   âœ… é»˜è®¤æä¾›å•†æ¨æ–­: {result3.primary_domains}")
        
    except Exception as e:
        print(f"   âŒ æ¨æ–­æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    # æ¸…ç†
    await manager.cleanup()
    print("   âœ… ç®¡ç†å™¨æ¸…ç†å®Œæˆ")
    
    return True


async def test_perception_layer_integration():
    """æµ‹è¯•æ„ŸçŸ¥å±‚é›†æˆ"""
    print("\nğŸ§ª æµ‹è¯•3: æ„ŸçŸ¥å±‚é›†æˆæµ‹è¯•")
    print("-" * 40)
    
    try:
        # åˆ›å»ºæ¨¡æ‹ŸLLMæä¾›å•†
        mock_llm = MockLLMProvider("perception_test")
        
        # æ„ŸçŸ¥å±‚é…ç½®ï¼ŒåŒ…å«é¢†åŸŸæ¨æ–­é…ç½®
        perception_config = {
            'domain_inference': {
                'llm_provider': mock_llm,
                'model_name': 'mock_perception_model',
                'temperature': 0.1,
                'max_tokens': 300,
                'timeout': 10,
                'custom_domains': ['technology', 'science', 'education', 'business', 'art']
            }
        }
        
        # åˆ›å»ºæ„ŸçŸ¥å±‚
        perception_layer = PerceptionLayer(
            config=perception_config,
            memory_system=None,  # æš‚ä¸ä½¿ç”¨è®°å¿†ç³»ç»Ÿ
            llm_provider=mock_llm
        )
        
        # åˆå§‹åŒ–æ„ŸçŸ¥å±‚
        success = await perception_layer.initialize()
        if not success:
            print("   âŒ æ„ŸçŸ¥å±‚åˆå§‹åŒ–å¤±è´¥")
            return False
        
        print("   âœ… æ„ŸçŸ¥å±‚åˆå§‹åŒ–æˆåŠŸ")
        
        # åˆ›å»ºæµ‹è¯•è¾“å…¥
        test_input = PerceptionInput(
            query="æˆ‘æƒ³å­¦ä¹ å¦‚ä½•è®¾è®¡å’Œå¼€å‘äººå·¥æ™ºèƒ½ç®—æ³•ï¼Œç‰¹åˆ«æ˜¯æœºå™¨å­¦ä¹ ç›¸å…³çš„æŠ€æœ¯",
            user_id="test_user_001", 
            context={"session_type": "learning", "complexity_level": "intermediate"},
            historical_data=None
        )
        
        # å¤„ç†æ„ŸçŸ¥å±‚è¾“å…¥
        output = await perception_layer.process(test_input)
        
        if output.status.name == "COMPLETED":
            print("   âœ… æ„ŸçŸ¥å±‚å¤„ç†æˆåŠŸ")
            
            # æ£€æŸ¥ç”¨æˆ·ç”»åƒä¸­çš„æ¨æ–­é¢†åŸŸ
            if 'inferred_domains' in str(output.data):
                print("   âœ… é¢†åŸŸæ¨æ–­å·²é›†æˆåˆ°ç”¨æˆ·ç”»åƒä¸­")
            else:
                print("   âš ï¸  é¢†åŸŸæ¨æ–­æœªåœ¨ç”¨æˆ·ç”»åƒä¸­æ‰¾åˆ°")
            
            # æ˜¾ç¤ºéƒ¨åˆ†ç»“æœ
            if output.user_profile:
                print(f"   ç”¨æˆ·ID: {output.user_profile.user_id}")
                print(f"   çŸ¥è¯†é¢†åŸŸ: {output.user_profile.knowledge_profile}")
            
        else:
            print(f"   âŒ æ„ŸçŸ¥å±‚å¤„ç†å¤±è´¥: {output.error_message}")
            return False
        
        # æ¸…ç†
        await perception_layer.cleanup()
        print("   âœ… æ„ŸçŸ¥å±‚æ¸…ç†å®Œæˆ")
        
        return True
        
    except Exception as e:
        print(f"   âŒ æ„ŸçŸ¥å±‚é›†æˆæµ‹è¯•å¤±è´¥: {e}")
        return False


async def test_multi_llm_config_integration():
    """æµ‹è¯•å¤šLLMé…ç½®é›†æˆ"""
    print("\nğŸ§ª æµ‹è¯•4: å¤šLLMé…ç½®é›†æˆ")
    print("-" * 40)
    
    try:
        # åˆ›å»ºå¤šLLMé…ç½®ç®¡ç†å™¨
        config_manager = MultiLLMConfigManager()
        
        # è®¾ç½®é»˜è®¤é…ç½®
        default_config = LLMModelConfig(
            provider="mock",
            model_name="default_model",
            temperature=0.7,
            max_tokens=800
        )
        config_manager.set_default_config(default_config)
        print("   âœ… è®¾ç½®é»˜è®¤é…ç½®")
        
        # æ³¨å†Œé¢†åŸŸæ¨æ–­ä¸“ç”¨é…ç½®
        domain_inference_config = BusinessFunctionLLMConfig(
            function_name="domain_inference",
            primary_model=LLMModelConfig(
                provider="mock",
                model_name="domain_specialized_model",
                temperature=0.1,  # ä½æ¸©åº¦ç”¨äºåˆ†ç±»ä»»åŠ¡
                max_tokens=300,
                timeout=10
            ),
            fallback_model=LLMModelConfig(
                provider="mock",
                model_name="domain_fallback_model", 
                temperature=0.1,
                max_tokens=200,
                timeout=8
            )
        )
        
        config_manager.register_business_function(domain_inference_config)
        print("   âœ… æ³¨å†Œé¢†åŸŸæ¨æ–­ä¸“ç”¨é…ç½®")
        
        # è·å–å¹¶éªŒè¯é…ç½®
        retrieved_config = config_manager.get_config_for_function("domain_inference")
        if retrieved_config:
            print(f"   âœ… è·å–é…ç½®: {retrieved_config.provider}/{retrieved_config.model_name}")
            print(f"      å‚æ•°: T={retrieved_config.temperature}, Max={retrieved_config.max_tokens}")
        else:
            print("   âŒ é…ç½®è·å–å¤±è´¥")
            return False
        
        # è·å–å¤‡é€‰é…ç½®
        fallback_config = config_manager.get_fallback_config_for_function("domain_inference")
        if fallback_config:
            print(f"   âœ… å¤‡é€‰é…ç½®: {fallback_config.provider}/{fallback_config.model_name}")
        
        # æµ‹è¯•åŠŸèƒ½æ§åˆ¶
        print("   âœ… åŠŸèƒ½æ§åˆ¶æµ‹è¯•:")
        print(f"      ç¦ç”¨åŠŸèƒ½: {config_manager.disable_function('domain_inference')}")
        print(f"      å¯ç”¨åŠŸèƒ½: {config_manager.enable_function('domain_inference')}")
        
        return True
        
    except Exception as e:
        print(f"   âŒ å¤šLLMé…ç½®é›†æˆæµ‹è¯•å¤±è´¥: {e}")
        return False


async def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("ğŸš€ LLM-basedé¢†åŸŸæ¨æ–­ç³»ç»Ÿæµ‹è¯•å¥—ä»¶")
    print("=" * 60)
    
    tests = [
        ("åŸºç¡€é¢†åŸŸæ¨æ–­", test_domain_inference_basic),
        ("é¢†åŸŸæ¨æ–­ç®¡ç†å™¨", test_domain_inference_manager), 
        ("æ„ŸçŸ¥å±‚é›†æˆ", test_perception_layer_integration),
        ("å¤šLLMé…ç½®é›†æˆ", test_multi_llm_config_integration)
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        print(f"\nğŸ“‹ {test_name}")
        try:
            result = await test_func()
            if result:
                passed += 1
                print(f"   âœ… {test_name} - é€šè¿‡")
            else:
                print(f"   âŒ {test_name} - å¤±è´¥")
        except Exception as e:
            print(f"   âŒ {test_name} - å¼‚å¸¸: {e}")
    
    print("\n" + "=" * 60)
    print(f"ğŸ“Š æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡! LLM-basedé¢†åŸŸæ¨æ–­ç³»ç»Ÿå·¥ä½œæ­£å¸¸")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥å®ç°")
    
    print("\nğŸ’¡ ç³»ç»Ÿç‰¹æ€§:")
    print("   â€¢ ä½¿ç”¨å¤§æ¨¡å‹è¿›è¡Œæ™ºèƒ½é¢†åŸŸæ¨æ–­ï¼Œæ›¿ä»£ç®€å•å…³é”®è¯åŒ¹é…")
    print("   â€¢ æ”¯æŒå¤šæ¨¡å‹é…ç½®ï¼Œä¸ºä¸åŒä¸šåŠ¡åŠŸèƒ½ä½¿ç”¨ä¸åŒLLM")
    print("   â€¢ æ”¯æŒä¸»è¦æ¨¡å‹å’Œå¤‡é€‰æ¨¡å‹é…ç½®")
    print("   â€¢ é›†æˆåˆ°æ„ŸçŸ¥å±‚ï¼Œè‡ªåŠ¨è¿›è¡Œé¢†åŸŸæ¨æ–­")
    print("   â€¢ ç®€åŒ–çš„å•ä¸€å®ç°è·¯å¾„ï¼Œä¸“æ³¨LLMæ¨æ–­")
    
    return passed == total


async def main():
    """ä¸»å‡½æ•°"""
    success = await run_all_tests()
    return 0 if success else 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())