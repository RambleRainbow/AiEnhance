#!/usr/bin/env python3
"""
LLMæ¥å£é›†æˆæµ‹è¯•
æµ‹è¯•å®Œæ•´çš„è®°å¿†-è®¤çŸ¥ååŒç³»ç»Ÿä¸LLMçš„é›†æˆåŠŸèƒ½
"""

import asyncio
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from aienhance import MemorySystemConfig, create_model_config, create_system


async def test_ollama_llm_integration():
    """æµ‹è¯•Ollama LLMé›†æˆ"""
    print("\n" + "=" * 60)
    print("ğŸ¤– æµ‹è¯•Ollama LLMé›†æˆ")
    print("=" * 60)

    try:
        # åˆ›å»ºOllama LLMé…ç½®
        llm_config = create_model_config(
            provider="ollama",
            model_name="qwen3:8b",  # ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹è¿›è¡Œæµ‹è¯•
            api_base="http://localhost:11434",
            temperature=0.7,
            max_tokens=500,
        )

        # åˆ›å»ºç³»ç»Ÿï¼ˆä»…LLMï¼Œä¸ä½¿ç”¨è®°å¿†ç³»ç»Ÿè¿›è¡Œç®€å•æµ‹è¯•ï¼‰
        system = create_system(
            system_type="default",
            llm_provider="ollama",
            llm_model_name="qwen3:8b",
            llm_api_base="http://localhost:11434",
            llm_temperature=0.7,
            llm_max_tokens=500,
        )

        print("âœ… ç³»ç»Ÿåˆ›å»ºæˆåŠŸ")

        # è·å–ç³»ç»ŸçŠ¶æ€
        status = system.get_system_status()
        print(f"ğŸ“Š ç³»ç»ŸçŠ¶æ€: åˆå§‹åŒ–={status['initialized']}")
        print(f"ğŸ§  LLMæä¾›å•†: {status.get('llm_provider', 'None')}")

        # æµ‹è¯•ç®€å•æŸ¥è¯¢
        print("\nğŸ” æµ‹è¯•ç”¨æˆ·æŸ¥è¯¢...")
        response = await system.process_query(
            query="ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿè¯·ç®€è¦è§£é‡Šã€‚", user_id="test_user_001"
        )

        print(f"ğŸ’¬ ç³»ç»Ÿå“åº”: {response.content[:200]}...")
        print(f"ğŸ¯ å“åº”é•¿åº¦: {len(response.content)} å­—ç¬¦")

        # æ£€æŸ¥LLMå…ƒæ•°æ®
        if (
            hasattr(response.adaptation_info, "metadata")
            and response.adaptation_info.metadata
        ):
            if response.adaptation_info.metadata.get("llm_generated"):
                print("âœ… LLMæˆåŠŸç”Ÿæˆå“åº”")
                print(
                    f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {response.adaptation_info.metadata.get('llm_model')}"
                )
                usage = response.adaptation_info.metadata.get("llm_usage", {})
                if usage:
                    print(f"ğŸ“ˆ Tokenä½¿ç”¨: {usage}")
            else:
                print("âš ï¸ ä½¿ç”¨é»˜è®¤å“åº”ç”Ÿæˆ")

        return True

    except Exception as e:
        print(f"âŒ Ollama LLMæµ‹è¯•å¤±è´¥: {e}")
        return False


async def test_memory_llm_integration():
    """æµ‹è¯•è®°å¿†ç³»ç»Ÿä¸LLMçš„å®Œæ•´é›†æˆ"""
    print("\n" + "=" * 60)
    print("ğŸ§  æµ‹è¯•è®°å¿†ç³»ç»Ÿä¸LLMå®Œæ•´é›†æˆ")
    print("=" * 60)

    try:
        # åˆ›å»ºå¸¦è®°å¿†ç³»ç»Ÿçš„å®Œæ•´é…ç½®
        memory_config = MemorySystemConfig(
            system_type="mirix", api_key="test-key", api_base="http://localhost:8000"
        )

        llm_config = create_model_config(
            provider="ollama",
            model_name="qwen3:8b",
            api_base="http://localhost:11434",
            temperature=0.7,
            max_tokens=300,
        )

        # åˆ›å»ºå®Œæ•´ç³»ç»Ÿ
        system = create_system(
            system_type="educational",
            memory_system_type="mirix",
            llm_provider="ollama",
            memory_api_key="test-key",
            memory_api_base="http://localhost:8000",
            llm_model_name="qwen3:8b",
            llm_api_base="http://localhost:11434",
            llm_temperature=0.7,
            llm_max_tokens=300,
        )

        print("âœ… å®Œæ•´ç³»ç»Ÿåˆ›å»ºæˆåŠŸ")

        # è·å–ç³»ç»ŸçŠ¶æ€
        status = system.get_system_status()
        print("ğŸ“Š ç³»ç»ŸçŠ¶æ€:")
        print(f"  - åˆå§‹åŒ–: {status['initialized']}")
        print(
            f"  - è®°å¿†ç³»ç»Ÿ: {status.get('memory_system', {}).get('system_type', 'None')}"
        )
        print(
            f"  - LLMæä¾›å•†: {status.get('llm_provider', {}).get('provider', 'None')}"
        )

        # æµ‹è¯•è¿ç»­å¯¹è¯
        print("\nğŸ”„ æµ‹è¯•è¿ç»­å¯¹è¯...")

        queries = [
            "æˆ‘å¯¹æœºå™¨å­¦ä¹ å¾ˆæ„Ÿå…´è¶£ï¼Œèƒ½ä»‹ç»ä¸€ä¸‹åŸºæœ¬æ¦‚å¿µå—ï¼Ÿ",
            "åˆšæ‰æåˆ°çš„ç›‘ç£å­¦ä¹ èƒ½ä¸¾ä¸ªä¾‹å­å—ï¼Ÿ",
            "æˆ‘æƒ³æ·±å…¥äº†è§£ç¥ç»ç½‘ç»œï¼Œæœ‰ä»€ä¹ˆå»ºè®®ï¼Ÿ",
        ]

        user_id = "test_user_002"

        for i, query in enumerate(queries, 1):
            print(f"\nğŸ“ æŸ¥è¯¢ {i}: {query}")

            try:
                response = await system.process_query(
                    query=query,
                    user_id=user_id,
                    context={"session_id": "test_session_001"},
                )

                print(f"ğŸ’­ å“åº” {i}: {response.content[:150]}...")

                # åˆ†æå¤„ç†æ­¥éª¤
                steps = response.processing_metadata.get("processing_steps", [])
                print(f"ğŸ”§ å¤„ç†æ­¥éª¤: {', '.join(steps)}")

                # æ£€æŸ¥è®°å¿†æ¿€æ´»
                if response.activated_memories:
                    print(f"ğŸ§  æ¿€æ´»è®°å¿†: {len(response.activated_memories)} æ¡")

                # æ£€æŸ¥LLMç”Ÿæˆ
                if (
                    hasattr(response.adaptation_info, "metadata")
                    and response.adaptation_info.metadata
                ):
                    if response.adaptation_info.metadata.get("llm_generated"):
                        print("âœ… LLMå‚ä¸å“åº”ç”Ÿæˆ")

            except Exception as e:
                print(f"âš ï¸ æŸ¥è¯¢ {i} å¤„ç†å¤±è´¥: {e}")
                # ç»§ç»­å…¶ä»–æŸ¥è¯¢çš„æµ‹è¯•
                continue

        return True

    except Exception as e:
        print(f"âŒ å®Œæ•´é›†æˆæµ‹è¯•å¤±è´¥: {e}")
        return False


async def test_different_llm_providers():
    """æµ‹è¯•ä¸åŒLLMæä¾›å•†çš„åˆ‡æ¢"""
    print("\n" + "=" * 60)
    print("ğŸ”„ æµ‹è¯•ä¸åŒLLMæä¾›å•†åˆ‡æ¢")
    print("=" * 60)

    providers_configs = [
        {
            "name": "Ollama",
            "provider": "ollama",
            "model_name": "qwen3:8b",
            "api_base": "http://localhost:11434",
        },
        # æ³¨é‡Šæ‰éœ€è¦APIå¯†é’¥çš„æä¾›å•†ï¼Œé¿å…æµ‹è¯•å¤±è´¥
        # {
        #     "name": "OpenAI",
        #     "provider": "openai",
        #     "model_name": "gpt-3.5-turbo",
        #     "api_key": "your-openai-key"
        # },
        # {
        #     "name": "Anthropic",
        #     "provider": "anthropic",
        #     "model_name": "claude-3-haiku-20240307",
        #     "api_key": "your-anthropic-key"
        # }
    ]

    success_count = 0

    for config in providers_configs:
        print(f"\nğŸ¤– æµ‹è¯• {config['name']} æä¾›å•†...")

        try:
            # åŠ¨æ€æ„å»ºå‚æ•°
            kwargs = {
                "system_type": "default",
                "llm_provider": config["provider"],
                "llm_model_name": config["model_name"],
            }

            # æ·»åŠ å¯é€‰å‚æ•°
            if "api_base" in config:
                kwargs["llm_api_base"] = config["api_base"]
            if "api_key" in config:
                kwargs["llm_api_key"] = config["api_key"]

            # åˆ›å»ºç³»ç»Ÿ
            system = create_system(**kwargs)

            print(f"âœ… {config['name']} ç³»ç»Ÿåˆ›å»ºæˆåŠŸ")

            # ç®€å•æµ‹è¯•æŸ¥è¯¢
            try:
                response = await system.process_query(
                    query="Hello, how are you?",
                    user_id=f"test_user_{config['provider']}",
                )

                if response.content:
                    print(
                        f"ğŸ’¬ {config['name']} å“åº”æ­£å¸¸ (é•¿åº¦: {len(response.content)})"
                    )
                    success_count += 1
                else:
                    print(f"âš ï¸ {config['name']} å“åº”ä¸ºç©º")

            except Exception as e:
                print(f"âš ï¸ {config['name']} æŸ¥è¯¢æµ‹è¯•å¤±è´¥: {e}")

        except Exception as e:
            print(f"âŒ {config['name']} åˆå§‹åŒ–å¤±è´¥: {e}")

    print(f"\nğŸ“Š æä¾›å•†æµ‹è¯•ç»“æœ: {success_count}/{len(providers_configs)} æˆåŠŸ")
    return success_count > 0


async def test_streaming_functionality():
    """æµ‹è¯•æµå¼å“åº”åŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("ğŸŒŠ æµ‹è¯•æµå¼å“åº”åŠŸèƒ½")
    print("=" * 60)

    try:
        # åˆ›å»ºæ”¯æŒæµå¼çš„ç³»ç»Ÿ
        system = create_system(
            system_type="default",
            llm_provider="ollama",
            llm_model_name="qwen3:8b",
            llm_api_base="http://localhost:11434",
        )

        print("âœ… æµå¼ç³»ç»Ÿåˆ›å»ºæˆåŠŸ")

        # æµ‹è¯•æµå¼å“åº”ï¼ˆéœ€è¦æ·»åŠ åˆ°ç³»ç»ŸAPIä¸­ï¼‰
        # æ³¨æ„ï¼šå½“å‰ç³»ç»Ÿæ¶æ„ä¸­æ²¡æœ‰ç›´æ¥çš„æµå¼æ¥å£ï¼Œè¿™é‡Œæ¼”ç¤ºæ¦‚å¿µ
        if hasattr(system, "llm_provider") and system.llm_provider:
            print("ğŸ”„ æµ‹è¯•LLMæµå¼æ¥å£...")

            from aienhance.llm import create_chat_message

            messages = [create_chat_message("user", "è¯·å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„çŸ­è¯—")]

            try:
                # ç›´æ¥æµ‹è¯•LLMé€‚é…å™¨çš„æµå¼åŠŸèƒ½
                await system.llm_provider.initialize()

                print("ğŸ’­ æµå¼å“åº”å¼€å§‹:")
                content_parts = []
                async for chunk in system.llm_provider.chat_stream(messages):
                    content_parts.append(chunk)
                    print(chunk, end="", flush=True)

                full_content = "".join(content_parts)
                print(f"\nâœ… æµå¼å“åº”å®Œæˆ (æ€»é•¿åº¦: {len(full_content)})")
                return True

            except Exception as e:
                print(f"âš ï¸ æµå¼å“åº”æµ‹è¯•å¤±è´¥: {e}")
                return False
        else:
            print("âš ï¸ ç³»ç»Ÿæœªé…ç½®LLMæä¾›å•†")
            return False

    except Exception as e:
        print(f"âŒ æµå¼åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        return False


async def test_system_performance():
    """æµ‹è¯•ç³»ç»Ÿæ€§èƒ½"""
    print("\n" + "=" * 60)
    print("âš¡ æµ‹è¯•ç³»ç»Ÿæ€§èƒ½")
    print("=" * 60)

    import time

    try:
        # åˆ›å»ºæµ‹è¯•ç³»ç»Ÿ
        system = create_system(
            system_type="default",
            llm_provider="ollama",
            llm_model_name="qwen3:8b",
            llm_api_base="http://localhost:11434",
            llm_temperature=0.5,
            llm_max_tokens=100,  # é™åˆ¶tokenæ•°é‡ä»¥æé«˜é€Ÿåº¦
        )

        print("âœ… æ€§èƒ½æµ‹è¯•ç³»ç»Ÿåˆ›å»ºæˆåŠŸ")

        # å‡†å¤‡æµ‹è¯•æŸ¥è¯¢
        test_queries = [
            "ä»€ä¹ˆæ˜¯AIï¼Ÿ",
            "è§£é‡Šæœºå™¨å­¦ä¹ ",
            "Pythonçš„ç‰¹ç‚¹",
            "æ•°æ®ç§‘å­¦åº”ç”¨",
            "æ·±åº¦å­¦ä¹ æ¦‚å¿µ",
        ]

        print(f"ğŸ”„ å¼€å§‹å¤„ç† {len(test_queries)} ä¸ªæŸ¥è¯¢...")

        total_start_time = time.time()
        results = []

        for i, query in enumerate(test_queries, 1):
            start_time = time.time()

            try:
                response = await system.process_query(
                    query=query, user_id=f"perf_user_{i:03d}"
                )

                end_time = time.time()
                duration = end_time - start_time

                results.append(
                    {
                        "query": query,
                        "duration": duration,
                        "response_length": len(response.content),
                        "success": True,
                    }
                )

                print(
                    f"âœ… æŸ¥è¯¢ {i}: {duration:.2f}s (å“åº”é•¿åº¦: {len(response.content)})"
                )

            except Exception as e:
                end_time = time.time()
                duration = end_time - start_time

                results.append(
                    {
                        "query": query,
                        "duration": duration,
                        "error": str(e),
                        "success": False,
                    }
                )

                print(f"âŒ æŸ¥è¯¢ {i}: {duration:.2f}s (å¤±è´¥: {e})")

        total_end_time = time.time()
        total_duration = total_end_time - total_start_time

        # è®¡ç®—æ€§èƒ½ç»Ÿè®¡
        successful_results = [r for r in results if r["success"]]

        if successful_results:
            avg_duration = sum(r["duration"] for r in successful_results) / len(
                successful_results
            )
            avg_response_length = sum(
                r["response_length"] for r in successful_results
            ) / len(successful_results)

            print("\nğŸ“Š æ€§èƒ½ç»Ÿè®¡:")
            print(f"  - æ€»æ—¶é—´: {total_duration:.2f}s")
            print(
                f"  - æˆåŠŸç‡: {len(successful_results)}/{len(test_queries)} ({len(successful_results) / len(test_queries) * 100:.1f}%)"
            )
            print(f"  - å¹³å‡å“åº”æ—¶é—´: {avg_duration:.2f}s")
            print(f"  - å¹³å‡å“åº”é•¿åº¦: {avg_response_length:.0f} å­—ç¬¦")
            print(f"  - ååé‡: {len(successful_results) / total_duration:.2f} æŸ¥è¯¢/ç§’")

        return len(successful_results) > 0

    except Exception as e:
        print(f"âŒ æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
        return False


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹LLMæ¥å£é›†æˆæµ‹è¯•")
    print("æ—¶é—´:", asyncio.get_event_loop().time())

    test_results = {}

    # æ‰§è¡Œå„é¡¹æµ‹è¯•
    tests = [
        ("Ollama LLMé›†æˆ", test_ollama_llm_integration),
        ("è®°å¿†-LLMå®Œæ•´é›†æˆ", test_memory_llm_integration),
        ("å¤šæä¾›å•†åˆ‡æ¢", test_different_llm_providers),
        ("æµå¼å“åº”åŠŸèƒ½", test_streaming_functionality),
        ("ç³»ç»Ÿæ€§èƒ½", test_system_performance),
    ]

    for test_name, test_func in tests:
        print(f"\n{'=' * 20} {test_name} {'=' * 20}")
        try:
            result = await test_func()
            test_results[test_name] = "âœ… é€šè¿‡" if result else "âš ï¸ éƒ¨åˆ†å¤±è´¥"
        except Exception as e:
            test_results[test_name] = f"âŒ å¤±è´¥: {e}"
            print(f"âŒ æµ‹è¯•å¼‚å¸¸: {e}")

    # æ±‡æ€»ç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ“‹ æµ‹è¯•ç»“æœæ±‡æ€»")
    print("=" * 60)

    for test_name, result in test_results.items():
        print(f"{test_name}: {result}")

    # ç»Ÿè®¡
    passed = sum(1 for r in test_results.values() if "âœ…" in r)
    partial = sum(1 for r in test_results.values() if "âš ï¸" in r)
    failed = sum(1 for r in test_results.values() if "âŒ" in r)

    print(f"\nğŸ“Š æµ‹è¯•ç»Ÿè®¡: é€šè¿‡={passed}, éƒ¨åˆ†å¤±è´¥={partial}, å¤±è´¥={failed}")

    if passed == len(tests):
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼LLMé›†æˆåŠŸèƒ½æ­£å¸¸ã€‚")
    elif passed + partial > 0:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•é€šè¿‡ï¼Œç³»ç»ŸåŸºæœ¬å¯ç”¨ã€‚")
    else:
        print("âŒ æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®å’Œç¯å¢ƒã€‚")


if __name__ == "__main__":
    asyncio.run(main())
