#!/usr/bin/env python3
"""
LLMé›†æˆæ¼”ç¤ºè„šæœ¬
å±•ç¤ºè®°å¿†-è®¤çŸ¥ååŒç³»ç»Ÿä¸LLMæä¾›å•†çš„æ— ç¼é›†æˆèƒ½åŠ›
"""

import asyncio
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from aienhance import create_system


async def demo_system_creation():
    """æ¼”ç¤ºç³»ç»Ÿåˆ›å»ºä¸é…ç½®"""
    print("ğŸ”§ ç³»ç»Ÿåˆ›å»ºä¸é…ç½®æ¼”ç¤º")
    print("=" * 50)
    
    # 1. åˆ›å»ºé»˜è®¤ç³»ç»Ÿï¼ˆä¸é…ç½®LLMï¼‰
    print("\n1ï¸âƒ£ åˆ›å»ºé»˜è®¤ç³»ç»Ÿï¼ˆæ— LLMï¼‰")
    default_system = create_system(system_type="default")
    status = default_system.get_system_status()
    print(f"   âœ… ç³»ç»Ÿåˆ›å»ºæˆåŠŸ")
    print(f"   ğŸ“Š LLMçŠ¶æ€: {status.get('llm_provider', 'None')}")
    
    # 2. åˆ›å»ºå¸¦Ollama LLMçš„ç³»ç»Ÿ
    print("\n2ï¸âƒ£ åˆ›å»ºå¸¦Ollama LLMçš„ç³»ç»Ÿ")
    ollama_system = create_system(
        system_type="educational",
        llm_provider="ollama",
        llm_model_name="llama3.2:1b",
        llm_api_base="http://localhost:11434",
        llm_temperature=0.7,
        llm_max_tokens=300
    )
    status = ollama_system.get_system_status()
    print(f"   âœ… ç³»ç»Ÿåˆ›å»ºæˆåŠŸ")
    print(f"   ğŸ¤– LLMé…ç½®: {status.get('llm_provider', {})}")
    
    # 3. åˆ›å»ºå®Œæ•´é…ç½®ç³»ç»Ÿï¼ˆLLM + è®°å¿†ï¼‰
    print("\n3ï¸âƒ£ åˆ›å»ºå®Œæ•´é…ç½®ç³»ç»Ÿï¼ˆLLM + è®°å¿†ï¼‰")
    full_system = create_system(
        system_type="research",
        # è®°å¿†ç³»ç»Ÿé…ç½®
        memory_system_type="mirix",
        memory_api_key="demo-key",
        memory_api_base="http://localhost:8000",
        # LLMé…ç½®
        llm_provider="ollama",
        llm_model_name="llama3.2:1b",
        llm_temperature=0.8,
        # åµŒå…¥æ¨¡å‹é…ç½®
        embedding_provider="ollama",
        embedding_model_name="mxbai-embed-large"
    )
    status = full_system.get_system_status()
    print(f"   âœ… å®Œæ•´ç³»ç»Ÿåˆ›å»ºæˆåŠŸ")
    print(f"   ğŸ§  è®°å¿†ç³»ç»Ÿ: {status.get('memory_system', {}).get('system_type', 'None')}")
    print(f"   ğŸ¤– LLMæä¾›å•†: {status.get('llm_provider', {}).get('provider', 'None')}")
    print(f"   ğŸ¯ åµŒå…¥æä¾›å•†: {status.get('embedding_provider', {}).get('provider', 'None')}")
    
    return full_system


async def demo_provider_switching():
    """æ¼”ç¤ºæä¾›å•†åˆ‡æ¢èƒ½åŠ›"""
    print("\n\nğŸ”„ æä¾›å•†åˆ‡æ¢èƒ½åŠ›æ¼”ç¤º")
    print("=" * 50)
    
    providers = [
        {
            "name": "Ollamaæœ¬åœ°éƒ¨ç½²",
            "config": {
                "llm_provider": "ollama",
                "llm_model_name": "llama3.2:1b",
                "llm_api_base": "http://localhost:11434"
            }
        },
        {
            "name": "OpenAIæœåŠ¡",
            "config": {
                "llm_provider": "openai",
                "llm_model_name": "gpt-3.5-turbo",
                "llm_api_key": "${OPENAI_API_KEY}"  # ä»ç¯å¢ƒå˜é‡è·å–
            }
        },
        {
            "name": "Anthropic Claude",
            "config": {
                "llm_provider": "anthropic",
                "llm_model_name": "claude-3-haiku-20240307",
                "llm_api_key": "${ANTHROPIC_API_KEY}"  # ä»ç¯å¢ƒå˜é‡è·å–
            }
        }
    ]
    
    for i, provider in enumerate(providers, 1):
        print(f"\n{i}ï¸âƒ£ é…ç½® {provider['name']}")
        try:
            system = create_system(
                system_type="default",
                **provider['config']
            )
            
            status = system.get_system_status()
            llm_info = status.get('llm_provider', {})
            print(f"   âœ… {provider['name']} é…ç½®æˆåŠŸ")
            print(f"   ğŸ“‹ æ¨¡å‹: {llm_info.get('model', 'Unknown')}")
            print(f"   ğŸ”§ æä¾›å•†: {llm_info.get('provider', 'Unknown')}")
            
        except Exception as e:
            print(f"   âš ï¸ {provider['name']} é…ç½®å¤±è´¥: {e}")


async def demo_cognitive_processing():
    """æ¼”ç¤ºè®¤çŸ¥å¤„ç†æµç¨‹"""
    print("\n\nğŸ§  è®¤çŸ¥å¤„ç†æµç¨‹æ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºæ¼”ç¤ºç³»ç»Ÿ
    system = create_system(
        system_type="educational",
        llm_provider="ollama",
        llm_model_name="llama3.2:1b",
        llm_temperature=0.7
    )
    
    print("âœ… æ¼”ç¤ºç³»ç»Ÿåˆ›å»ºæˆåŠŸ")
    
    # æ¨¡æ‹Ÿç”¨æˆ·æŸ¥è¯¢
    test_queries = [
        "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
        "æ·±åº¦å­¦ä¹ ä¸ä¼ ç»Ÿæœºå™¨å­¦ä¹ æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
        "èƒ½ç»™æˆ‘ä¸€ä¸ªç¥ç»ç½‘ç»œçš„ç®€å•ä¾‹å­å—ï¼Ÿ"
    ]
    
    print(f"\nğŸ” æ¨¡æ‹Ÿè®¤çŸ¥å¤„ç†æµç¨‹ï¼ˆå…±{len(test_queries)}ä¸ªæŸ¥è¯¢ï¼‰")
    
    for i, query in enumerate(test_queries, 1):
        print(f"\n--- æŸ¥è¯¢ {i}: {query} ---")
        
        try:
            # è¿™é‡Œä¸å®é™…æ‰§è¡Œç½‘ç»œè¯·æ±‚ï¼Œè€Œæ˜¯å±•ç¤ºå¤„ç†æµç¨‹
            print(f"ğŸ“¥ è¾“å…¥: {query}")
            print(f"ğŸ”„ å¤„ç†æµç¨‹:")
            print(f"   1ï¸âƒ£ æ„ŸçŸ¥å±‚: ç”¨æˆ·å»ºæ¨¡ + æƒ…å¢ƒåˆ†æ")
            print(f"   2ï¸âƒ£ è®¤çŸ¥å±‚: è®°å¿†æ¿€æ´» + è¯­ä¹‰å¢å¼º + ç±»æ¯”æ¨ç†")
            print(f"   3ï¸âƒ£ è¡Œä¸ºå±‚: ä¸ªæ€§åŒ–é€‚é… + LLMç”Ÿæˆ")
            print(f"   4ï¸âƒ£ è¾“å‡ºå±‚: ç»“æ„åŒ–å“åº”")
            
            # æ£€æŸ¥ç³»ç»Ÿç»„ä»¶
            components_status = {
                "ç”¨æˆ·å»ºæ¨¡å™¨": hasattr(system, 'user_modeler'),
                "è®°å¿†æ¿€æ´»å™¨": hasattr(system, 'memory_activator'),
                "LLMæä¾›å•†": hasattr(system, 'llm_provider'),
                "è‡ªé€‚åº”è¾“å‡º": hasattr(system, 'adaptive_output')
            }
            
            print(f"ğŸ”§ ç³»ç»Ÿç»„ä»¶çŠ¶æ€:")
            for component, status in components_status.items():
                status_icon = "âœ…" if status else "âŒ"
                print(f"   {status_icon} {component}")
                
        except Exception as e:
            print(f"âŒ å¤„ç†å¤±è´¥: {e}")


async def demo_configuration_flexibility():
    """æ¼”ç¤ºé…ç½®çµæ´»æ€§"""
    print("\n\nâš™ï¸ é…ç½®çµæ´»æ€§æ¼”ç¤º")
    print("=" * 50)
    
    scenarios = [
        {
            "name": "æ•™è‚²åœºæ™¯ - ä½æ¸©åº¦ï¼Œè¯¦ç»†è§£é‡Š",
            "config": {
                "system_type": "educational",
                "llm_provider": "ollama",
                "llm_model_name": "llama3.2:1b",
                "llm_temperature": 0.3,
                "llm_max_tokens": 500
            }
        },
        {
            "name": "ç ”ç©¶åœºæ™¯ - é«˜æ¸©åº¦ï¼Œåˆ›æ–°æ€ç»´",
            "config": {
                "system_type": "research", 
                "llm_provider": "ollama",
                "llm_model_name": "llama3.2:1b",
                "llm_temperature": 0.9,
                "llm_max_tokens": 800
            }
        },
        {
            "name": "å¤šæ¨¡æ€åœºæ™¯ - LLM+åµŒå…¥+è®°å¿†",
            "config": {
                "system_type": "default",
                "llm_provider": "ollama",
                "llm_model_name": "llama3.2:1b",
                "embedding_provider": "ollama",
                "embedding_model_name": "mxbai-embed-large",
                "memory_system_type": "mirix",
                "memory_api_key": "demo-key"
            }
        }
    ]
    
    for i, scenario in enumerate(scenarios, 1):
        print(f"\n{i}ï¸âƒ£ {scenario['name']}")
        
        try:
            system = create_system(**scenario['config'])
            status = system.get_system_status()
            
            print(f"   âœ… åœºæ™¯é…ç½®æˆåŠŸ")
            print(f"   ğŸ“Š ç³»ç»Ÿç±»å‹: {scenario['config']['system_type']}")
            
            if status.get('llm_provider'):
                print(f"   ğŸ¤– LLMé…ç½®: {status['llm_provider']}")
            
            if status.get('embedding_provider'):
                print(f"   ğŸ¯ åµŒå…¥æ¨¡å‹: {status['embedding_provider']}")
                
            if status.get('memory_system'):
                print(f"   ğŸ§  è®°å¿†ç³»ç»Ÿ: {status['memory_system']}")
                
        except Exception as e:
            print(f"   âŒ é…ç½®å¤±è´¥: {e}")


async def demo_architecture_benefits():
    """æ¼”ç¤ºæ¶æ„ä¼˜åŠ¿"""
    print("\n\nğŸ—ï¸ æ¶æ„ä¼˜åŠ¿æ¼”ç¤º")
    print("=" * 50)
    
    print("ğŸ¯ æœ¬æ¶æ„çš„æ ¸å¿ƒä¼˜åŠ¿:")
    print()
    
    advantages = [
        {
            "title": "1. æä¾›å•†æ— å…³æ€§",
            "description": "é€šè¿‡ç»Ÿä¸€æ¥å£æ”¯æŒOllamaã€OpenAIã€Anthropicç­‰å¤šç§LLMæä¾›å•†",
            "example": "system.llm_provider.chat(messages) - æ— è®ºåº•å±‚æ˜¯ä»€ä¹ˆæ¨¡å‹"
        },
        {
            "title": "2. çƒ­æ’æ‹”èƒ½åŠ›",
            "description": "å¯ä»¥åœ¨è¿è¡Œæ—¶åˆ‡æ¢LLMæä¾›å•†ï¼Œæ— éœ€ä¿®æ”¹æ ¸å¿ƒä¸šåŠ¡é€»è¾‘",
            "example": "create_system(llm_provider='ollama') -> create_system(llm_provider='openai')"
        },
        {
            "title": "3. é…ç½®é©±åŠ¨",
            "description": "é€šè¿‡ç®€å•é…ç½®å³å¯ç»„åˆä¸åŒçš„è®°å¿†ç³»ç»Ÿå’ŒLLMæä¾›å•†",
            "example": "memory_system_type='mirix' + llm_provider='anthropic'"
        },
        {
            "title": "4. æ¨¡å—åŒ–è®¾è®¡",
            "description": "æ„ŸçŸ¥å±‚ã€è®¤çŸ¥å±‚ã€è¡Œä¸ºå±‚ç‹¬ç«‹å·¥ä½œï¼ŒLLMä½œä¸ºå¯é€‰å¢å¼º",
            "example": "ç³»ç»Ÿå¯ä»¥åœ¨æ²¡æœ‰LLMçš„æƒ…å†µä¸‹æ­£å¸¸å·¥ä½œï¼ŒLLMä»…ä½œä¸ºè¾“å‡ºå¢å¼º"
        },
        {
            "title": "5. æ‰©å±•å‹å¥½",
            "description": "é€šè¿‡å·¥å‚æ¨¡å¼è½»æ¾æ·»åŠ æ–°çš„LLMæä¾›å•†é€‚é…å™¨",
            "example": "LLMProviderFactory.register_provider('new_provider', NewAdapter)"
        }
    ]
    
    for advantage in advantages:
        print(f"ğŸ“Œ {advantage['title']}")
        print(f"   ğŸ’¡ {advantage['description']}")
        print(f"   ğŸ”§ ç¤ºä¾‹: {advantage['example']}")
        print()


async def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("ğŸš€ AiEnhance LLMé›†æˆæ¼”ç¤º")
    print("=" * 60)
    print("å±•ç¤ºè®°å¿†-è®¤çŸ¥ååŒç³»ç»Ÿä¸LLMçš„æ·±åº¦é›†æˆèƒ½åŠ›")
    print("=" * 60)
    
    try:
        # æ‰§è¡Œå„ä¸ªæ¼”ç¤º
        await demo_system_creation()
        await demo_provider_switching()
        await demo_cognitive_processing()
        await demo_configuration_flexibility()
        await demo_architecture_benefits()
        
        print("\n\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
        print("=" * 60)
        print("âœ… LLMæ¥å£é›†æˆåŠŸèƒ½å·²å®Œå…¨å®ç°")
        print("âœ… æ”¯æŒå¤šç§LLMæä¾›å•†çš„æ— ç¼åˆ‡æ¢")
        print("âœ… æä¾›çµæ´»çš„é…ç½®å’Œæ‰©å±•èƒ½åŠ›")
        print("âœ… æ¶æ„è®¾è®¡ç¬¦åˆå¼€æ”¾-å°é—­åŸåˆ™")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        return False


if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)