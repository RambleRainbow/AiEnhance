# AiEnhance + Ollama ä½¿ç”¨æŒ‡å—

## ğŸ¯ æ¦‚è¿°

æœ¬æŒ‡å—ä»‹ç»å¦‚ä½•å°†AiEnhanceè®°å¿†-è®¤çŸ¥ååŒç³»ç»Ÿä¸Ollama qwen3:8bæ¨¡å‹é›†æˆï¼Œå®ç°å®Œæ•´çš„æ™ºèƒ½å¯¹è¯å’Œè®¤çŸ¥å¢å¼ºåŠŸèƒ½ã€‚

## ğŸ› ï¸ ç¯å¢ƒå‡†å¤‡

### 1. å®‰è£…Ollama

```bash
# macOS
brew install ollama

# Linux
curl -fsSL https://ollama.ai/install.sh | sh

# Windows
# ä¸‹è½½ https://ollama.ai/download/windows
```

### 2. å¯åŠ¨OllamaæœåŠ¡

```bash
ollama serve
```

### 3. å®‰è£…æ¨èæ¨¡å‹

```bash
# å®‰è£…LLMæ¨¡å‹
ollama pull qwen3:8b

# å®‰è£…åµŒå…¥æ¨¡å‹ï¼ˆå¯é€‰ï¼‰
ollama pull bge-m3
```

### 4. éªŒè¯å®‰è£…

```bash
# æ£€æŸ¥å¯ç”¨æ¨¡å‹
ollama list

# æµ‹è¯•æ¨¡å‹
ollama run qwen3:8b "ä½ å¥½"
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### åŸºç¡€ç”¨æ³•

```python
import asyncio
import aienhance

async def basic_usage():
    # åˆ›å»ºç³»ç»Ÿ - æœ€ç®€é…ç½®
    system = aienhance.create_system(
        system_type="educational",    # æ•™è‚²æ¨¡å¼
        llm_provider="ollama",        # ä½¿ç”¨Ollama
        llm_model_name="qwen3:8b"     # qwen3:8bæ¨¡å‹
    )
    
    # å¤„ç†æŸ¥è¯¢
    response = await system.process_query(
        query="ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
        user_id="user123"
    )
    
    # æŸ¥çœ‹ç»“æœ
    print(f"AIå›ç­”: {response.content}")
    print(f"æ€ç»´æ¨¡å¼: {response.user_profile.cognitive.thinking_mode.value}")
    print(f"é€‚é…å¯†åº¦: {response.adaptation_info.density_level.value}")

# è¿è¡Œ
asyncio.run(basic_usage())
```

### å®Œæ•´é…ç½®

```python
import aienhance

# åˆ›å»ºå®Œæ•´é…ç½®çš„ç³»ç»Ÿ
system = aienhance.create_system(
    # ç³»ç»Ÿç±»å‹
    system_type="educational",           # default | educational | research
    
    # LLMé…ç½®
    llm_provider="ollama",
    llm_model_name="qwen3:8b",
    llm_api_base="http://localhost:11434",
    llm_temperature=0.7,                 # 0.0-1.0ï¼Œè¶Šé«˜è¶Šæœ‰åˆ›é€ æ€§
    llm_max_tokens=1000,                 # æœ€å¤§è¾“å‡ºé•¿åº¦
    
    # åµŒå…¥æ¨¡å‹é…ç½®ï¼ˆå¯é€‰ï¼‰
    embedding_provider="ollama",
    embedding_model_name="bge-m3",
    embedding_api_base="http://localhost:11434"
)
```

## ğŸ“š ç³»ç»Ÿé…ç½®ç±»å‹

### 1. é»˜è®¤ç³»ç»Ÿ (default)

```python
system = aienhance.create_system(
    system_type="default",
    llm_provider="ollama",
    llm_model_name="qwen3:8b",
    llm_temperature=0.5              # å¹³è¡¡çš„åˆ›é€ æ€§
)
```

**ç‰¹ç‚¹ï¼š**
- å‡è¡¡çš„è¾“å‡ºå¯†åº¦
- é€‚ä¸­çš„è®¤çŸ¥è´Ÿè·
- é€šç”¨åœºæ™¯é€‚ç”¨

### 2. æ•™è‚²ç³»ç»Ÿ (educational)

```python
system = aienhance.create_system(
    system_type="educational",
    llm_provider="ollama", 
    llm_model_name="qwen3:8b",
    llm_temperature=0.3              # è¾ƒä½æ¸©åº¦ï¼Œæ›´ç¨³å®š
)
```

**ç‰¹ç‚¹ï¼š**
- ä½å¯†åº¦è¾“å‡ºï¼Œæ˜“äºç†è§£
- æ”¯æŒåä½œå±‚åŠŸèƒ½
- å¾ªåºæ¸è¿›çš„è§£é‡Šæ–¹å¼
- è‡ªé€‚åº”éš¾åº¦è°ƒæ•´

### 3. ç ”ç©¶ç³»ç»Ÿ (research)

```python
system = aienhance.create_system(
    system_type="research",
    llm_provider="ollama",
    llm_model_name="qwen3:8b", 
    llm_temperature=0.8              # è¾ƒé«˜æ¸©åº¦ï¼Œæ›´æœ‰åˆ›é€ æ€§
)
```

**ç‰¹ç‚¹ï¼š**
- é«˜å¯†åº¦è¾“å‡ºï¼Œä¿¡æ¯ä¸°å¯Œ
- æ”¯æŒç±»æ¯”æ¨ç†
- åˆ›é€ æ€§å…³è”
- æ·±åº¦åˆ†æèƒ½åŠ›

## ğŸ® å®é™…ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹1ï¼šæ•™è‚²å¯¹è¯

```python
import asyncio
import aienhance

async def educational_chat():
    system = aienhance.create_system(
        system_type="educational",
        llm_provider="ollama",
        llm_model_name="qwen3:8b",
        llm_temperature=0.3
    )
    
    queries = [
        "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿæˆ‘æ˜¯åˆå­¦è€…",
        "ç›‘ç£å­¦ä¹ å’Œæ— ç›‘ç£å­¦ä¹ æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
        "è¯·ä¸¾ä¸ªæ·±åº¦å­¦ä¹ çš„å®é™…åº”ç”¨ä¾‹å­"
    ]
    
    for query in queries:
        response = await system.process_query(
            query=query,
            user_id="student001"
        )
        
        print(f"ğŸ‘¤ å­¦ç”Ÿ: {query}")
        print(f"ğŸ¤– è€å¸ˆ: {response.content}")
        print(f"ğŸ“Š é€‚é…: {response.adaptation_info.density_level.value}å¯†åº¦")
        print("-" * 50)

asyncio.run(educational_chat())
```

### ç¤ºä¾‹2ï¼šç ”ç©¶åŠ©æ‰‹

```python
async def research_assistant():
    system = aienhance.create_system(
        system_type="research",
        llm_provider="ollama",
        llm_model_name="qwen3:8b",
        llm_temperature=0.8
    )
    
    response = await system.process_query(
        query="åˆ†ææ·±åº¦å­¦ä¹ åœ¨åŒ»ç–—å½±åƒè¯Šæ–­ä¸­çš„æœ€æ–°è¿›å±•å’ŒæŒ‘æˆ˜",
        user_id="researcher001",
        context={"domain": "medical_ai"}
    )
    
    print(f"ğŸ”¬ ç ”ç©¶åˆ†æ: {response.content}")
    print(f"ğŸ§  è®¤çŸ¥è´Ÿè·: {response.adaptation_info.cognitive_load:.2f}")

asyncio.run(research_assistant())
```

### ç¤ºä¾‹3ï¼šå¤šè½®å¯¹è¯

```python
async def multi_turn_conversation():
    system = aienhance.create_system(
        system_type="default",
        llm_provider="ollama",
        llm_model_name="qwen3:8b"
    )
    
    user_id = "conversation_user"
    
    conversations = [
        "è¯·ä»‹ç»ä¸€ä¸‹Pythonç¼–ç¨‹è¯­è¨€",
        "Pythonæœ‰å“ªäº›ä¸»è¦çš„åº”ç”¨é¢†åŸŸï¼Ÿ",
        "èƒ½æ¨èä¸€äº›Pythonå­¦ä¹ èµ„æºå—ï¼Ÿ"
    ]
    
    for i, query in enumerate(conversations, 1):
        response = await system.process_query(
            query=query,
            user_id=user_id,
            context={"turn": i}
        )
        
        print(f"ç¬¬{i}è½®å¯¹è¯:")
        print(f"ğŸ‘¤ ç”¨æˆ·: {query}")
        print(f"ğŸ¤– åŠ©æ‰‹: {response.content}")
        print(f"ğŸ“ˆ ç”¨æˆ·ç”»åƒæ›´æ–°: {response.user_profile.cognitive.thinking_mode.value}")
        print()

asyncio.run(multi_turn_conversation())
```

## ğŸ”§ é«˜çº§é…ç½®

### æ€§èƒ½ä¼˜åŒ–

```python
# å¿«é€Ÿå“åº”é…ç½®
fast_system = aienhance.create_system(
    system_type="default",
    llm_provider="ollama",
    llm_model_name="qwen3:8b",
    llm_temperature=0.3,
    llm_max_tokens=300               # é™åˆ¶è¾“å‡ºé•¿åº¦
)

# é«˜è´¨é‡è¾“å‡ºé…ç½®
quality_system = aienhance.create_system(
    system_type="research", 
    llm_provider="ollama",
    llm_model_name="qwen3:8b",
    llm_temperature=0.7,
    llm_max_tokens=2000              # å…è®¸é•¿è¾“å‡º
)
```

### é”™è¯¯å¤„ç†

```python
async def robust_query_processing():
    system = aienhance.create_system(
        system_type="educational",
        llm_provider="ollama",
        llm_model_name="qwen3:8b"
    )
    
    try:
        response = await system.process_query(
            query="å¤æ‚çš„æŠ€æœ¯é—®é¢˜",
            user_id="user123"
        )
        
        if response.content:
            print(f"æˆåŠŸ: {response.content}")
        else:
            print("è­¦å‘Š: æ— å†…å®¹ç”Ÿæˆ")
            
    except Exception as e:
        print(f"é”™è¯¯: {e}")
        # é™çº§å¤„ç†æˆ–é‡è¯•é€»è¾‘
```

## ğŸ“Š æ€§èƒ½ç›‘æ§

```python
import time

async def performance_monitoring():
    system = aienhance.create_system(
        system_type="default",
        llm_provider="ollama", 
        llm_model_name="qwen3:8b"
    )
    
    start_time = time.time()
    
    response = await system.process_query(
        query="æµ‹è¯•æŸ¥è¯¢",
        user_id="perf_user"
    )
    
    end_time = time.time()
    duration = end_time - start_time
    
    print(f"å“åº”æ—¶é—´: {duration:.2f}ç§’")
    print(f"å“åº”é•¿åº¦: {len(response.content)}å­—ç¬¦")
    print(f"ç”Ÿæˆé€Ÿåº¦: {len(response.content)/duration:.1f}å­—ç¬¦/ç§’")
```

## ğŸš¨ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### 1. Ollamaè¿æ¥å¤±è´¥
```
âŒ æ— æ³•è¿æ¥OllamaæœåŠ¡
```

**è§£å†³æ–¹æ¡ˆ:**
```bash
# æ£€æŸ¥Ollamaæ˜¯å¦è¿è¡Œ
curl http://localhost:11434/api/tags

# å¯åŠ¨OllamaæœåŠ¡
ollama serve
```

#### 2. æ¨¡å‹æœªæ‰¾åˆ°
```
âŒ æ¨¡å‹qwen3:8bä¸å­˜åœ¨
```

**è§£å†³æ–¹æ¡ˆ:**
```bash
# æ‹‰å–æ¨¡å‹
ollama pull qwen3:8b

# éªŒè¯æ¨¡å‹
ollama list
```

#### 3. å“åº”ä¸ºç©º
```
âš ï¸ response.contentä¸ºç©º
```

**å¯èƒ½åŸå› :**
- OllamaæœåŠ¡ä¸ç¨³å®š
- æ¨¡å‹åŠ è½½ä¸­
- æŸ¥è¯¢è¿‡äºå¤æ‚

**è§£å†³æ–¹æ¡ˆ:**
- æ£€æŸ¥Ollamaæ—¥å¿—
- ç®€åŒ–æŸ¥è¯¢å†…å®¹
- è°ƒæ•´temperatureå‚æ•°

#### 4. å“åº”é€Ÿåº¦æ…¢
```
âš ï¸ å“åº”æ—¶é—´è¿‡é•¿
```

**ä¼˜åŒ–æ–¹æ¡ˆ:**
- å‡å°‘max_tokenså‚æ•°
- é™ä½temperature
- ä½¿ç”¨æ›´å°çš„æ¨¡å‹

### è°ƒè¯•æ¨¡å¼

```python
# å¯ç”¨è¯¦ç»†æ—¥å¿—
import logging
logging.basicConfig(level=logging.DEBUG)

# åˆ›å»ºç³»ç»Ÿæ—¶å¯ç”¨è°ƒè¯•
system = aienhance.create_system(
    system_type="default",
    llm_provider="ollama",
    llm_model_name="qwen3:8b",
    debug=True  # å¦‚æœæ”¯æŒè°ƒè¯•æ¨¡å¼
)
```

## ğŸ“ æœ€ä½³å®è·µ

### 1. æ¨¡å‹é€‰æ‹©
- **è½»é‡çº§åº”ç”¨**: ä½¿ç”¨è¾ƒå°æ¨¡å‹æˆ–é™ä½max_tokens
- **é«˜è´¨é‡éœ€æ±‚**: ä½¿ç”¨qwen3:8bç­‰å¤§æ¨¡å‹
- **å¤šè¯­è¨€æ”¯æŒ**: é€‰æ‹©æ”¯æŒç›®æ ‡è¯­è¨€çš„æ¨¡å‹

### 2. æ¸©åº¦è®¾ç½®
- **äº‹å®æŸ¥è¯¢**: temperature=0.1-0.3
- **åˆ›æ„å†…å®¹**: temperature=0.7-0.9
- **å¹³è¡¡æ¨¡å¼**: temperature=0.5-0.7

### 3. ç³»ç»Ÿç±»å‹é€‰æ‹©
- **æ•™å­¦åœºæ™¯**: educationalç³»ç»Ÿ
- **ç ”ç©¶åˆ†æ**: researchç³»ç»Ÿ  
- **é€šç”¨å¯¹è¯**: defaultç³»ç»Ÿ

### 4. æ€§èƒ½ä¼˜åŒ–
- åˆç†è®¾ç½®max_tokens
- ä½¿ç”¨è¿æ¥æ± ï¼ˆå¦‚é€‚ç”¨ï¼‰
- å®ç°å“åº”ç¼“å­˜
- ç›‘æ§ç³»ç»Ÿèµ„æº

## ğŸ”— ç›¸å…³é“¾æ¥

- [Ollamaå®˜ç½‘](https://ollama.ai/)
- [qwen3æ¨¡å‹æ–‡æ¡£](https://github.com/QwenLM/Qwen)
- [AiEnhanceé¡¹ç›®README](./README.md)
- [å¼€å‘æ¨¡å¼å¯åŠ¨æŒ‡å—](./start-dev.sh)

---

ğŸ‰ ç°åœ¨æ‚¨å·²ç»å‡†å¤‡å¥½ä½¿ç”¨AiEnhance + Ollamaæ„å»ºæ™ºèƒ½åº”ç”¨äº†ï¼