#!/usr/bin/env python3
"""
AiEnhance Gradioç•Œé¢
æä¾›åˆ†å±‚è®¤çŸ¥ç³»ç»Ÿçš„å¯è§†åŒ–äº¤äº’ç•Œé¢ï¼Œå±•ç¤ºå„å±‚å¤„ç†è¿‡ç¨‹å’ŒMIRIXè®°å¿†ç³»ç»Ÿ
"""

import asyncio
import json
import logging
import traceback
from datetime import datetime
from typing import Any

import gradio as gr
import plotly.graph_objects as go

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from aienhance.core.layered_cognitive_system import LayeredCognitiveSystem
from aienhance.enhanced_system_factory import (
    create_creative_layered_system,
    create_educational_layered_system,
    create_layered_system,
    create_lightweight_layered_system,
    create_research_layered_system,
)
from aienhance.memory.interfaces import MemoryQuery, create_user_context

# MIRIXå‰ç«¯é›†æˆåŠŸèƒ½å·²ç®€åŒ–ï¼Œç§»é™¤å¤æ‚ä¾èµ–

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class LayeredSystemVisualizer:
    """åˆ†å±‚è®¤çŸ¥ç³»ç»Ÿå¯è§†åŒ–å™¨"""

    def __init__(self):
        self.system: LayeredCognitiveSystem | None = None
        self.current_system_type = "educational"
        self.user_context = create_user_context("gradio_user", "gradio_session")

    async def initialize_system(self, system_type: str, llm_provider: str = "ollama",
                              llm_model: str = "qwen3:8b", temperature: float = 0.7) -> str:
        """åˆå§‹åŒ–åˆ†å±‚è®¤çŸ¥ç³»ç»Ÿ"""
        try:
            logger.info(f"åˆå§‹åŒ– {system_type} ç³»ç»Ÿ...")

            # åˆ›å»ºç³»ç»Ÿé…ç½®
            config = {
                "system_type": system_type,
                "llm_provider": llm_provider,
                "llm_model_name": llm_model,
                "llm_temperature": temperature,
                "use_unified_llm": True,
                "memory_system_type": "mirix_unified"
            }

            # æ ¹æ®ç³»ç»Ÿç±»å‹åˆ›å»ºå¯¹åº”çš„ç³»ç»Ÿ
            if system_type == "educational":
                self.system = create_educational_layered_system(**config)
            elif system_type == "research":
                self.system = create_research_layered_system(**config)
            elif system_type == "creative":
                self.system = create_creative_layered_system(**config)
            elif system_type == "lightweight":
                self.system = create_lightweight_layered_system(**config)
            else:
                self.system = create_layered_system(**config)

            # åˆå§‹åŒ–ç³»ç»Ÿ
            await self.system.initialize_layers()

            self.current_system_type = system_type

            return f"âœ… {system_type} ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸï¼\nç³»ç»Ÿä¿¡æ¯ï¼š\n{json.dumps(self.system.get_system_info(), ensure_ascii=False, indent=2)}"

        except Exception as e:
            error_msg = f"âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {str(e)}\n{traceback.format_exc()}"
            logger.error(error_msg)
            return error_msg

    async def process_query_with_layers(self, query: str) -> tuple[str, dict[str, Any]]:
        """å¤„ç†æŸ¥è¯¢å¹¶è¿”å›å„å±‚çš„è¯¦ç»†è¾“å‡º"""
        if not self.system:
            return "âŒ ç³»ç»Ÿæœªåˆå§‹åŒ–ï¼Œè¯·å…ˆåˆå§‹åŒ–ç³»ç»Ÿ", {}

        try:
            logger.info(f"å¤„ç†æŸ¥è¯¢: {query}")

            # å­˜å‚¨å„å±‚è¾“å‡º
            layer_outputs = {
                "perception": {},
                "cognition": {},
                "behavior": {},
                "collaboration": {}
            }

            # 1. æ„ŸçŸ¥å±‚å¤„ç†
            logger.info("ğŸ” æ„ŸçŸ¥å±‚å¤„ç†ä¸­...")
            from aienhance.core.layer_interfaces import PerceptionInput

            perception_input = PerceptionInput(
                query=query,
                user_id=self.user_context.user_id,
                context={"session_id": self.user_context.session_id},
                historical_data=None
            )

            perception_result = await self.system.perception_layer.process(perception_input)

            layer_outputs["perception"] = {
                "è¾“å…¥æŸ¥è¯¢": query,
                "å¤„ç†çŠ¶æ€": perception_result.status.value,
                "ç”¨æˆ·ç”»åƒ": {
                    "ç”¨æˆ·ID": perception_result.user_profile.user_id if perception_result.user_profile else "æœªçŸ¥",
                    "è®¤çŸ¥ç‰¹å¾": perception_result.user_profile.cognitive_characteristics if perception_result.user_profile else {},
                    "çŸ¥è¯†ç”»åƒ": perception_result.user_profile.knowledge_profile if perception_result.user_profile else {}
                } if perception_result.user_profile else {},
                "æƒ…å¢ƒåˆ†æ": {
                    "ä»»åŠ¡ç±»å‹": perception_result.context_profile.task_type if perception_result.context_profile else "æœªçŸ¥",
                    "å¤æ‚åº¦": perception_result.context_profile.complexity_level if perception_result.context_profile else 0,
                    "é¢†åŸŸç‰¹å¾": perception_result.context_profile.domain_characteristics if perception_result.context_profile else {}
                } if perception_result.context_profile else {},
                "æ„ŸçŸ¥æ´å¯Ÿ": perception_result.perception_insights,
                "å¤„ç†æ—¶é—´": perception_result.processing_time
            }

            # 2. è®¤çŸ¥å±‚å¤„ç†
            logger.info("ğŸ§  è®¤çŸ¥å±‚å¤„ç†ä¸­...")
            from aienhance.core.layer_interfaces import CognitionInput

            cognition_input = CognitionInput(
                query=query,
                user_profile=perception_result.user_profile,
                context_profile=perception_result.context_profile,
                external_memories=[],
                perception_insights=perception_result.perception_insights
            )

            cognition_result = await self.system.cognition_layer.process(cognition_input)

            layer_outputs["cognition"] = {
                "å¤„ç†çŠ¶æ€": cognition_result.status.value,
                "è®°å¿†æ¿€æ´»": {
                    "æ¿€æ´»è®°å¿†æ•°": len(cognition_result.memory_activation.activated_fragments) if cognition_result.memory_activation else 0,
                    "æ¿€æ´»ç½®ä¿¡åº¦": cognition_result.memory_activation.activation_confidence if cognition_result.memory_activation else 0,
                    "æ¿€æ´»å…ƒæ•°æ®": len(cognition_result.memory_activation.activation_metadata) if cognition_result.memory_activation else 0
                } if hasattr(cognition_result, 'memory_activation') and cognition_result.memory_activation else {},
                "è¯­ä¹‰å¢å¼º": {
                    "å¢å¼ºå†…å®¹æ•°": len(cognition_result.semantic_enhancement.enhanced_content) if hasattr(cognition_result, 'semantic_enhancement') and cognition_result.semantic_enhancement else 0,
                    "è¯­ä¹‰è¡¥å…¨æ•°": len(cognition_result.semantic_enhancement.semantic_gaps_filled) if hasattr(cognition_result, 'semantic_enhancement') and cognition_result.semantic_enhancement else 0,
                    "å¢å¼ºç½®ä¿¡åº¦": cognition_result.semantic_enhancement.enhancement_confidence if hasattr(cognition_result, 'semantic_enhancement') and cognition_result.semantic_enhancement else 0
                } if hasattr(cognition_result, 'semantic_enhancement') and cognition_result.semantic_enhancement else {},
                "ç±»æ¯”æ¨ç†": {
                    "ç±»æ¯”æ•°é‡": len(cognition_result.analogy_reasoning.analogies) if hasattr(cognition_result, 'analogy_reasoning') and cognition_result.analogy_reasoning else 0,
                    "æ¨ç†é“¾æ•°": len(cognition_result.analogy_reasoning.reasoning_chains) if hasattr(cognition_result, 'analogy_reasoning') and cognition_result.analogy_reasoning else 0,
                    "å¹³å‡ç½®ä¿¡åº¦": sum(cognition_result.analogy_reasoning.confidence_scores) / len(cognition_result.analogy_reasoning.confidence_scores) if hasattr(cognition_result, 'analogy_reasoning') and cognition_result.analogy_reasoning and cognition_result.analogy_reasoning.confidence_scores else 0
                } if hasattr(cognition_result, 'analogy_reasoning') and cognition_result.analogy_reasoning else {},
                "å¤„ç†æ—¶é—´": cognition_result.processing_time
            }

            # 3. è¡Œä¸ºå±‚å¤„ç†
            logger.info("ğŸ¯ è¡Œä¸ºå±‚å¤„ç†ä¸­...")
            from aienhance.core.layer_interfaces import BehaviorInput

            behavior_input = BehaviorInput(
                query=query,
                user_profile=perception_result.user_profile,
                context_profile=perception_result.context_profile,
                cognition_output=cognition_result,
                generation_requirements={"format": "text", "style": "informative"}
            )

            behavior_result = await self.system.behavior_layer.process(behavior_input)

            layer_outputs["behavior"] = {
                "å¤„ç†çŠ¶æ€": behavior_result.status.value,
                "å“åº”ç”Ÿæˆ": {
                    "ç”Ÿæˆå†…å®¹": behavior_result.adapted_content.content[:200] + "..." if hasattr(behavior_result, 'adapted_content') and behavior_result.adapted_content and len(behavior_result.adapted_content.content) > 200 else (behavior_result.adapted_content.content if hasattr(behavior_result, 'adapted_content') and behavior_result.adapted_content else ""),
                    "é€‚é…ç­–ç•¥": behavior_result.adapted_content.adaptation_strategy if hasattr(behavior_result, 'adapted_content') and behavior_result.adapted_content else "é»˜è®¤",
                    "è®¤çŸ¥è´Ÿè·": behavior_result.adapted_content.cognitive_load if hasattr(behavior_result, 'adapted_content') and behavior_result.adapted_content else 0,
                    "ä¿¡æ¯å¯†åº¦": behavior_result.adapted_content.information_density if hasattr(behavior_result, 'adapted_content') and behavior_result.adapted_content else "ä¸­ç­‰",
                    "ä¸ªæ€§åŒ–ç¨‹åº¦": behavior_result.adapted_content.personalization_level if hasattr(behavior_result, 'adapted_content') and behavior_result.adapted_content else 0
                } if hasattr(behavior_result, 'adapted_content') and behavior_result.adapted_content else {},
                "ç”Ÿæˆå…ƒæ•°æ®": behavior_result.generation_metadata if hasattr(behavior_result, 'generation_metadata') else {},
                "è´¨é‡æŒ‡æ ‡": behavior_result.quality_metrics if hasattr(behavior_result, 'quality_metrics') else {},
                "å¤„ç†æ—¶é—´": behavior_result.processing_time
            }

            # 4. åä½œå±‚å¤„ç†ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if hasattr(self.system, 'collaboration_layer') and self.system.collaboration_layer:
                logger.info("ğŸ¤ åä½œå±‚å¤„ç†ä¸­...")
                from aienhance.core.layer_interfaces import CollaborationInput

                collaboration_input = CollaborationInput(
                    query=query,
                    user_profile=perception_result.user_profile,
                    context_profile=perception_result.context_profile,
                    behavior_output=behavior_result,
                    collaboration_context={"mode": "enhancement", "perspectives": ["analytical", "creative"]}
                )

                collaboration_result = await self.system.collaboration_layer.process(collaboration_input)

                layer_outputs["collaboration"] = {
                    "å¤„ç†çŠ¶æ€": collaboration_result.status.value,
                    "å¤šè§’åº¦åˆ†æ": {
                        "ç”Ÿæˆè§†è§’æ•°": len(collaboration_result.perspective_generation.perspectives) if hasattr(collaboration_result, 'perspective_generation') and collaboration_result.perspective_generation else 0,
                        "è§†è§’å¤šæ ·æ€§": collaboration_result.perspective_generation.perspective_diversity if hasattr(collaboration_result, 'perspective_generation') and collaboration_result.perspective_generation else 0,
                        "ç”Ÿæˆå…ƒæ•°æ®": len(collaboration_result.perspective_generation.generation_metadata) if hasattr(collaboration_result, 'perspective_generation') and collaboration_result.perspective_generation else 0
                    } if hasattr(collaboration_result, 'perspective_generation') and collaboration_result.perspective_generation else {},
                    "è®¤çŸ¥æŒ‘æˆ˜": {
                        "æŒ‘æˆ˜æ•°é‡": len(collaboration_result.cognitive_challenge.challenges) if hasattr(collaboration_result, 'cognitive_challenge') and collaboration_result.cognitive_challenge else 0,
                        "æŒ‘æˆ˜å¼ºåº¦": collaboration_result.cognitive_challenge.challenge_intensity if hasattr(collaboration_result, 'cognitive_challenge') and collaboration_result.cognitive_challenge else 0,
                        "æ•™è‚²ä»·å€¼": collaboration_result.cognitive_challenge.educational_value if hasattr(collaboration_result, 'cognitive_challenge') and collaboration_result.cognitive_challenge else 0
                    } if hasattr(collaboration_result, 'cognitive_challenge') and collaboration_result.cognitive_challenge else {},
                    "åä½œå¢å¼º": collaboration_result.enhanced_content[:200] + "..." if hasattr(collaboration_result, 'enhanced_content') and collaboration_result.enhanced_content and len(collaboration_result.enhanced_content) > 200 else (collaboration_result.enhanced_content if hasattr(collaboration_result, 'enhanced_content') else ""),
                    "å¤„ç†æ—¶é—´": collaboration_result.processing_time
                }
            else:
                layer_outputs["collaboration"] = {"çŠ¶æ€": "åä½œå±‚æœªå¯ç”¨æˆ–ä¸å¯ç”¨"}

            # è·å–æœ€ç»ˆå“åº”
            if hasattr(behavior_result, 'adapted_content') and behavior_result.adapted_content:
                final_response = behavior_result.adapted_content.content
            else:
                final_response = "å¤„ç†å®Œæˆï¼Œä½†æ— æ³•è·å–ç”Ÿæˆå†…å®¹"

            return final_response, layer_outputs

        except Exception as e:
            error_msg = f"âŒ æŸ¥è¯¢å¤„ç†å¤±è´¥: {str(e)}\n{traceback.format_exc()}"
            logger.error(error_msg)
            return error_msg, {}

    async def get_memory_status(self) -> dict[str, Any]:
        """è·å–MIRIXè®°å¿†ç³»ç»ŸçŠ¶æ€"""
        if not self.system or not self.system.memory_system:
            return {"çŠ¶æ€": "è®°å¿†ç³»ç»Ÿæœªåˆå§‹åŒ–"}

        try:
            # è·å–ç³»ç»Ÿä¿¡æ¯
            system_info = self.system.memory_system.get_system_info()

            # è·å–ç”¨æˆ·è®°å¿†ç»Ÿè®¡
            user_memories = await self.system.memory_system.get_user_memories(
                self.user_context, limit=100
            )

            # æŒ‰è®°å¿†ç±»å‹åˆ†ç»„ç»Ÿè®¡
            memory_stats = {}
            for memory in user_memories.memories:
                mem_type = memory.memory_type.value
                if mem_type not in memory_stats:
                    memory_stats[mem_type] = 0
                memory_stats[mem_type] += 1

            return {
                "ç³»ç»Ÿç±»å‹": system_info.get("system_type", "unknown"),
                "åˆå§‹åŒ–çŠ¶æ€": system_info.get("initialized", False),
                "LLMæ¨¡å¼": system_info.get("mode", "unknown"),
                "LLMæä¾›å•†": system_info.get("llm_provider", {}),
                "è®°å¿†æ€»æ•°": user_memories.total_count,
                "è®°å¿†ç±»å‹ç»Ÿè®¡": memory_stats,
                "æŸ¥è¯¢æ—¶é—´": f"{user_memories.query_time:.3f}ç§’",
                "æ”¯æŒçš„åŠŸèƒ½": system_info.get("features", {})
            }

        except Exception as e:
            return {"é”™è¯¯": f"è·å–è®°å¿†çŠ¶æ€å¤±è´¥: {str(e)}"}


# å…¨å±€å¯è§†åŒ–å™¨å®ä¾‹
visualizer = LayeredSystemVisualizer()


def sync_initialize_system(system_type: str, llm_provider: str, llm_model: str, temperature: float) -> str:
    """åŒæ­¥åŒ…è£…å™¨ç”¨äºåˆå§‹åŒ–ç³»ç»Ÿ"""
    return asyncio.run(visualizer.initialize_system(system_type, llm_provider, llm_model, temperature))


def sync_process_query_stream(query: str):
    """æµå¼å¤„ç†æŸ¥è¯¢çš„ç”Ÿæˆå™¨ - æ–°çš„é»˜è®¤å¤„ç†æ–¹å¼"""
    if not query.strip():
        yield "âŒ è¯·è¾“å…¥æŸ¥è¯¢é—®é¢˜"
        return

    try:
        # ä½¿ç”¨å¼‚æ­¥ç”Ÿæˆå™¨è¿›è¡Œæµå¼å¤„ç†
        async def _process():
            if not visualizer.system:
                yield "âŒ ç³»ç»Ÿæœªåˆå§‹åŒ–ï¼Œè¯·å…ˆåˆå§‹åŒ–ç³»ç»Ÿ"
                return

            layer_info = {
                "perception": "",
                "cognition": "",
                "behavior": "",
                "collaboration": ""
            }

            content_parts = []
            current_layer = ""

            async for chunk in visualizer.system.process_stream(
                query=query,
                user_id="gradio_user",
                context={"source": "gradio", "timestamp": datetime.now().isoformat()}
            ):
                # è¯†åˆ«å½“å‰å¤„ç†å±‚
                if "æ„ŸçŸ¥å±‚" in chunk:
                    current_layer = "perception"
                    layer_info[current_layer] += chunk
                elif "è®¤çŸ¥å±‚" in chunk:
                    current_layer = "cognition"
                    layer_info[current_layer] += chunk
                elif "è¡Œä¸ºå±‚" in chunk:
                    current_layer = "behavior"
                    layer_info[current_layer] += chunk
                elif "åä½œå±‚" in chunk:
                    current_layer = "collaboration"
                    layer_info[current_layer] += chunk
                elif chunk.startswith(("ğŸš€", "âœ…", "âŒ", "âš ï¸", "ğŸ¯")):
                    # ç³»ç»ŸçŠ¶æ€ä¿¡æ¯
                    if current_layer:
                        layer_info[current_layer] += chunk
                    yield chunk
                else:
                    # AIç”Ÿæˆçš„å†…å®¹
                    content_parts.append(chunk)
                    yield chunk

            # å¤„ç†å®Œæˆåè¿”å›å±‚çº§ä¿¡æ¯
            yield "\n\nğŸ“Š **å¤„ç†å±‚çº§ä¿¡æ¯:**\n"
            for layer, info in layer_info.items():
                if info:
                    yield f"\n**{layer.title()}å±‚:** {info.strip()}\n"

            yield f"\nğŸ“ˆ **æµå¼è¾“å‡ºç»Ÿè®¡:** æ€»è®¡{len(''.join(content_parts))}å­—ç¬¦\n"

        # è¿è¡Œå¼‚æ­¥ç”Ÿæˆå™¨
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            async_gen = _process()
            while True:
                try:
                    chunk = loop.run_until_complete(async_gen.__anext__())
                    yield chunk
                except StopAsyncIteration:
                    break
        finally:
            loop.close()

    except Exception as e:
        yield f"âŒ æµå¼å¤„ç†å¼‚å¸¸: {str(e)}"


def sync_process_query(query: str) -> tuple[str, str, str, str, str, str]:
    """åŒæ­¥åŒ…è£…å™¨ç”¨äºå¤„ç†æŸ¥è¯¢ - ä¿æŒå‘åå…¼å®¹"""
    if not query.strip():
        return "âŒ è¯·è¾“å…¥æŸ¥è¯¢é—®é¢˜", "", "", "", "", ""

    try:
        # æ”¶é›†æµå¼è¾“å‡ºä½œä¸ºæœ€ç»ˆå“åº”
        final_response = ""
        for chunk in sync_process_query_stream(query):
            final_response += chunk

        # ç®€åŒ–çš„å±‚çº§è¾“å‡ºï¼ˆæµå¼æ¨¡å¼ä¸‹å±‚çº§ä¿¡æ¯å·²åŒ…å«åœ¨å“åº”ä¸­ï¼‰
        layer_status = {
            "perception": {"çŠ¶æ€": "âœ… å·²å®Œæˆæµå¼å¤„ç†"},
            "cognition": {"çŠ¶æ€": "âœ… å·²å®Œæˆæµå¼å¤„ç†"},
            "behavior": {"çŠ¶æ€": "âœ… å·²å®Œæˆæµå¼å¤„ç†"},
            "collaboration": {"çŠ¶æ€": "âœ… å·²å®Œæˆæµå¼å¤„ç†"}
        }

        perception_output = json.dumps(layer_status["perception"], ensure_ascii=False, indent=2)
        cognition_output = json.dumps(layer_status["cognition"], ensure_ascii=False, indent=2)
        behavior_output = json.dumps(layer_status["behavior"], ensure_ascii=False, indent=2)
        collaboration_output = json.dumps(layer_status["collaboration"], ensure_ascii=False, indent=2)

        return final_response, perception_output, cognition_output, behavior_output, collaboration_output, "âœ… æµå¼å¤„ç†å®Œæˆ"

    except Exception as e:
        error_msg = f"âŒ æŸ¥è¯¢å¤„ç†å¼‚å¸¸: {str(e)}"
        return error_msg, "", "", "", "", error_msg


def sync_get_memory_status() -> str:
    """åŒæ­¥åŒ…è£…å™¨ç”¨äºè·å–è®°å¿†çŠ¶æ€"""
    try:
        memory_status = asyncio.run(visualizer.get_memory_status())
        return json.dumps(memory_status, ensure_ascii=False, indent=2)
    except Exception as e:
        return f"âŒ è·å–è®°å¿†çŠ¶æ€å¤±è´¥: {str(e)}"


def create_gradio_interface():
    """åˆ›å»ºGradioç•Œé¢"""

    with gr.Blocks(
        title="AiEnhance - åˆ†å±‚è®¤çŸ¥ç³»ç»Ÿå¯è§†åŒ–ç•Œé¢",
        theme=gr.themes.Soft(),
        css="""
        .layer-output {
            border: 1px solid #ddd;
            border-radius: 8px;
            padding: 12px;
            margin: 8px 0;
            background-color: #f9f9f9;
        }
        .memory-panel {
            border: 1px solid #007acc;
            border-radius: 8px;
            padding: 12px;
            background-color: #f0f8ff;
        }
        """
    ) as interface:

        gr.Markdown("# ğŸ§  AiEnhance - åˆ†å±‚è®¤çŸ¥ç³»ç»Ÿå¯è§†åŒ–ç•Œé¢")
        gr.Markdown("æ¢ç´¢æ„ŸçŸ¥å±‚â†’è®¤çŸ¥å±‚â†’è¡Œä¸ºå±‚â†’åä½œå±‚çš„å®Œæ•´å¤„ç†æµç¨‹ï¼Œå¹¶å¯è§†åŒ–MIRIXè®°å¿†ç³»ç»Ÿ")

        with gr.Tab("ğŸ›ï¸ ç³»ç»Ÿé…ç½®"):
            gr.Markdown("## ç³»ç»Ÿåˆå§‹åŒ–é…ç½®")

            with gr.Row():
                with gr.Column(scale=1):
                    system_type = gr.Dropdown(
                        choices=["educational", "research", "creative", "lightweight"],
                        value="educational",
                        label="ğŸ¯ ç³»ç»Ÿç±»å‹",
                        info="é€‰æ‹©é€‚åˆçš„è®¤çŸ¥ç³»ç»Ÿç±»å‹"
                    )

                    llm_provider = gr.Dropdown(
                        choices=["ollama", "openai", "anthropic", "google_ai"],
                        value="ollama",
                        label="ğŸ¤– LLMæä¾›å•†",
                        info="é€‰æ‹©å¤§è¯­è¨€æ¨¡å‹æä¾›å•†"
                    )

                with gr.Column(scale=1):
                    llm_model = gr.Textbox(
                        value="qwen3:8b",
                        label="ğŸ“¦ æ¨¡å‹åç§°",
                        info="è¾“å…¥å…·ä½“çš„æ¨¡å‹åç§°"
                    )

                    temperature = gr.Slider(
                        minimum=0.0,
                        maximum=1.0,
                        value=0.7,
                        step=0.1,
                        label="ğŸŒ¡ï¸ æ¸©åº¦å‚æ•°",
                        info="æ§åˆ¶è¾“å‡ºçš„éšæœºæ€§"
                    )

            with gr.Row():
                init_btn = gr.Button("ğŸš€ åˆå§‹åŒ–ç³»ç»Ÿ", variant="primary", size="lg")

            init_status = gr.Textbox(
                label="ğŸ“Š åˆå§‹åŒ–çŠ¶æ€",
                lines=10,
                max_lines=20,
                interactive=False
            )

        with gr.Tab("ğŸ’¬ åˆ†å±‚å¤„ç†å¯è§†åŒ–"):
            gr.Markdown("## æŸ¥è¯¢å¤„ç†å’Œåˆ†å±‚è¾“å‡ºå±•ç¤º")

            with gr.Row():
                with gr.Column(scale=2):
                    query_input = gr.Textbox(
                        label="â“ è¾“å…¥æŸ¥è¯¢é—®é¢˜",
                        placeholder="ä¾‹å¦‚ï¼šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿè¯·è§£é‡Šæ·±åº¦å­¦ä¹ çš„åŸç†ã€‚",
                        lines=3
                    )

                    with gr.Row():
                        process_btn = gr.Button("ğŸ”„ å¼€å§‹å¤„ç†", variant="primary", size="lg")
                        stream_toggle = gr.Checkbox(
                            label="âš¡ æµå¼è¾“å‡º",
                            value=True,
                            info="é»˜è®¤å¯ç”¨æµå¼è¾“å‡ºä»¥è·å¾—æ›´å¥½ä½“éªŒ"
                        )

                    process_status = gr.Textbox(
                        label="âš¡ å¤„ç†çŠ¶æ€",
                        lines=2,
                        interactive=False
                    )

            with gr.Row():
                final_response = gr.Textbox(
                    label="ğŸ’¡ å®æ—¶å“åº”ï¼ˆæµå¼è¾“å‡ºï¼‰",
                    lines=8,
                    max_lines=15,
                    interactive=False,
                    info="AIå›ç­”å°†åœ¨æ­¤å¤„å®æ—¶æ˜¾ç¤º"
                )

            with gr.Row():
                with gr.Column(scale=1):
                    perception_output = gr.Code(
                        label="ğŸ” æ„ŸçŸ¥å±‚è¾“å‡º",
                        language="json",
                        lines=12
                    )

                with gr.Column(scale=1):
                    cognition_output = gr.Code(
                        label="ğŸ§  è®¤çŸ¥å±‚è¾“å‡º",
                        language="json",
                        lines=12
                    )

            with gr.Row():
                with gr.Column(scale=1):
                    behavior_output = gr.Code(
                        label="ğŸ¯ è¡Œä¸ºå±‚è¾“å‡º",
                        language="json",
                        lines=12
                    )

                with gr.Column(scale=1):
                    collaboration_output = gr.Code(
                        label="ğŸ¤ åä½œå±‚è¾“å‡º",
                        language="json",
                        lines=12
                    )

        with gr.Tab("ğŸ§  MIRIXè®°å¿†ç³»ç»Ÿ"):
            gr.Markdown("## MIRIXè®°å¿†ç³»ç»ŸçŠ¶æ€å’Œå¯è§†åŒ–")

            with gr.Row():
                with gr.Column(scale=1):
                    refresh_memory_btn = gr.Button("ğŸ”„ åˆ·æ–°è®°å¿†çŠ¶æ€", variant="secondary")
                    dashboard_btn = gr.Button("ğŸ“Š ç”Ÿæˆè®°å¿†ä»ªè¡¨æ¿", variant="primary")

                with gr.Column(scale=1):
                    search_query = gr.Textbox(
                        label="ğŸ” æœç´¢è®°å¿†",
                        placeholder="è¾“å…¥å…³é”®è¯æœç´¢ç›¸å…³è®°å¿†..."
                    )
                    search_btn = gr.Button("ğŸ” æœç´¢", variant="secondary")

            with gr.Row():
                with gr.Column(scale=1):
                    memory_status = gr.Code(
                        label="ğŸ“Š è®°å¿†ç³»ç»ŸçŠ¶æ€",
                        language="json",
                        lines=15
                    )

                with gr.Column(scale=1):
                    search_results = gr.Code(
                        label="ğŸ” æœç´¢ç»“æœ",
                        language="json",
                        lines=15
                    )

            with gr.Tab("ğŸ“ˆ è®°å¿†åˆ†æå›¾è¡¨"):
                gr.Markdown("### è®°å¿†ç±»å‹åˆ†å¸ƒ")
                memory_type_plot = gr.Plot(label="è®°å¿†ç±»å‹åˆ†å¸ƒå›¾")

                gr.Markdown("### è®°å¿†æ—¶é—´çº¿")
                memory_timeline_plot = gr.Plot(label="è®°å¿†åˆ›å»ºæ—¶é—´çº¿")

                gr.Markdown("### ç½®ä¿¡åº¦åˆ†æ")
                memory_confidence_plot = gr.Plot(label="è®°å¿†ç½®ä¿¡åº¦åˆ†æ")

                gr.Markdown("### è®°å¿†å…³ç³»ç½‘ç»œ")
                memory_network_plot = gr.Plot(label="è®°å¿†å…³ç³»ç½‘ç»œå›¾")

            with gr.Tab("ğŸŒ MIRIX Webç•Œé¢"):
                gr.Markdown("### MIRIXåŸç”ŸWebç•Œé¢é›†æˆ")

                with gr.Row():
                    mirix_url = gr.Textbox(
                        value="http://localhost:3000",
                        label="ğŸŒ MIRIX Webç•Œé¢URL",
                        info="è¯·ç¡®ä¿MIRIXæœåŠ¡å·²å¯åŠ¨"
                    )
                    load_iframe_btn = gr.Button("ğŸš€ åŠ è½½MIRIXç•Œé¢", variant="primary")

                mirix_iframe = gr.HTML(
                    label="MIRIX Webç•Œé¢",
                    value="<p>ç‚¹å‡»ä¸Šæ–¹æŒ‰é’®åŠ è½½MIRIXç•Œé¢</p>"
                )

                gr.Markdown("### ğŸ’¡ MIRIXå‰ç«¯é›†æˆè¯´æ˜")
                gr.Markdown("""
                **é›†æˆMIRIXé¡¹ç›®å‰ç«¯çš„æ­¥éª¤:**
                
                1. **å®‰è£…MIRIX SDK**: `pip install mirix`
                2. **å¯åŠ¨MIRIXæœåŠ¡**: æ ¹æ®MIRIXæ–‡æ¡£å¯åŠ¨åç«¯æœåŠ¡
                3. **å¯åŠ¨Webç•Œé¢**: é€šå¸¸è¿è¡Œ `mirix serve` æˆ–è®¿é—®ç®¡ç†ç•Œé¢
                4. **é…ç½®URL**: åœ¨ä¸Šæ–¹è¾“å…¥æ¡†ä¸­è®¾ç½®æ­£ç¡®çš„MIRIX Webç•Œé¢åœ°å€
                
                **å½“å‰çŠ¶æ€**: ä½¿ç”¨MIRIXç»Ÿä¸€é€‚é…å™¨ï¼Œæ”¯æŒé¡¹ç›®çš„LLMæŠ½è±¡å±‚
                
                **æ”¯æŒçš„åŠŸèƒ½:**
                - âœ… è®°å¿†ç±»å‹å¯è§†åŒ–åˆ†æ
                - âœ… æ—¶é—´çº¿å›¾è¡¨
                - âœ… ç½®ä¿¡åº¦åˆ†æ
                - âœ… è®°å¿†æœç´¢åŠŸèƒ½
                - âœ… åŸç”ŸWebç•Œé¢åµŒå…¥
                """)

        with gr.Tab("ğŸ“Š ç³»ç»Ÿç›‘æ§"):
            gr.Markdown("## ç³»ç»Ÿæ€§èƒ½å’ŒçŠ¶æ€ç›‘æ§")

            with gr.Row():
                monitor_btn = gr.Button("ğŸ“ˆ è·å–ç³»ç»ŸçŠ¶æ€", variant="secondary")

            system_monitor = gr.Code(
                label="ğŸ” ç³»ç»Ÿç›‘æ§ä¿¡æ¯",
                language="json",
                lines=10
            )

        # ç»‘å®šäº‹ä»¶å¤„ç†å™¨
        init_btn.click(
            fn=sync_initialize_system,
            inputs=[system_type, llm_provider, llm_model, temperature],
            outputs=init_status
        )

        # å®šä¹‰å¤„ç†å‡½æ•°é€‰æ‹©å™¨
        def process_query_handler(query: str, use_stream: bool):
            """æ ¹æ®ç”¨æˆ·é€‰æ‹©ä½¿ç”¨æµå¼æˆ–ä¼ ç»Ÿå¤„ç†"""
            if use_stream:
                # æµå¼å¤„ç† - è¿”å›ç”Ÿæˆå™¨çš„å®Œæ•´ç»“æœ
                full_response = ""
                for chunk in sync_process_query_stream(query):
                    full_response += chunk

                # ç®€åŒ–çš„å±‚çº§çŠ¶æ€
                simple_status = {"çŠ¶æ€": "âœ… æµå¼å¤„ç†å®Œæˆ"}
                status_json = json.dumps(simple_status, ensure_ascii=False, indent=2)

                return (
                    full_response,
                    status_json,  # perception
                    status_json,  # cognition
                    status_json,  # behavior
                    status_json,  # collaboration
                    "âœ… æµå¼å¤„ç†å®Œæˆ"
                )
            else:
                # ä¼ ç»Ÿå¤„ç†
                return sync_process_query(query)

        process_btn.click(
            fn=process_query_handler,
            inputs=[query_input, stream_toggle],
            outputs=[final_response, perception_output, cognition_output,
                    behavior_output, collaboration_output, process_status]
        )

        refresh_memory_btn.click(
            fn=sync_get_memory_status,
            outputs=memory_status
        )

        # è®°å¿†æœç´¢åŠŸèƒ½
        def sync_search_memories(query: str) -> str:
            if not query.strip():
                return "è¯·è¾“å…¥æœç´¢å…³é”®è¯"

            try:
                if visualizer.system and visualizer.system.memory_system:
                    # ç®€åŒ–çš„è®°å¿†æœç´¢å®ç°
                    user_context = create_user_context("gradio_user", "search_session")
                    search_query = MemoryQuery(
                        query=query,
                        limit=10,
                        memory_types=None
                    )
                    results = asyncio.run(visualizer.system.memory_system.search_memories(
                        user_context, search_query
                    ))

                    if results.memories:
                        summary = f"æ‰¾åˆ° {len(results.memories)} ä¸ªç›¸å…³è®°å¿†:\n\n"
                        for i, memory in enumerate(results.memories[:5], 1):
                            summary += f"{i}. {memory.content[:100]}...\n"
                        return summary
                    else:
                        return "æœªæ‰¾åˆ°ç›¸å…³è®°å¿†"
                else:
                    return "è®°å¿†ç³»ç»Ÿæœªåˆå§‹åŒ–"
            except Exception as e:
                return f"æœç´¢å¤±è´¥: {str(e)}"

        search_btn.click(
            fn=sync_search_memories,
            inputs=search_query,
            outputs=search_results
        )

        # è®°å¿†ä»ªè¡¨æ¿ç”Ÿæˆ
        def sync_generate_dashboard() -> tuple[go.Figure, go.Figure, go.Figure, go.Figure]:
            try:
                if visualizer.system and visualizer.system.memory_system:
                    # ç®€åŒ–çš„ä»ªè¡¨æ¿ç”Ÿæˆ
                    user_context = create_user_context("gradio_user", "dashboard_session")
                    memories = asyncio.run(visualizer.system.memory_system.get_user_memories(
                        user_context, limit=100
                    ))

                    # åˆ›å»ºç®€å•çš„ç»Ÿè®¡å›¾è¡¨
                    # è®°å¿†ç±»å‹åˆ†å¸ƒå›¾
                    type_counts = {}
                    for memory in memories.memories:
                        mem_type = memory.memory_type.value
                        type_counts[mem_type] = type_counts.get(mem_type, 0) + 1

                    type_fig = go.Figure(data=[go.Pie(
                        labels=list(type_counts.keys()),
                        values=list(type_counts.values()),
                        title="è®°å¿†ç±»å‹åˆ†å¸ƒ"
                    )])

                    # ç®€å•çš„æ—¶é—´çº¿å›¾
                    timeline_fig = go.Figure()
                    timeline_fig.add_trace(go.Scatter(
                        x=list(range(len(memories.memories))),
                        y=[1] * len(memories.memories),
                        mode='markers',
                        name='è®°å¿†åˆ›å»ºæ—¶é—´çº¿'
                    ))
                    timeline_fig.update_layout(title="è®°å¿†æ—¶é—´çº¿")

                    # ç½®ä¿¡åº¦åˆ†æ
                    confidences = [memory.confidence for memory in memories.memories if hasattr(memory, 'confidence')]
                    conf_fig = go.Figure(data=[go.Histogram(x=confidences, title="ç½®ä¿¡åº¦åˆ†å¸ƒ")])

                    # ç®€å•çš„ç½‘ç»œå›¾ï¼ˆå ä½ï¼‰
                    network_fig = go.Figure()
                    network_fig.add_annotation(
                        text="è®°å¿†å…³ç³»ç½‘ç»œå›¾ï¼ˆå¼€å‘ä¸­ï¼‰",
                        xref="paper", yref="paper",
                        x=0.5, y=0.5, showarrow=False
                    )

                    return type_fig, timeline_fig, conf_fig, network_fig
                else:
                    empty_fig = go.Figure()
                    empty_fig.add_annotation(
                        text="è®°å¿†ç³»ç»Ÿæœªåˆå§‹åŒ–",
                        xref="paper", yref="paper",
                        x=0.5, y=0.5, showarrow=False
                    )
                    return empty_fig, empty_fig, empty_fig, empty_fig

            except Exception as e:
                error_fig = go.Figure()
                error_fig.add_annotation(
                    text=f"ç”Ÿæˆå›¾è¡¨å¤±è´¥: {str(e)}",
                    xref="paper", yref="paper",
                    x=0.5, y=0.5, showarrow=False
                )
                return error_fig, error_fig, error_fig, error_fig

        dashboard_btn.click(
            fn=sync_generate_dashboard,
            outputs=[memory_type_plot, memory_timeline_plot, memory_confidence_plot, memory_network_plot]
        )

        # MIRIX Webç•Œé¢åŠ è½½
        def load_mirix_iframe(url: str) -> str:
            if not url.strip():
                return "<p>è¯·è¾“å…¥æœ‰æ•ˆçš„MIRIX Webç•Œé¢URL</p>"

            iframe_html = f"""
            <iframe 
                src="{url}"
                width="100%" 
                height="600px"
                frameborder="0"
                style="border-radius: 8px; border: 1px solid #ddd;"
                sandbox="allow-scripts allow-same-origin allow-forms"
            >
                <p>æ— æ³•åŠ è½½MIRIXç•Œé¢ã€‚è¯·ç¡®ä¿:</p>
                <ul>
                    <li>MIRIXæœåŠ¡æ­£åœ¨è¿è¡Œ</li>
                    <li>URLåœ°å€æ­£ç¡®: <a href="{url}" target="_blank">{url}</a></li>
                    <li>ç½‘ç»œè¿æ¥æ­£å¸¸</li>
                </ul>
            </iframe>
            """
            return iframe_html

        load_iframe_btn.click(
            fn=load_mirix_iframe,
            inputs=mirix_url,
            outputs=mirix_iframe
        )

        # ç›‘æ§æŒ‰é’®å¤„ç†
        def get_system_monitor():
            try:
                if visualizer.system:
                    info = visualizer.system.get_system_info()
                    return json.dumps(info, ensure_ascii=False, indent=2)
                else:
                    return "ç³»ç»Ÿæœªåˆå§‹åŒ–"
            except Exception as e:
                return f"è·å–ç³»ç»Ÿä¿¡æ¯å¤±è´¥: {str(e)}"

        monitor_btn.click(
            fn=get_system_monitor,
            outputs=system_monitor
        )

        # ç¤ºä¾‹æŸ¥è¯¢æŒ‰é’®
        with gr.Row():
            example_queries = [
                "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿè¯·ç»™å‡ºè¯¦ç»†è§£é‡Šã€‚",
                "æ·±åº¦å­¦ä¹ å’Œæœºå™¨å­¦ä¹ æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
                "å¦‚ä½•è®¾è®¡ä¸€ä¸ªæ¨èç³»ç»Ÿï¼Ÿ",
                "è¯·è§£é‡Šæ³¨æ„åŠ›æœºåˆ¶çš„å·¥ä½œåŸç†ã€‚"
            ]

            for i, example in enumerate(example_queries):
                btn = gr.Button(f"ç¤ºä¾‹ {i+1}", variant="secondary", size="sm")
                btn.click(lambda x=example: x, outputs=query_input)

    return interface


def main():
    """ä¸»å‡½æ•°"""
    logger.info("å¯åŠ¨ AiEnhance Gradio ç•Œé¢...")

    # åˆ›å»ºç•Œé¢
    interface = create_gradio_interface()

    # å¯åŠ¨æœåŠ¡
    interface.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        debug=True,
        show_error=True
    )


if __name__ == "__main__":
    main()
