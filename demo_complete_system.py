#!/usr/bin/env python3
"""
AiEnhance å®Œæ•´ç³»ç»Ÿæ¼”ç¤º
å±•ç¤ºå¦‚ä½•é…ç½®å’Œä½¿ç”¨å®Œæ•´çš„è®°å¿†-è®¤çŸ¥ååŒç³»ç»Ÿï¼Œé›†æˆOllama qwen3:8bæ¨¡å‹
"""

import asyncio
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

import aienhance


async def demo_complete_system_setup():
    """æ¼”ç¤ºå®Œæ•´ç³»ç»Ÿé…ç½®"""
    print("ğŸ› ï¸ å®Œæ•´ç³»ç»Ÿé…ç½®æ¼”ç¤º")
    print("=" * 60)
    
    # 1. åˆ›å»ºå¸¦Ollama LLMçš„å®Œæ•´ç³»ç»Ÿ
    print("\n1ï¸âƒ£ åˆ›å»ºå®Œæ•´é…ç½®ç³»ç»Ÿï¼ˆLLM + åµŒå…¥ï¼‰")
    system = aienhance.create_system(
        system_type="educational",
        # LLMé…ç½®
        llm_provider="ollama",
        llm_model_name="qwen3:8b",
        llm_api_base="http://localhost:11434",
        llm_temperature=0.7,
        llm_max_tokens=800,
        # åµŒå…¥æ¨¡å‹é…ç½®
        embedding_provider="ollama", 
        embedding_model_name="bge-m3",
        embedding_api_base="http://localhost:11434"
    )
    
    print(f"   âœ… ç³»ç»Ÿåˆ›å»ºæˆåŠŸ")
    
    # 2. æ£€æŸ¥ç³»ç»ŸçŠ¶æ€
    status = system.get_system_status()
    print(f"\nğŸ“Š ç³»ç»ŸçŠ¶æ€:")
    print(f"   â€¢ ç³»ç»Ÿç±»å‹: {status.get('config', {})}")
    print(f"   â€¢ LLMåˆå§‹åŒ–: {status.get('llm_initialized', False)}")
    print(f"   â€¢ åµŒå…¥æ¨¡å‹åˆå§‹åŒ–: {status.get('embedding_initialized', False)}")
    print(f"   â€¢ è®°å¿†ç³»ç»Ÿåˆå§‹åŒ–: {status.get('memory_initialized', False)}")
    
    return system


async def demo_intelligent_conversation(system):
    """æ¼”ç¤ºæ™ºèƒ½å¯¹è¯åŠŸèƒ½"""
    print("\n\nğŸ’¬ æ™ºèƒ½å¯¹è¯æ¼”ç¤º")
    print("=" * 60)
    
    # æµ‹è¯•æŸ¥è¯¢åˆ—è¡¨
    test_queries = [
        {
            "query": "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ",
            "user_id": "student_001",
            "context": {"session_id": "learning_session_1"}
        },
        {
            "query": "æ·±åº¦å­¦ä¹ å’Œæœºå™¨å­¦ä¹ æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
            "user_id": "student_001", 
            "context": {"session_id": "learning_session_1"}
        },
        {
            "query": "è¯·ä¸¾ä¸ªç¥ç»ç½‘ç»œçš„å®é™…åº”ç”¨ä¾‹å­",
            "user_id": "student_001",
            "context": {"session_id": "learning_session_1"}
        }
    ]
    
    print(f"ğŸ” æ¨¡æ‹Ÿæ™ºèƒ½å¯¹è¯ï¼ˆå…±{len(test_queries)}ä¸ªæŸ¥è¯¢ï¼‰")
    
    for i, test in enumerate(test_queries, 1):
        print(f"\n--- å¯¹è¯è½®æ¬¡ {i} ---")
        print(f"ğŸ‘¤ ç”¨æˆ·: {test['query']}")
        
        try:
            # å¤„ç†æŸ¥è¯¢
            response = await system.process_query(**test)
            
            # æ˜¾ç¤ºç³»ç»Ÿå“åº”
            print(f"ğŸ¤– ç³»ç»Ÿå›ç­”:")
            print(f"   {'-' * 50}")
            if response.content:
                print(f"   {response.content}")
            else:
                print(f"   (æ— å†…å®¹ç”Ÿæˆ - æ£€æŸ¥LLMé…ç½®)")
            print(f"   {'-' * 50}")
            
            # æ˜¾ç¤ºå¤„ç†ä¿¡æ¯
            if hasattr(response, 'processing_metadata'):
                print(f"ğŸ“ˆ å¤„ç†ä¿¡æ¯:")
                metadata = response.processing_metadata
                print(f"   â€¢ å¤„ç†æ­¥éª¤: {' â†’ '.join(metadata.get('processing_steps', []))}")
                print(f"   â€¢ å¤„ç†æ—¶é—´: {metadata.get('total_time', 0):.2f}s")
            
            # æ˜¾ç¤ºç”¨æˆ·å»ºæ¨¡ä¿¡æ¯
            if hasattr(response, 'user_profile'):
                print(f"ğŸ‘¤ ç”¨æˆ·ç”»åƒ:")
                print(f"   â€¢ æ€ç»´æ¨¡å¼: {response.user_profile.cognitive.thinking_mode.value}")
                print(f"   â€¢ è®¤çŸ¥å¤æ‚åº¦: {response.user_profile.cognitive.cognitive_complexity:.2f}")
                
            # æ˜¾ç¤ºé€‚é…ä¿¡æ¯
            if hasattr(response, 'adaptation_info'):
                print(f"âš™ï¸ é€‚é…ç­–ç•¥:")
                print(f"   â€¢ è¾“å‡ºå¯†åº¦: {response.adaptation_info.density_level.value}")
                print(f"   â€¢ ç»“æ„ç±»å‹: {response.adaptation_info.structure_type.value}")
                print(f"   â€¢ è®¤çŸ¥è´Ÿè·: {response.adaptation_info.cognitive_load:.2f}")
                
        except Exception as e:
            print(f"âŒ å¤„ç†å¤±è´¥: {e}")
            
        # çŸ­æš‚åœé¡¿ï¼Œæ¨¡æ‹ŸçœŸå®å¯¹è¯
        await asyncio.sleep(0.5)


async def demo_system_configurations():
    """æ¼”ç¤ºä¸åŒç³»ç»Ÿé…ç½®"""
    print("\n\nğŸ”§ ç³»ç»Ÿé…ç½®å¯¹æ¯”æ¼”ç¤º")
    print("=" * 60)
    
    configurations = [
        {
            "name": "åŸºç¡€ç³»ç»Ÿï¼ˆä»…LLMï¼‰",
            "config": {
                "system_type": "default",
                "llm_provider": "ollama",
                "llm_model_name": "qwen3:8b",
                "llm_temperature": 0.5
            }
        },
        {
            "name": "æ•™è‚²ç³»ç»Ÿï¼ˆLLM + åä½œï¼‰",
            "config": {
                "system_type": "educational", 
                "llm_provider": "ollama",
                "llm_model_name": "qwen3:8b",
                "llm_temperature": 0.3,  # æ›´ä½æ¸©åº¦ï¼Œæ›´ç¨³å®šè¾“å‡º
                "llm_max_tokens": 1000
            }
        },
        {
            "name": "ç ”ç©¶ç³»ç»Ÿï¼ˆé«˜åˆ›é€ æ€§ï¼‰",
            "config": {
                "system_type": "research",
                "llm_provider": "ollama", 
                "llm_model_name": "qwen3:8b",
                "llm_temperature": 0.8,  # æ›´é«˜æ¸©åº¦ï¼Œæ›´æœ‰åˆ›é€ æ€§
                "llm_max_tokens": 1200
            }
        }
    ]
    
    test_query = "äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸæœ‰å“ªäº›åº”ç”¨ï¼Ÿ"
    
    for i, config_info in enumerate(configurations, 1):
        print(f"\n{i}ï¸âƒ£ {config_info['name']}")
        print(f"   é…ç½®: {config_info['config']}")
        
        try:
            # åˆ›å»ºç³»ç»Ÿ
            system = aienhance.create_system(**config_info['config'])
            
            # æµ‹è¯•æŸ¥è¯¢
            response = await system.process_query(
                query=test_query,
                user_id=f"test_user_{i}",
                context={"test_config": config_info['name']}
            )
            
            print(f"   âœ… åˆ›å»ºæˆåŠŸ")
            print(f"   ğŸ¯ å“åº”ç‰¹å¾:")
            if hasattr(response, 'adaptation_info'):
                print(f"     â€¢ é€‚é…å¯†åº¦: {response.adaptation_info.density_level.value}")
                print(f"     â€¢ è®¤çŸ¥è´Ÿè·: {response.adaptation_info.cognitive_load:.2f}")
            
            if response.content:
                # æ˜¾ç¤ºå“åº”ç‰‡æ®µ
                content_preview = response.content[:100] + "..." if len(response.content) > 100 else response.content
                print(f"     â€¢ å“åº”é¢„è§ˆ: {content_preview}")
            else:
                print(f"     â€¢ å“åº”: (ç©ºå†…å®¹)")
                
        except Exception as e:
            print(f"   âŒ é…ç½®å¤±è´¥: {e}")


async def demo_ollama_integration():
    """æ¼”ç¤ºOllamaé›†æˆçš„å…·ä½“ç»†èŠ‚"""
    print("\n\nğŸ¤– Ollamaé›†æˆæ¼”ç¤º")
    print("=" * 60)
    
    print("ğŸ“‹ Ollamaé…ç½®æ£€æŸ¥:")
    
    # 1. æ£€æŸ¥OllamaæœåŠ¡
    import httpx
    try:
        async with httpx.AsyncClient() as client:
            response = await client.get("http://localhost:11434/api/tags", timeout=5.0)
            if response.status_code == 200:
                models_data = response.json()
                print(f"   âœ… OllamaæœåŠ¡è¿è¡Œæ­£å¸¸")
                print(f"   ğŸ“š å¯ç”¨æ¨¡å‹:")
                for model in models_data.get('models', []):
                    model_name = model.get('name', 'Unknown')
                    model_size = model.get('size', 0) / (1024**3)  # è½¬æ¢ä¸ºGB
                    print(f"     â€¢ {model_name} ({model_size:.1f}GB)")
            else:
                print(f"   âŒ OllamaæœåŠ¡å“åº”å¼‚å¸¸: {response.status_code}")
    except Exception as e:
        print(f"   âŒ æ— æ³•è¿æ¥OllamaæœåŠ¡: {e}")
        print(f"   ğŸ’¡ è¯·ç¡®ä¿Ollamaæ­£åœ¨è¿è¡Œ: ollama serve")
        return
    
    # 2. åˆ›å»ºå…·ä½“çš„Ollamaé…ç½®
    print(f"\nğŸ› ï¸ åˆ›å»ºOllamaç³»ç»Ÿ:")
    system = aienhance.create_system(
        system_type="educational",
        llm_provider="ollama",
        llm_model_name="qwen3:8b",
        llm_api_base="http://localhost:11434",
        llm_temperature=0.7,
        llm_max_tokens=500,
        embedding_provider="ollama",
        embedding_model_name="bge-m3"
    )
    
    print(f"   âœ… ç³»ç»Ÿåˆ›å»ºæˆåŠŸ")
    
    # 3. æµ‹è¯•ä¸åŒç±»å‹çš„æŸ¥è¯¢
    test_cases = [
        ("äº‹å®æ€§æŸ¥è¯¢", "åŒ—äº¬æ˜¯ä¸­å›½çš„é¦–éƒ½å—ï¼Ÿ"),
        ("è§£é‡Šæ€§æŸ¥è¯¢", "è¯·è§£é‡Šä»€ä¹ˆæ˜¯é‡å­è®¡ç®—"),
        ("åˆ›é€ æ€§æŸ¥è¯¢", "è®¾è®¡ä¸€ä¸ªæ™ºèƒ½å®¶å±…ç³»ç»Ÿçš„æ–¹æ¡ˆ"),
        ("ä¸­æ–‡ç†è§£", "ç”¨ä¸€å¥è¯æ€»ç»“ã€Šçº¢æ¥¼æ¢¦ã€‹çš„ä¸»é¢˜")
    ]
    
    print(f"\nğŸ§ª å¤šç±»å‹æŸ¥è¯¢æµ‹è¯•:")
    for query_type, query in test_cases:
        print(f"\n   ğŸ“ {query_type}: {query}")
        try:
            response = await system.process_query(
                query=query,
                user_id="ollama_test_user",
                context={"query_type": query_type}
            )
            
            if response.content:
                # æ˜¾ç¤ºå“åº”å’Œç‰¹å¾åˆ†æ
                print(f"   âœ… ç”ŸæˆæˆåŠŸ ({len(response.content)}å­—ç¬¦)")
                if hasattr(response, 'adaptation_info'):
                    print(f"   ğŸ“Š é€‚é…ä¿¡æ¯: {response.adaptation_info.density_level.value}å¯†åº¦")
            else:
                print(f"   âš ï¸ æ— å†…å®¹ç”Ÿæˆ")
                
        except Exception as e:
            print(f"   âŒ å¤„ç†å¤±è´¥: {e}")


async def demo_performance_monitoring():
    """æ¼”ç¤ºæ€§èƒ½ç›‘æ§"""
    print("\n\nğŸ“Š æ€§èƒ½ç›‘æ§æ¼”ç¤º")
    print("=" * 60)
    
    # åˆ›å»ºç³»ç»Ÿ
    system = aienhance.create_system(
        system_type="default",
        llm_provider="ollama",
        llm_model_name="qwen3:8b",
        llm_temperature=0.6
    )
    
    print("ğŸš€ æ€§èƒ½æµ‹è¯•å¼€å§‹...")
    
    # æ‰¹é‡æµ‹è¯•
    test_queries = [
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
        "æœºå™¨å­¦ä¹ çš„ä¸»è¦ç®—æ³•æœ‰å“ªäº›ï¼Ÿ", 
        "æ·±åº¦å­¦ä¹ åœ¨å›¾åƒè¯†åˆ«ä¸­çš„åº”ç”¨",
        "è‡ªç„¶è¯­è¨€å¤„ç†çš„å‘å±•å†ç¨‹",
        "æ¨èç³»ç»Ÿçš„å·¥ä½œåŸç†"
    ]
    
    start_time = asyncio.get_event_loop().time()
    successful_queries = 0
    total_response_length = 0
    
    for i, query in enumerate(test_queries, 1):
        try:
            query_start = asyncio.get_event_loop().time()
            response = await system.process_query(
                query=query,
                user_id=f"perf_test_user_{i}",
                context={"batch_test": True}
            )
            query_end = asyncio.get_event_loop().time()
            
            if response.content:
                successful_queries += 1
                total_response_length += len(response.content)
                print(f"   âœ… æŸ¥è¯¢{i}: {query_end - query_start:.2f}s, {len(response.content)}å­—ç¬¦")
            else:
                print(f"   âš ï¸ æŸ¥è¯¢{i}: æ— å†…å®¹ç”Ÿæˆ")
                
        except Exception as e:
            print(f"   âŒ æŸ¥è¯¢{i}å¤±è´¥: {e}")
    
    end_time = asyncio.get_event_loop().time()
    total_time = end_time - start_time
    
    # æ€§èƒ½ç»Ÿè®¡
    print(f"\nğŸ“ˆ æ€§èƒ½ç»Ÿè®¡:")
    print(f"   â€¢ æ€»è€—æ—¶: {total_time:.2f}ç§’")
    print(f"   â€¢ æˆåŠŸæŸ¥è¯¢: {successful_queries}/{len(test_queries)}")
    print(f"   â€¢ å¹³å‡å“åº”æ—¶é—´: {total_time/len(test_queries):.2f}ç§’/æŸ¥è¯¢")
    print(f"   â€¢ æ€»ç”Ÿæˆå­—ç¬¦æ•°: {total_response_length}")
    if successful_queries > 0:
        print(f"   â€¢ å¹³å‡å“åº”é•¿åº¦: {total_response_length/successful_queries:.0f}å­—ç¬¦/å“åº”")


async def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("ğŸš€ AiEnhance å®Œæ•´ç³»ç»Ÿé›†æˆæ¼”ç¤º")
    print("=" * 80)
    print("å±•ç¤ºè®°å¿†-è®¤çŸ¥ååŒç³»ç»Ÿä¸Ollama qwen3:8bçš„å®Œæ•´é›†æˆ")
    print("=" * 80)
    
    try:
        # 1. ç³»ç»Ÿé…ç½®æ¼”ç¤º
        system = await demo_complete_system_setup()
        
        # 2. æ™ºèƒ½å¯¹è¯æ¼”ç¤º
        await demo_intelligent_conversation(system)
        
        # 3. ç³»ç»Ÿé…ç½®å¯¹æ¯”
        await demo_system_configurations()
        
        # 4. Ollamaé›†æˆç»†èŠ‚
        await demo_ollama_integration()
        
        # 5. æ€§èƒ½ç›‘æ§
        await demo_performance_monitoring()
        
        print("\n\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
        print("=" * 80)
        print("âœ… AiEnhanceç³»ç»Ÿä¸Ollama qwen3:8bé›†æˆæ¼”ç¤ºæˆåŠŸ")
        print("âœ… å››å±‚æ¶æ„ï¼ˆæ„ŸçŸ¥â†’è®¤çŸ¥â†’è¡Œä¸ºâ†’åä½œï¼‰è¿è¡Œæ­£å¸¸")
        print("âœ… ç”¨æˆ·å»ºæ¨¡å’Œè‡ªé€‚åº”è¾“å‡ºåŠŸèƒ½éªŒè¯")
        print("âœ… å¤šç§ç³»ç»Ÿé…ç½®æ¨¡å¼éªŒè¯")
        print("âœ… æ€§èƒ½ç›‘æ§å’Œè´¨é‡è¯„ä¼°å®Œæˆ")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)