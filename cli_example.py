#!/usr/bin/env python3
"""
AiEnhance å‘½ä»¤è¡Œå·¥å…·
ç®€å•çš„å‘½ä»¤è¡Œç•Œé¢ï¼Œå¿«é€Ÿä½“éªŒè®°å¿†-è®¤çŸ¥ååŒç³»ç»Ÿ
"""

import argparse
import asyncio
import logging
import os
import sys
from pathlib import Path

import aienhance

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

def setup_logging():
    """è®¾ç½®æ—¥å¿—é…ç½®"""
    # ä»ç¯å¢ƒå˜é‡è¯»å–æ—¥å¿—çº§åˆ«ï¼Œé»˜è®¤ä¸ºINFO
    log_level = os.getenv("LOG_LEVEL", "INFO").upper()
    
    # éªŒè¯æ—¥å¿—çº§åˆ«
    valid_levels = ["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"]
    if log_level not in valid_levels:
        log_level = "INFO"
    
    # è®¾ç½®æ—¥å¿—æ ¼å¼
    log_format = os.getenv("LOG_FORMAT", "%(asctime)s - %(name)s - %(levelname)s - %(message)s")
    
    # é…ç½®æ ¹æ—¥å¿—å™¨
    logging.basicConfig(
        level=getattr(logging, log_level),
        format=log_format,
        handlers=[
            logging.StreamHandler(sys.stdout)
        ],
        force=True  # å¼ºåˆ¶é‡æ–°é…ç½®
    )
    
    # è®¾ç½® aienhance æ¨¡å—çš„æ—¥å¿—çº§åˆ«
    aienhance_logger = logging.getLogger('aienhance')
    aienhance_logger.setLevel(getattr(logging, log_level))
    
    return logging.getLogger(__name__)

# é…ç½®æ§åˆ¶å°æ—¥å¿—è¾“å‡º
logger = setup_logging()


class AiEnhanceCliTool:
    """AiEnhanceå‘½ä»¤è¡Œå·¥å…·"""

    def __init__(self):
        self.system = None  # type: Optional[aienhance.CognitiveSystem]

    async def check_ollama(self):
        """æ£€æŸ¥OllamaæœåŠ¡çŠ¶æ€"""
        try:
            import httpx

            async with httpx.AsyncClient() as client:
                ollama_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
                response = await client.get(f"{ollama_url}/api/tags", timeout=3.0)
                if response.status_code == 200:
                    return True
                return False
        except:
            return False

    async def initialize_system(
        self, system_type=None, temperature=None, use_memory=None
    ):
        """åˆå§‹åŒ–ç³»ç»Ÿ"""
        try:
            # ä»ç¯å¢ƒå˜é‡è·å–é»˜è®¤é…ç½®
            system_type = system_type or os.getenv("DEFAULT_SYSTEM_TYPE", "educational")
            temperature = temperature or float(
                os.getenv("DEFAULT_LLM_TEMPERATURE", "0.7")
            )
            use_memory = (
                use_memory
                if use_memory is not None
                else os.getenv("ENABLE_MEMORY_SYSTEM", "true").lower() == "true"
            )
            
            logger.info(f"æ­£åœ¨åˆå§‹åŒ–è®¤çŸ¥ç³»ç»Ÿ - ç±»å‹: {system_type}, æ¸©åº¦: {temperature}, è®°å¿†ç³»ç»Ÿ: {use_memory}")

            if use_memory:
                # ä½¿ç”¨æ–°çš„å±‚-æ¨¡å—-å­æ¨¡å—è®¤çŸ¥ç³»ç»Ÿï¼Œå¸¦è®°å¿†åŠŸèƒ½
                if system_type == "educational":
                    self.system = aienhance.create_educational_system(
                        llm_provider=os.getenv("DEFAULT_LLM_PROVIDER", "ollama"),
                        llm_model_name=os.getenv("DEFAULT_LLM_MODEL", "qwen3:8b"),
                        memory_provider=os.getenv("DEFAULT_MEMORY_SYSTEM", "graphiti"),
                        config={
                            "temperature": temperature,
                            "max_tokens": None,  # æ— é•¿åº¦é™åˆ¶ï¼Œé¿å…æˆªæ–­
                            "ollama_base_url": os.getenv("OLLAMA_BASE_URL", "http://localhost:11434"),
                        }
                    )
                elif system_type == "research":
                    self.system = aienhance.create_research_system(
                        llm_provider=os.getenv("DEFAULT_LLM_PROVIDER", "ollama"),
                        llm_model_name=os.getenv("DEFAULT_LLM_MODEL", "qwen3:8b"),
                        memory_provider=os.getenv("DEFAULT_MEMORY_SYSTEM", "graphiti"),
                        config={
                            "temperature": temperature,
                            "max_tokens": None,  # æ— é•¿åº¦é™åˆ¶ï¼Œé¿å…æˆªæ–­
                            "ollama_base_url": os.getenv("OLLAMA_BASE_URL", "http://localhost:11434"),
                        }
                    )
                elif system_type == "creative":
                    self.system = aienhance.create_creative_system(
                        llm_provider=os.getenv("DEFAULT_LLM_PROVIDER", "ollama"),
                        llm_model_name=os.getenv("DEFAULT_LLM_MODEL", "qwen3:8b"),
                        memory_provider=os.getenv("DEFAULT_MEMORY_SYSTEM", "graphiti"),
                        config={
                            "temperature": temperature,
                            "max_tokens": None,  # æ— é•¿åº¦é™åˆ¶ï¼Œé¿å…æˆªæ–­
                            "ollama_base_url": os.getenv("OLLAMA_BASE_URL", "http://localhost:11434"),
                        }
                    )
                else:
                    # é»˜è®¤ä½¿ç”¨æ•™è‚²ç³»ç»Ÿ
                    self.system = aienhance.create_educational_system(
                        llm_provider=os.getenv("DEFAULT_LLM_PROVIDER", "ollama"),
                        llm_model_name=os.getenv("DEFAULT_LLM_MODEL", "qwen3:8b"),
                        memory_provider=os.getenv("DEFAULT_MEMORY_SYSTEM", "graphiti"),
                        config={
                            "temperature": temperature,
                            "max_tokens": None,  # æ— é•¿åº¦é™åˆ¶ï¼Œé¿å…æˆªæ–­
                            "ollama_base_url": os.getenv("OLLAMA_BASE_URL", "http://localhost:11434"),
                        }
                    )
            else:
                # ç®€åŒ–é…ç½®ï¼Œä»…ä½¿ç”¨LLMåŠŸèƒ½ï¼ˆè½»é‡çº§ç³»ç»Ÿï¼‰
                print("âš ï¸  ç®€åŒ–æ¨¡å¼ï¼šä½¿ç”¨è½»é‡çº§ç³»ç»Ÿï¼Œæ— è®°å¿†åŠŸèƒ½")
                self.system = aienhance.create_lightweight_system(
                    llm_provider=os.getenv("DEFAULT_LLM_PROVIDER", "ollama"),
                    llm_model_name=os.getenv("DEFAULT_LLM_MODEL", "qwen3:8b"),
                    config={
                        "llm_temperature": temperature,
                        "llm_max_tokens": None,  # æ— é•¿åº¦é™åˆ¶ï¼Œé¿å…æˆªæ–­
                        "ollama_base_url": os.getenv("OLLAMA_BASE_URL", "http://localhost:11434"),
                    }
                )

            # åˆå§‹åŒ–æ–°æ¶æ„ç³»ç»Ÿ
            logger.info("å¼€å§‹ç³»ç»Ÿåˆå§‹åŒ–è¿‡ç¨‹...")
            success = await self.system.initialize()
            if success:
                logger.info("âœ… è®¤çŸ¥ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
            else:
                logger.warning("âš ï¸ ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼Œä½†å­˜åœ¨éƒ¨åˆ†è­¦å‘Š")
            return success
        except Exception as e:
            print(f"âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
            # å¦‚æœå®Œæ•´æ¨¡å¼å¤±è´¥ï¼Œå°è¯•ç®€åŒ–æ¨¡å¼
            if use_memory:
                print("ğŸ”„ å°è¯•ç®€åŒ–æ¨¡å¼...")
                return await self.initialize_system(
                    system_type, temperature, use_memory=False
                )
            return False

    async def single_query(
        self, query, system_type="educational", temperature=0.7, show_details=False
    ):
        """å•æ¬¡æŸ¥è¯¢ - é»˜è®¤ä½¿ç”¨æµå¼è¾“å‡º"""
        print("ğŸ”§ åˆå§‹åŒ–ç³»ç»Ÿ...")
        if not await self.initialize_system(system_type, temperature):
            return

        print("ğŸ¤” å¤„ç†æŸ¥è¯¢ä¸­...")
        logger.info(f"å¼€å§‹å¤„ç†ç”¨æˆ·æŸ¥è¯¢: {query[:50]}{'...' if len(query) > 50 else ''}")
        try:
            # é»˜è®¤ä½¿ç”¨æµå¼å¤„ç†
            print("\n" + "=" * 50)
            print("ğŸ¤– AIå®æ—¶å›ç­”:")
            print("=" * 50)

            content_parts = []
            result = await self.system.process(
                user_id="cli_user", 
                query=query, 
                session_context={"source": "cli"}
            )
            
            if result.success and "final_response" in result.data:
                print(result.data["final_response"])
            else:
                print("âŒ å¤„ç†å¤±è´¥:", result.error_message if hasattr(result, 'error_message') else "æœªçŸ¥é”™è¯¯")

            if show_details:
                # è·å–ç³»ç»ŸçŠ¶æ€ç”¨äºè¯¦ç»†ä¿¡æ¯æ˜¾ç¤º
                status = self.system.get_system_status()
                print("\n" + "=" * 50)
                print("ğŸ“Š è¯¦ç»†ä¿¡æ¯:")
                print("=" * 50)
                print("ğŸ” ç³»ç»ŸçŠ¶æ€:")
                print(f"   ç³»ç»Ÿç±»å‹: {system_type}")
                print(
                    f"   LLMé…ç½®: {status.get('components', {}).get('llm_provider', {}).get('provider', 'None')}"
                )
                print(f"   å“åº”é•¿åº¦: {len(''.join(content_parts))}å­—ç¬¦")
                print("   æµå¼è¾“å‡º: âœ… å·²å¯ç”¨")

        except Exception as e:
            print(f"âŒ æŸ¥è¯¢å¤„ç†å¤±è´¥: {e}")

    async def interactive_mode(self, system_type="educational", temperature=0.7):
        """äº¤äº’æ¨¡å¼"""
        print("ğŸš€ AiEnhance äº¤äº’æ¨¡å¼")
        print("=" * 50)
        print("ğŸ’¡ æç¤º:")
        print("  â€¢ ç›´æ¥è¾“å…¥é—®é¢˜ï¼ŒæŒ‰å›è½¦å‘é€")
        print("  â€¢ è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
        print("  â€¢ è¾“å…¥ 'clear' æ¸…å±")
        print("  â€¢ è¾“å…¥ 'status' æŸ¥çœ‹ç³»ç»ŸçŠ¶æ€")
        print("=" * 50)

        # åˆå§‹åŒ–ç³»ç»Ÿ
        print("ğŸ”§ åˆå§‹åŒ–ç³»ç»Ÿ...")
        if not await self.initialize_system(system_type, temperature):
            return
        print(f"âœ… ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ (ç±»å‹: {system_type}, æ¸©åº¦: {temperature})")

        session_count = 0

        while True:
            try:
                # è·å–ç”¨æˆ·è¾“å…¥
                user_input = input(f"\n[{session_count}] ğŸ‘¤ æ‚¨: ").strip()

                # å¤„ç†ç‰¹æ®Šå‘½ä»¤
                if user_input.lower() in ["quit", "exit", "é€€å‡º", "q"]:
                    print("ğŸ‘‹ å†è§ï¼æ„Ÿè°¢ä½¿ç”¨AiEnhance")
                    break
                elif user_input.lower() == "clear":
                    import os

                    os.system("clear" if os.name == "posix" else "cls")
                    continue
                elif user_input.lower() == "status":
                    status = self.system.get_system_status()
                    print(f"ğŸ“Š ç³»ç»ŸçŠ¶æ€: {status}")
                    continue
                elif not user_input:
                    continue

                # å¤„ç†æŸ¥è¯¢ - ä½¿ç”¨æµå¼è¾“å‡º
                print("ğŸ¤” æ€è€ƒä¸­...")
                print("ğŸ¤– AI: ", end="", flush=True)

                content_parts = []
                result = await self.system.process(
                    user_id="interactive_user",
                    query=user_input,
                    session_context={"session": session_count}
                )
                
                if result.success and "final_response" in result.data:
                    print(result.data["final_response"])
                else:
                    print("âŒ å¤„ç†å¤±è´¥:", result.error_message if hasattr(result, 'error_message') else "æœªçŸ¥é”™è¯¯")

                if not content_parts:
                    print("(æ— æ³•ç”Ÿæˆå“åº”)")
                else:
                    print(f"\n    âš™ï¸ [æµå¼è¾“å‡º, é•¿åº¦{len(''.join(content_parts))}å­—ç¬¦]")

                session_count += 1

            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ç¨‹åºå·²é€€å‡º")
                break
            except Exception as e:
                print(f"âŒ å¤„ç†é”™è¯¯: {e}")

    async def demo_mode(self):
        """æ¼”ç¤ºæ¨¡å¼"""
        print("ğŸ® AiEnhance æ¼”ç¤ºæ¨¡å¼")
        print("=" * 50)

        # æ¼”ç¤ºæŸ¥è¯¢
        demo_queries = [
            ("åŸºç¡€é—®ç­”", "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±"),
            ("çŸ¥è¯†è§£é‡Š", "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿè¯·ç®€å•è§£é‡Š"),
            ("åˆ›æ„æ€ç»´", "è®¾è®¡ä¸€ä¸ªæ™ºèƒ½å®¶å±…ç³»ç»Ÿçš„åŸºæœ¬æ–¹æ¡ˆ"),
            ("é—®é¢˜åˆ†æ", "åˆ†æä¸€ä¸‹è¿œç¨‹å·¥ä½œçš„åˆ©å¼Š"),
        ]

        # åˆå§‹åŒ–ç³»ç»Ÿ
        print("ğŸ”§ åˆå§‹åŒ–æ¼”ç¤ºç³»ç»Ÿ...")
        if not await self.initialize_system("educational", 0.7):
            return
        print("âœ… ç³»ç»Ÿå°±ç»ª\n")

        for i, (category, query) in enumerate(demo_queries, 1):
            print(f"{i}ï¸âƒ£ {category}æ¼”ç¤º")
            print(f"ğŸ‘¤ ç”¨æˆ·: {query}")
            print("ğŸ¤” AIæ€è€ƒä¸­...")

            try:
                print("ğŸ¤– AI: ", end="", flush=True)

                content_parts = []
                result = await self.system.process(
                    user_id="demo_user",
                    query=query,
                    session_context={"demo_type": category}
                )
                
                if result.success and "final_response" in result.data:
                    print(result.data["final_response"])
                else:
                    print("âŒ å¤„ç†å¤±è´¥:", result.error_message if hasattr(result, 'error_message') else "æœªçŸ¥é”™è¯¯")

                if not content_parts:
                    print("(æ— å“åº”ç”Ÿæˆ)")
                else:
                    print(f"\nğŸ“Š æµå¼è¾“å‡º: é•¿åº¦{len(''.join(content_parts))}å­—ç¬¦")

                print("-" * 50)

                # çŸ­æš‚åœé¡¿
                if i < len(demo_queries):
                    await asyncio.sleep(1)

            except Exception as e:
                print(f"âŒ æ¼”ç¤ºå¤±è´¥: {e}")
                print("-" * 50)

        print("ğŸ‰ æ¼”ç¤ºå®Œæˆï¼")


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="AiEnhance å‘½ä»¤è¡Œå·¥å…· - è®°å¿†-è®¤çŸ¥ååŒç³»ç»Ÿ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python ai.py "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"                    # å•æ¬¡æŸ¥è¯¢
  python ai.py -i                                    # äº¤äº’æ¨¡å¼  
  python ai.py -d                                    # æ¼”ç¤ºæ¨¡å¼
  python ai.py "è§£é‡Šæ·±åº¦å­¦ä¹ " --type research        # ç ”ç©¶æ¨¡å¼æŸ¥è¯¢
  python ai.py "åˆ›æ„å†™ä½œ" --temp 0.9 --details       # é«˜åˆ›é€ æ€§ + è¯¦ç»†ä¿¡æ¯
        """,
    )

    # ä½ç½®å‚æ•°
    parser.add_argument("query", nargs="?", help="è¦å¤„ç†çš„æŸ¥è¯¢é—®é¢˜")

    # å¯é€‰å‚æ•°
    parser.add_argument("-i", "--interactive", action="store_true", help="å¯åŠ¨äº¤äº’æ¨¡å¼")
    parser.add_argument("-d", "--demo", action="store_true", help="è¿è¡Œæ¼”ç¤ºæ¨¡å¼")
    parser.add_argument(
        "--type",
        choices=["default", "educational", "research"],
        default="educational",
        help="ç³»ç»Ÿç±»å‹ (é»˜è®¤: educational)",
    )
    parser.add_argument(
        "--temp", type=float, default=0.7, help="æ¸©åº¦å‚æ•° 0.0-1.0 (é»˜è®¤: 0.7)"
    )
    parser.add_argument("--details", action="store_true", help="æ˜¾ç¤ºè¯¦ç»†çš„å¤„ç†ä¿¡æ¯")

    args = parser.parse_args()

    # åˆ›å»ºå·¥å…·å®ä¾‹
    tool = AiEnhanceCliTool()

    # æ£€æŸ¥OllamaæœåŠ¡
    print("ğŸ” æ£€æŸ¥OllamaæœåŠ¡çŠ¶æ€...")
    if not await tool.check_ollama():
        print("âŒ OllamaæœåŠ¡æœªè¿è¡Œæˆ–æ¨¡å‹æœªå®‰è£…")
        print("ğŸ’¡ è¯·ç¡®ä¿:")
        print("   1. OllamaæœåŠ¡è¿è¡Œ: ollama serve")
        print("   2. æ¨¡å‹å·²å®‰è£…: ollama pull qwen3:8b")
        return
    print("âœ… OllamaæœåŠ¡æ­£å¸¸")

    # æ ¹æ®å‚æ•°é€‰æ‹©æ¨¡å¼
    if args.interactive:
        await tool.interactive_mode(args.type, args.temp)
    elif args.demo:
        await tool.demo_mode()
    elif args.query:
        await tool.single_query(args.query, args.type, args.temp, args.details)
    else:
        # é»˜è®¤æ˜¾ç¤ºå¸®åŠ©
        parser.print_help()
        print("\nğŸ’¡ å¿«é€Ÿå¼€å§‹:")
        print('  python ai.py "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½"')
        print("  python ai.py -i  # è¿›å…¥äº¤äº’æ¨¡å¼")
        print("  python ai.py -d  # æŸ¥çœ‹æ¼”ç¤º")


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç¨‹åºå·²é€€å‡º")
    except Exception as e:
        print(f"âŒ ç¨‹åºé”™è¯¯: {e}")
