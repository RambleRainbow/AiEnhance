#!/usr/bin/env python3
"""
ä¿®å¤GraphitiåµŒå…¥æ¨¡å‹é—®é¢˜
é€šè¿‡åˆ›å»ºæ¨¡å‹åˆ«åæˆ–ç›´æ¥é…ç½®è§£å†³æ–¹æ¡ˆ
"""

import asyncio
import json
import subprocess
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

import aiohttp
from aienhance.config import Config


async def test_model_alias_approach():
    """æµ‹è¯•é€šè¿‡æ¨¡å‹åˆ«åè§£å†³é—®é¢˜"""
    print("ğŸ”§ å°è¯•æ¨¡å‹åˆ«åè§£å†³æ–¹æ¡ˆ")
    print("=" * 40)
    
    # æ£€æŸ¥Ollamaæ˜¯å¦æ”¯æŒåˆ›å»ºæ¨¡å‹åˆ«å
    try:
        # å°è¯•åˆ›å»ºä¸€ä¸ªtext-embedding-3-smallæ¨¡å‹æ–‡ä»¶
        modelfile_content = """FROM nomic-embed-text
PARAMETER temperature 0.0"""
        
        modelfile_path = "/tmp/text-embedding-3-small.modelfile"
        with open(modelfile_path, "w") as f:
            f.write(modelfile_content)
        
        print("1ï¸âƒ£ åˆ›å»ºæ¨¡å‹åˆ«å...")
        result = subprocess.run([
            "ollama", "create", "text-embedding-3-small", "-f", modelfile_path
        ], capture_output=True, text=True)
        
        if result.returncode == 0:
            print("   âœ… æˆåŠŸåˆ›å»ºtext-embedding-3-smallåˆ«å")
            return True
        else:
            print(f"   âŒ åˆ«ååˆ›å»ºå¤±è´¥: {result.stderr}")
            return False
            
    except Exception as e:
        print(f"   âŒ åˆ«ååˆ›å»ºå¼‚å¸¸: {e}")
        return False


async def test_direct_api_with_alias():
    """æµ‹è¯•ä½¿ç”¨åˆ«ååçš„API"""
    print("\n2ï¸âƒ£ æµ‹è¯•åˆ«ååçš„Ollama API")
    
    async with aiohttp.ClientSession() as session:
        # æµ‹è¯•OpenAIå…¼å®¹æ¥å£æ˜¯å¦èƒ½è¯†åˆ«æ–°æ¨¡å‹
        try:
            headers = {
                "Authorization": "Bearer fake_key_for_ollama",
                "Content-Type": "application/json"
            }
            
            payload = {
                "model": "text-embedding-3-small",
                "input": ["æµ‹è¯•æ–‡æœ¬"]
            }
            
            async with session.post(
                f"{Config.OLLAMA_BASE_URL}/v1/embeddings",
                json=payload,
                headers=headers
            ) as response:
                if response.status == 200:
                    result = await response.json()
                    embeddings = result.get("data", [])
                    if embeddings:
                        dim = len(embeddings[0].get("embedding", []))
                        print(f"   âœ… text-embedding-3-smallå¯ç”¨: {dim}ç»´")
                        return True
                    else:
                        print("   âš ï¸  text-embedding-3-smallè¿”å›ç©ºæ•°æ®")
                        return False
                else:
                    error_text = await response.text()
                    print(f"   âŒ text-embedding-3-smallä¸å¯ç”¨: {response.status}")
                    print(f"   é”™è¯¯: {error_text}")
                    return False
        except Exception as e:
            print(f"   âŒ æµ‹è¯•å¼‚å¸¸: {e}")
            return False


async def test_graphiti_after_fix():
    """ä¿®å¤åæµ‹è¯•Graphiti"""
    print("\n3ï¸âƒ£ æµ‹è¯•ä¿®å¤åçš„Graphiti")
    
    # é‡å¯GraphitiæœåŠ¡
    try:
        print("   é‡å¯GraphitiæœåŠ¡...")
        subprocess.run([
            "docker", "compose", "restart", "graph"
        ], cwd="/Users/hongling/Dev/claude/graphiti", check=True, capture_output=True)
        
        # ç­‰å¾…å¯åŠ¨
        await asyncio.sleep(10)
        
        # è¿è¡Œç®€å•æµ‹è¯•
        print("   æµ‹è¯•æœç´¢åŠŸèƒ½...")
        result = subprocess.run([
            "uv", "run", "python", "tests/test_graphiti_simple.py"
        ], capture_output=True, text=True, cwd="/Users/hongling/Dev/claude/AiEnhance")
        
        if "æœç´¢åŠŸèƒ½å­˜åœ¨åµŒå…¥æ¨¡å‹é…ç½®é—®é¢˜" in result.stdout:
            print("   âŒ æœç´¢é—®é¢˜ä»ç„¶å­˜åœ¨")
            return False
        else:
            print("   âœ… Graphitiæœç´¢å¯èƒ½å·²ä¿®å¤")
            return True
            
    except Exception as e:
        print(f"   âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False


def cleanup_model_alias():
    """æ¸…ç†åˆ›å»ºçš„æ¨¡å‹åˆ«å"""
    print("\n4ï¸âƒ£ æ¸…ç†æ¨¡å‹åˆ«å")
    try:
        result = subprocess.run([
            "ollama", "rm", "text-embedding-3-small"
        ], capture_output=True, text=True)
        
        if result.returncode == 0:
            print("   âœ… æ¸…ç†å®Œæˆ")
        else:
            print(f"   âš ï¸  æ¸…ç†ç»“æœ: {result.stderr}")
    except Exception as e:
        print(f"   âŒ æ¸…ç†å¼‚å¸¸: {e}")


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ GraphitiåµŒå…¥æ¨¡å‹ä¿®å¤å°è¯•")
    print("=" * 50)
    
    # å°è¯•åˆ›å»ºæ¨¡å‹åˆ«å
    alias_created = await test_model_alias_approach()
    
    if alias_created:
        # æµ‹è¯•åˆ«åæ˜¯å¦å·¥ä½œ
        alias_works = await test_direct_api_with_alias()
        
        if alias_works:
            # æµ‹è¯•Graphitiæ˜¯å¦ä¿®å¤
            graphiti_fixed = await test_graphiti_after_fix()
            
            if not graphiti_fixed:
                print("\nğŸ’¡ ç»“è®º:")
                print("   è™½ç„¶åˆ›å»ºäº†æ¨¡å‹åˆ«åï¼Œä½†Graphitié—®é¢˜ä¾ç„¶å­˜åœ¨")
                print("   å¯èƒ½éœ€è¦æŸ¥çœ‹Graphitiæºç æˆ–ä½¿ç”¨å…¶ä»–æ–¹æ³•")
        
        # æ¸…ç†åˆ«å
        cleanup_model_alias()
    
    print("\nğŸ“‹ æ€»ç»“:")
    print("   â€¢ Ollama OpenAIå…¼å®¹æ¥å£å·¥ä½œæ­£å¸¸")
    print("   â€¢ é—®é¢˜å‡ºç°åœ¨Graphitiçš„æ¨¡å‹é…ç½®å±‚é¢")
    print("   â€¢ å»ºè®®æ£€æŸ¥Graphitiæºç ä¸­çš„é»˜è®¤æ¨¡å‹è®¾ç½®")
    print("   â€¢ æˆ–è€…è”ç³»Graphitié¡¹ç›®è·å–æ­£ç¡®çš„é…ç½®æ–¹æ³•")


if __name__ == "__main__":
    asyncio.run(main())