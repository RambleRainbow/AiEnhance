#!/usr/bin/env python3
"""
Graphitié…ç½®è¯Šæ–­å’Œä¿®å¤è„šæœ¬
å¸®åŠ©è§£å†³åµŒå…¥æ¨¡å‹é…ç½®é—®é¢˜
"""

import asyncio
import json
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

import aiohttp
from aienhance.config import Config


async def diagnose_graphiti_config():
    """è¯Šæ–­Graphitié…ç½®é—®é¢˜"""
    print("ğŸ”§ Graphitié…ç½®è¯Šæ–­")
    print("=" * 40)
    
    # 1. æ£€æŸ¥å½“å‰é…ç½®
    print("\nğŸ“‹ å½“å‰ç³»ç»Ÿé…ç½®:")
    print(f"   Graphiti API: {Config.GRAPHITI_API_URL}")
    print(f"   Neo4j URI: {Config.NEO4J_URI}")
    print(f"   Ollama URL: {Config.OLLAMA_BASE_URL}")
    print(f"   åµŒå…¥æ¨¡å‹: {Config.EMBEDDING_MODEL}")
    
    # 2. æ£€æŸ¥Ollamaå¯ç”¨æ€§
    print("\nğŸ¤– æ£€æŸ¥OllamaæœåŠ¡:")
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(f"{Config.OLLAMA_BASE_URL}/api/tags") as response:
                if response.status == 200:
                    models = await response.json()
                    print("   âœ… OllamaæœåŠ¡è¿è¡Œæ­£å¸¸")
                    print("   ğŸ“¦ å¯ç”¨æ¨¡å‹:")
                    for model in models.get("models", []):
                        name = model.get("name", "æœªçŸ¥")
                        size = model.get("size", 0) // (1024**3)  # GB
                        print(f"     - {name} ({size}GB)")
                else:
                    print(f"   âŒ OllamaæœåŠ¡å¼‚å¸¸: {response.status}")
    except Exception as e:
        print(f"   âŒ Ollamaè¿æ¥å¤±è´¥: {e}")
    
    # 3. æµ‹è¯•åµŒå…¥æ¨¡å‹
    print(f"\nğŸ” æµ‹è¯•åµŒå…¥æ¨¡å‹ ({Config.EMBEDDING_MODEL}):")
    try:
        async with aiohttp.ClientSession() as session:
            embed_payload = {
                "model": Config.EMBEDDING_MODEL,
                "prompt": "æµ‹è¯•åµŒå…¥",
                "input": "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬"
            }
            
            async with session.post(
                f"{Config.OLLAMA_BASE_URL}/api/embeddings",
                json=embed_payload
            ) as response:
                if response.status == 200:
                    result = await response.json()
                    embedding = result.get("embedding", [])
                    print(f"   âœ… åµŒå…¥ç”ŸæˆæˆåŠŸ")
                    print(f"   ğŸ“Š å‘é‡ç»´åº¦: {len(embedding)}")
                else:
                    error_text = await response.text()
                    print(f"   âŒ åµŒå…¥ç”Ÿæˆå¤±è´¥: {response.status}")
                    print(f"   é”™è¯¯è¯¦æƒ…: {error_text}")
    except Exception as e:
        print(f"   âŒ åµŒå…¥æµ‹è¯•å¼‚å¸¸: {e}")


async def suggest_fixes():
    """æä¾›é…ç½®ä¿®å¤å»ºè®®"""
    print("\n" + "=" * 50)
    print("ğŸ› ï¸  é…ç½®ä¿®å¤å»ºè®®")
    print("=" * 50)
    
    print("\nğŸ¯ é—®é¢˜åˆ†æ:")
    print("   GraphitiæœåŠ¡é…ç½®ä¸ºä½¿ç”¨OpenAIåµŒå…¥æ¨¡å‹ï¼Œä½†è¯¥æ¨¡å‹ä¸å¯ç”¨")
    print("   è¿™å¯¼è‡´æœç´¢åŠŸèƒ½å‡ºç°500é”™è¯¯")
    
    print("\nğŸ”§ è§£å†³æ–¹æ¡ˆ1: é…ç½®Graphitiä½¿ç”¨OllamaåµŒå…¥")
    print("   1. åœæ­¢å½“å‰GraphitiæœåŠ¡:")
    print("      docker-compose down")
    print("")
    print("   2. ä¿®æ”¹Graphitié…ç½®æ–‡ä»¶ï¼Œè®¾ç½®åµŒå…¥æä¾›å•†ä¸ºOllama:")
    print("      - æŸ¥æ‰¾graphitié…ç½®æ–‡ä»¶ (å¯èƒ½åœ¨docker-compose.ymlæˆ–ç¯å¢ƒå˜é‡)")
    print("      - è®¾ç½® EMBEDDING_PROVIDER=ollama")
    print("      - è®¾ç½® OLLAMA_BASE_URL=http://host.docker.internal:11434")
    print("      - è®¾ç½® EMBEDDING_MODEL=bge-m3")
    print("")
    print("   3. é‡å¯æœåŠ¡:")
    print("      docker-compose up -d")
    
    print("\nğŸ”§ è§£å†³æ–¹æ¡ˆ2: æä¾›OpenAI APIå¯†é’¥")
    print("   1. è·å–OpenAI APIå¯†é’¥")
    print("   2. åœ¨docker-compose.ymlä¸­è®¾ç½®ç¯å¢ƒå˜é‡:")
    print("      OPENAI_API_KEY=your_openai_api_key")
    print("   3. é‡å¯æœåŠ¡")
    
    print("\nğŸ”§ è§£å†³æ–¹æ¡ˆ3: ä½¿ç”¨æœ¬åœ°åµŒå…¥æ¨¡å¼ (å¦‚æœæ”¯æŒ)")
    print("   1. æŸ¥çœ‹Graphitiæ–‡æ¡£ï¼Œç¡®è®¤æ˜¯å¦æ”¯æŒæœ¬åœ°åµŒå…¥")
    print("   2. é…ç½®ä¸ºä¸ä½¿ç”¨è¿œç¨‹åµŒå…¥æœåŠ¡")
    
    print("\nâœ… å½“å‰å¯ç”¨åŠŸèƒ½:")
    print("   â€¢ æœåŠ¡å¥åº·æ£€æŸ¥ âœ…")
    print("   â€¢ æ¶ˆæ¯æ·»åŠ  âœ…") 
    print("   â€¢ æ•°æ®æ¸…ç† âœ…")
    print("   â€¢ Neo4jç›´è¿ âœ…")
    
    print("\nâŒ å¾…ä¿®å¤åŠŸèƒ½:")
    print("   â€¢ è¯­ä¹‰æœç´¢ (éœ€è¦åµŒå…¥æ¨¡å‹)")
    print("   â€¢ ç›¸ä¼¼æ€§æŸ¥è¯¢")
    

async def test_workaround():
    """æµ‹è¯•ä¸ä¾èµ–åµŒå…¥çš„åŠŸèƒ½"""
    print("\n" + "=" * 50) 
    print("ğŸ§ª æ›¿ä»£åŠŸèƒ½æµ‹è¯•")
    print("=" * 50)
    
    # ç›´æ¥æŸ¥è¯¢Neo4jä¸­çš„æ•°æ®
    try:
        from neo4j import AsyncGraphDatabase
        
        driver = AsyncGraphDatabase.driver(
            Config.NEO4J_URI,
            auth=(Config.NEO4J_USER, Config.NEO4J_PASSWORD)
        )
        
        async with driver.session() as session:
            print("\nğŸ” ç›´æ¥æŸ¥è¯¢Neo4jæ•°æ®:")
            
            # æŸ¥è¯¢èŠ‚ç‚¹ç±»å‹ç»Ÿè®¡
            result = await session.run(
                "MATCH (n) RETURN labels(n) as labels, count(n) as count"
            )
            print("   èŠ‚ç‚¹ç±»å‹ç»Ÿè®¡:")
            async for record in result:
                labels = record["labels"]
                count = record["count"]
                print(f"     {labels}: {count}ä¸ª")
            
            # æŸ¥è¯¢å…³ç³»ç»Ÿè®¡
            result = await session.run(
                "MATCH ()-[r]->() RETURN type(r) as rel_type, count(r) as count"
            )
            print("   å…³ç³»ç±»å‹ç»Ÿè®¡:")
            async for record in result:
                rel_type = record["rel_type"]
                count = record["count"]
                print(f"     {rel_type}: {count}ä¸ª")
        
        await driver.close()
        
    except ImportError:
        print("   âš ï¸  neo4jåŒ…æœªå®‰è£…ï¼Œæ— æ³•ç›´æ¥æŸ¥è¯¢")
    except Exception as e:
        print(f"   âŒ Neo4jæŸ¥è¯¢å¤±è´¥: {e}")


async def main():
    """ä¸»å‡½æ•°"""
    await diagnose_graphiti_config()
    await suggest_fixes()
    await test_workaround()


if __name__ == "__main__":
    asyncio.run(main())