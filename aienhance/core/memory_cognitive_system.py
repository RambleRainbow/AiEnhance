"""
è®°å¿†-è®¤çŸ¥ååŒç³»ç»Ÿæ ¸å¿ƒ - å¯¹åº”è®¾è®¡æ–‡æ¡£æ•´ä½“æ¶æ„
æ•´åˆæ„ŸçŸ¥å±‚ã€è®¤çŸ¥å±‚ã€è¡Œä¸ºå±‚ã€åä½œå±‚çš„å®Œæ•´ç³»ç»Ÿ
"""

from ..llm import (
    LLMProvider,
    EmbeddingProvider,
    LLMProviderFactory,
    EmbeddingProviderFactory,
    ModelConfig,
    ChatMessage,
    MessageRole,
    create_chat_message,
    create_model_config
)
from ..memory import (
    MemorySystem,
    MemorySystemFactory,
    MemorySystemConfig,
    MemoryEntry,
    MemoryQuery,
    UserContext,
    MemoryType,
    create_user_context,
    create_memory_entry
)
from ..behavior.adaptive_output import (
    IntegratedAdaptiveOutput,
    AdaptedContent
)
from ..cognition import (
    MultiLevelMemoryActivator,
    IntegratedSemanticEnhancer,
    IntegratedAnalogyReasoner,
    MemoryFragment,
    ActivationResult,
    IntegrationResult
)
from ..perception import (
    DynamicUserModeler,
    IntegratedContextAnalyzer,
    UserProfile,
    ContextProfile
)
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
import asyncio
import logging

logger = logging.getLogger(__name__)


@dataclass
class SystemResponse:
    """ç³»ç»Ÿå“åº”"""
    content: str
    user_profile: UserProfile
    context_profile: ContextProfile
    activated_memories: List[ActivationResult]
    semantic_enhancement: IntegrationResult
    analogy_reasoning: Dict[str, Any]
    adaptation_info: AdaptedContent
    processing_metadata: Dict[str, Any]
    collaboration_result: Optional[Dict[str, Any]] = None


class MemoryCognitiveSystem:
    """
    è®°å¿†-è®¤çŸ¥ååŒç³»ç»Ÿä¸»ç±»
    å®ç°è®¾è®¡æ–‡æ¡£ä¸­çš„å››å±‚æ¶æ„å’Œæ ¸å¿ƒåŠŸèƒ½
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None,
                 memory_config: Optional[MemorySystemConfig] = None,
                 llm_config: Optional[ModelConfig] = None,
                 embedding_config: Optional[ModelConfig] = None):
        """
        åˆå§‹åŒ–ç³»ç»Ÿ

        Args:
            config: ç³»ç»Ÿé…ç½®å‚æ•°
            memory_config: è®°å¿†ç³»ç»Ÿé…ç½®
            llm_config: å¤§è¯­è¨€æ¨¡å‹é…ç½®
            embedding_config: åµŒå…¥æ¨¡å‹é…ç½®
        """
        self.config = config or {}
        self.memory_config = memory_config
        self.llm_config = llm_config
        self.embedding_config = embedding_config

        # åˆå§‹åŒ–å„å±‚æ¨¡å—
        self._initialize_perception_layer()
        self._initialize_cognition_layer()
        self._initialize_behavior_layer()
        self._initialize_memory_layer()
        self._initialize_llm_layer()
        self._initialize_collaboration_layer()

        # ç³»ç»ŸçŠ¶æ€
        self.is_initialized = True
        self.session_history = []

    def _initialize_perception_layer(self):
        """åˆå§‹åŒ–æ„ŸçŸ¥å±‚"""
        self.user_modeler = DynamicUserModeler()
        self.context_analyzer = IntegratedContextAnalyzer()

    def _initialize_cognition_layer(self):
        """åˆå§‹åŒ–è®¤çŸ¥å±‚"""
        self.memory_activator = MultiLevelMemoryActivator()
        self.semantic_enhancer = IntegratedSemanticEnhancer()
        self.analogy_reasoner = IntegratedAnalogyReasoner()

    def _initialize_behavior_layer(self):
        """åˆå§‹åŒ–è¡Œä¸ºå±‚"""
        self.adaptive_output = IntegratedAdaptiveOutput()

    def _initialize_memory_layer(self):
        """åˆå§‹åŒ–è®°å¿†å±‚"""
        if self.memory_config:
            try:
                self.memory_system = MemorySystemFactory.create_memory_system(
                    self.memory_config)
                # å¼‚æ­¥åˆå§‹åŒ–å°†åœ¨é¦–æ¬¡ä½¿ç”¨æ—¶è¿›è¡Œ
                self._memory_initialized = False
            except Exception as e:
                print(f"è®°å¿†ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
                self.memory_system = None
                self._memory_initialized = False
        else:
            self.memory_system = None
            self._memory_initialized = False

    def _initialize_llm_layer(self):
        """åˆå§‹åŒ–LLMå±‚"""
        # åˆå§‹åŒ–å¤§è¯­è¨€æ¨¡å‹
        if self.llm_config:
            try:
                self.llm_provider = LLMProviderFactory.create_provider(
                    self.llm_config)
                self._llm_initialized = False
            except Exception as e:
                print(f"LLMåˆå§‹åŒ–å¤±è´¥: {e}")
                self.llm_provider = None
                self._llm_initialized = False
        else:
            self.llm_provider = None
            self._llm_initialized = False

        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        if self.embedding_config:
            try:
                self.embedding_provider = EmbeddingProviderFactory.create_provider(
                    self.embedding_config)
                self._embedding_initialized = False
            except Exception as e:
                print(f"åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
                self.embedding_provider = None
                self._embedding_initialized = False
        else:
            self.embedding_provider = None
            self._embedding_initialized = False

    def _initialize_collaboration_layer(self):
        """åˆå§‹åŒ–åä½œå±‚"""
        from ..collaboration import CollaborativeCoordinator

        # åä½œå±‚éœ€è¦LLMæ”¯æŒï¼Œå…ˆç¡®ä¿LLMå·²é…ç½®
        if self.llm_provider:
            try:
                self.collaborative_coordinator = CollaborativeCoordinator(
                    llm_provider=self.llm_provider,
                    memory_system=self.memory_system
                )
                self._collaboration_initialized = True
                print("åä½œå±‚åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"åä½œå±‚åˆå§‹åŒ–å¤±è´¥: {e}")
                self.collaborative_coordinator = None
                self._collaboration_initialized = False
        else:
            print("åä½œå±‚éœ€è¦LLMæ”¯æŒï¼Œè·³è¿‡åˆå§‹åŒ–")
            self.collaborative_coordinator = None
            self._collaboration_initialized = False

    async def process_query(self, query: str, user_id: str, context: Optional[Dict[str, Any]] = None) -> SystemResponse:
        """
        å¤„ç†ç”¨æˆ·æŸ¥è¯¢ - ç³»ç»Ÿä¸»è¦æ¥å£

        å¯¹åº”è®¾è®¡æ–‡æ¡£çš„å®Œæ•´å¤„ç†æµç¨‹ï¼š
        æ„ŸçŸ¥å±‚ -> è®¤çŸ¥å±‚ -> è¡Œä¸ºå±‚ -> è¾“å‡º

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            user_id: ç”¨æˆ·ID
            context: é¢å¤–ä¸Šä¸‹æ–‡ä¿¡æ¯

        Returns:
            SystemResponse: å®Œæ•´çš„ç³»ç»Ÿå“åº”
        """
        if not self.is_initialized:
            raise RuntimeError("System not initialized")

        context = context or {}
        processing_metadata = {
            'query': query,
            'user_id': user_id,
            'timestamp': self._get_timestamp(),
            'processing_steps': []
        }

        try:
            # ==================== ç³»ç»Ÿåˆå§‹åŒ– ====================
            # åˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ
            if self.memory_system and not self._memory_initialized:
                await self.memory_system.initialize()
                self._memory_initialized = True
                processing_metadata['processing_steps'].append(
                    'memory_initialized')

            # åˆå§‹åŒ–LLM
            if self.llm_provider and not self._llm_initialized:
                await self.llm_provider.initialize()
                self._llm_initialized = True
                processing_metadata['processing_steps'].append(
                    'llm_initialized')

            # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
            if self.embedding_provider and not self._embedding_initialized:
                await self.embedding_provider.initialize()
                self._embedding_initialized = True
                processing_metadata['processing_steps'].append(
                    'embedding_initialized')

            # ==================== æ„ŸçŸ¥å±‚å¤„ç† ====================
            processing_metadata['processing_steps'].append('perception_start')

            # 1.1 ç”¨æˆ·å»ºæ¨¡ - ç»“åˆè®°å¿†ç³»ç»Ÿçš„ç”¨æˆ·æ•°æ®
            user_profile = self.user_modeler.get_user_profile(user_id)
            if not user_profile:
                # ä¸ºæ–°ç”¨æˆ·åˆ›å»ºåˆå§‹ç”»åƒ
                initial_data = self._extract_initial_user_data(query, context)

                # ä»è®°å¿†ç³»ç»Ÿè·å–ç”¨æˆ·å†å²æ•°æ®
                if self.memory_system:
                    user_context = create_user_context(user_id)
                    user_memories = await self.memory_system.get_user_memories(user_context, limit=50)
                    initial_data['memory_context'] = user_memories.memories

                user_profile = self.user_modeler.create_user_profile(
                    user_id, initial_data)

            # 1.2 æƒ…å¢ƒåˆ†æ
            enhanced_context = {**context, 'user_profile': user_profile}
            context_profile = self.context_analyzer.analyze_context(
                query, enhanced_context)

            processing_metadata['processing_steps'].append(
                'perception_complete')

            # ==================== è®¤çŸ¥å±‚å¤„ç† ====================
            processing_metadata['processing_steps'].append('cognition_start')

            # 2.0 è®°å¿†æ£€ç´¢ - ä»å¤–éƒ¨è®°å¿†ç³»ç»Ÿè·å–ç›¸å…³è®°å¿†
            relevant_memories = []
            if self.memory_system:
                user_context = create_user_context(
                    user_id, context.get('session_id'))
                memory_query = MemoryQuery(
                    query=query,
                    user_context=user_context,
                    limit=20,
                    similarity_threshold=0.6
                )
                memory_result = await self.memory_system.search_memories(memory_query)
                relevant_memories = memory_result.memories
                processing_metadata['processing_steps'].append(
                    'memory_retrieved')

            # 2.1 å¤šå±‚æ¬¡è®°å¿†æ¿€æ´» - æ•´åˆå¤–éƒ¨è®°å¿†å’Œå†…éƒ¨æ¿€æ´»
            cognitive_context = {
                'user_profile': user_profile,
                'context_profile': context_profile,
                'external_memories': relevant_memories,
                **enhanced_context
            }
            activation_results = self.memory_activator.activate_comprehensive_memories(
                query, cognitive_context)

            # 2.2 è¯­ä¹‰è¡¥å……
            all_fragments = []
            for result in activation_results:
                all_fragments.extend(result.fragments)

            # æ·»åŠ å¤–éƒ¨è®°å¿†ä½œä¸ºç‰‡æ®µ
            for memory in relevant_memories:
                # è½¬æ¢è®°å¿†ä¸ºè®°å¿†ç‰‡æ®µæ ¼å¼
                fragment = self._convert_memory_to_fragment(memory)
                if fragment:
                    all_fragments.append(fragment)

            semantic_result = self.semantic_enhancer.enhance_comprehensive_semantics(
                all_fragments, cognitive_context
            )

            # 2.3 ç±»æ¯”æ¨ç†
            analogy_result = self.analogy_reasoner.comprehensive_analogy_reasoning(
                query, cognitive_context)

            processing_metadata['processing_steps'].append(
                'cognition_complete')

            # ==================== è¡Œä¸ºå±‚å¤„ç† ====================
            processing_metadata['processing_steps'].append('behavior_start')

            # 3.1 ä¸ªæ€§åŒ–é€‚é…
            adapted_output = self.adaptive_output.comprehensive_adaptation(
                semantic_result.enhanced_fragments,
                user_profile,
                cognitive_context
            )

            # 3.2 LLMç”Ÿæˆå“åº” (å¦‚æœé…ç½®äº†LLM)
            if self.llm_provider and self._llm_initialized:
                try:
                    # æ„å»ºå¯¹è¯æ¶ˆæ¯
                    messages = await self._build_chat_messages(
                        query, user_profile, context_profile,
                        relevant_memories, semantic_result, adapted_output
                    )

                    # è°ƒç”¨LLMç”Ÿæˆå“åº”
                    llm_response = await self.llm_provider.chat(messages)

                    # æ›´æ–°é€‚é…è¾“å‡º
                    adapted_output.content = llm_response.content
                    adapted_output.metadata = adapted_output.metadata or {}
                    adapted_output.metadata.update({
                        "llm_generated": True,
                        "llm_model": self.llm_config.model_name,
                        "llm_usage": llm_response.usage
                    })

                    processing_metadata['processing_steps'].append(
                        'llm_generated')

                except Exception as e:
                    print(f"LLMç”Ÿæˆå“åº”å¤±è´¥: {e}")
                    processing_metadata['llm_error'] = str(e)

            processing_metadata['processing_steps'].append('behavior_complete')

            # ==================== åä½œå±‚å¤„ç† ====================
            collaboration_result = None
            if self._collaboration_initialized and self.collaborative_coordinator:
                try:
                    from ..collaboration.interfaces import CollaborationContext

                    # æ„å»ºåä½œä¸Šä¸‹æ–‡
                    collaboration_context = CollaborationContext(
                        user_id=user_id,
                        session_id=context.get(
                            'session_id', f'session_{user_id}'),
                        interaction_history=[],
                        user_cognitive_profile=None,
                        collaboration_preferences=context.get(
                            'collaboration_preferences', {}),
                        current_task_context=context
                    )

                    # ç¼–æ’åä½œè¿‡ç¨‹
                    collaboration_result = await self.collaborative_coordinator.orchestrate_collaboration(
                        query, collaboration_context
                    )
                    processing_metadata['processing_steps'].append(
                        'collaboration_complete')

                    # å¦‚æœåä½œå±‚äº§ç”Ÿäº†æ–°çš„æ´å¯Ÿï¼Œå¯ä»¥å½±å“æœ€ç»ˆå†…å®¹
                    if collaboration_result and not collaboration_result.get('error'):
                        # å°†åä½œæ´å¯Ÿæ·»åŠ åˆ°å¤„ç†å…ƒæ•°æ®
                        processing_metadata['collaboration_insights'] = collaboration_result.get(
                            'collaboration_insights')
                        processing_metadata['perspectives_generated'] = len(
                            collaboration_result.get(
                                'perspectives', {}).get('perspectives', [])
                        )
                        processing_metadata['challenges_generated'] = len(
                            collaboration_result.get(
                                'challenges', {}).get('challenges', [])
                        )

                except Exception as e:
                    print(f"åä½œå±‚å¤„ç†å¤±è´¥: {e}")
                    processing_metadata['collaboration_error'] = str(e)
            else:
                processing_metadata['processing_steps'].append(
                    'collaboration_skipped')

            # ==================== å“åº”æ„å»º ====================
            response = SystemResponse(
                content=adapted_output.content,
                user_profile=user_profile,
                context_profile=context_profile,
                activated_memories=activation_results,
                semantic_enhancement=semantic_result,
                analogy_reasoning=analogy_result,
                adaptation_info=adapted_output,
                processing_metadata=processing_metadata,
                collaboration_result=collaboration_result
            )

            # ==================== è®°å¿†ä¿å­˜ ====================
            if self.memory_system:
                try:
                    # ä¿å­˜ç”¨æˆ·æŸ¥è¯¢
                    user_context = create_user_context(
                        user_id, context.get('session_id'))

                    query_memory = create_memory_entry(
                        content=f"ç”¨æˆ·æŸ¥è¯¢: {query}",
                        memory_type=MemoryType.EPISODIC,
                        user_context=user_context,
                        metadata={
                            "type": "user_query",
                            "context_profile": context_profile.__dict__ if context_profile else {},
                            "processing_metadata": processing_metadata
                        }
                    )
                    await self.memory_system.add_memory(query_memory)

                    # ä¿å­˜ç³»ç»Ÿå“åº”
                    response_memory = create_memory_entry(
                        content=f"ç³»ç»Ÿå“åº”: {response.content}",
                        memory_type=MemoryType.EPISODIC,
                        user_context=user_context,
                        metadata={
                            "type": "system_response",
                            "adaptation_info": adapted_output.__dict__,
                            "activated_memories_count": len(relevant_memories)
                        }
                    )
                    await self.memory_system.add_memory(response_memory)

                    # ä¿å­˜é‡è¦çš„è¯­ä¹‰å¢å¼ºç»“æœ
                    if semantic_result and hasattr(semantic_result, 'enhanced_fragments'):
                        # åªä¿å­˜å‰5ä¸ªé‡è¦ç‰‡æ®µ
                        for fragment in semantic_result.enhanced_fragments[:5]:
                            semantic_memory = create_memory_entry(
                                content=f"è¯­ä¹‰å¢å¼º: {fragment.content if hasattr(fragment, 'content') else str(fragment)}",
                                memory_type=MemoryType.SEMANTIC,
                                user_context=user_context,
                                metadata={
                                    "type": "semantic_enhancement",
                                    "query": query,
                                    "confidence": getattr(fragment, 'confidence', 0.8)
                                }
                            )
                            await self.memory_system.add_memory(semantic_memory)

                    processing_metadata['processing_steps'].append(
                        'memory_saved')

                except Exception as e:
                    print(f"ä¿å­˜è®°å¿†å¤±è´¥: {e}")
                    processing_metadata['memory_save_error'] = str(e)

            # æ›´æ–°ç”¨æˆ·ç”»åƒ
            self._update_user_profile(user_id, query, response)

            # è®°å½•ä¼šè¯å†å²
            self.session_history.append({
                'query': query,
                'response': response,
                'timestamp': processing_metadata['timestamp']
            })

            return response

        except Exception as e:
            processing_metadata['error'] = str(e)
            processing_metadata['processing_steps'].append('error')
            raise RuntimeError(f"System processing failed: {e}")

    async def process_query_stream(self, query: str, user_id: str,
                                   context: Optional[Dict[str, Any]] = None,
                                   yield_steps: bool = True):
        """
        æµå¼å¤„ç†ç”¨æˆ·æŸ¥è¯¢ - æä¾›å®æ—¶å¤„ç†åé¦ˆ

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            user_id: ç”¨æˆ·ID
            context: é¢å¤–ä¸Šä¸‹æ–‡ä¿¡æ¯
            yield_steps: æ˜¯å¦è¾“å‡ºå¤„ç†æ­¥éª¤

        Yields:
            str: å¤„ç†æ­¥éª¤ä¿¡æ¯æˆ–AIç”Ÿæˆçš„å†…å®¹å—
        """
        if not self.is_initialized:
            raise RuntimeError("System not initialized")

        context = context or {}
        processing_metadata = {
            'query': query,
            'user_id': user_id,
            'timestamp': self._get_timestamp(),
            'processing_steps': []
        }

        try:
            # ==================== ç³»ç»Ÿåˆå§‹åŒ– ====================
            if yield_steps:
                yield "ğŸ”§ åˆå§‹åŒ–ç³»ç»Ÿç»„ä»¶..."

            # åˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ
            if self.memory_system and not self._memory_initialized:
                await self.memory_system.initialize()
                self._memory_initialized = True
                processing_metadata['processing_steps'].append(
                    'memory_initialized')

            # åˆå§‹åŒ–LLM
            if self.llm_provider and not self._llm_initialized:
                await self.llm_provider.initialize()
                self._llm_initialized = True
                processing_metadata['processing_steps'].append(
                    'llm_initialized')

            # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
            if self.embedding_provider and not self._embedding_initialized:
                await self.embedding_provider.initialize()
                self._embedding_initialized = True
                processing_metadata['processing_steps'].append(
                    'embedding_initialized')

            # ==================== æ„ŸçŸ¥å±‚å¤„ç† ====================
            if yield_steps:
                yield "ğŸ§  æ„ŸçŸ¥å±‚ï¼šåˆ†æç”¨æˆ·æŸ¥è¯¢å’Œä¸Šä¸‹æ–‡..."

            processing_metadata['processing_steps'].append('perception_start')

            # ç”¨æˆ·å»ºæ¨¡
            user_profile = self.user_modeler.get_user_profile(user_id)
            if not user_profile:
                initial_data = self._extract_initial_user_data(query, context)

                # ä»è®°å¿†ç³»ç»Ÿè·å–ç”¨æˆ·å†å²æ•°æ®
                if self.memory_system:
                    user_context = create_user_context(user_id)
                    user_memories = await self.memory_system.get_user_memories(user_context, limit=50)
                    initial_data['memory_context'] = user_memories.memories

                user_profile = self.user_modeler.create_user_profile(
                    user_id, initial_data)

            # æƒ…å¢ƒåˆ†æ
            enhanced_context = {**context, 'user_profile': user_profile}
            context_profile = self.context_analyzer.analyze_context(
                query, enhanced_context)

            processing_metadata['processing_steps'].append(
                'perception_complete')

            # ==================== è®¤çŸ¥å±‚å¤„ç† ====================
            if yield_steps:
                yield "ğŸ’¾ è®¤çŸ¥å±‚ï¼šæ£€ç´¢å’Œå¤„ç†ç›¸å…³è®°å¿†..."

            processing_metadata['processing_steps'].append('cognition_start')

            # è®°å¿†æ£€ç´¢
            relevant_memories = []
            if self.memory_system:
                user_context = create_user_context(
                    user_id, context.get('session_id'))
                memory_query = MemoryQuery(
                    query=query,
                    user_context=user_context,
                    limit=20,
                    similarity_threshold=0.6
                )
                memory_result = await self.memory_system.search_memories(memory_query)
                relevant_memories = memory_result.memories
                processing_metadata['processing_steps'].append(
                    'memory_retrieved')

            # å¤šå±‚æ¬¡è®°å¿†æ¿€æ´»
            cognitive_context = {
                'user_profile': user_profile,
                'context_profile': context_profile,
                'external_memories': relevant_memories,
                **enhanced_context
            }
            activation_results = self.memory_activator.activate_comprehensive_memories(
                query, cognitive_context)

            # è¯­ä¹‰è¡¥å……
            all_fragments = []
            for result in activation_results:
                all_fragments.extend(result.fragments)

            # æ·»åŠ å¤–éƒ¨è®°å¿†ä½œä¸ºç‰‡æ®µ
            for memory in relevant_memories:
                fragment = self._convert_memory_to_fragment(memory)
                if fragment:
                    all_fragments.append(fragment)

            semantic_result = self.semantic_enhancer.enhance_comprehensive_semantics(
                all_fragments, cognitive_context
            )

            # ç±»æ¯”æ¨ç†
            analogy_result = self.analogy_reasoner.comprehensive_analogy_reasoning(
                query, cognitive_context)

            processing_metadata['processing_steps'].append(
                'cognition_complete')

            # ==================== è¡Œä¸ºå±‚å¤„ç† ====================
            if yield_steps:
                yield "âš™ï¸ è¡Œä¸ºå±‚ï¼šä¸ªæ€§åŒ–é€‚é…å’Œå†…å®¹ç”Ÿæˆ..."

            processing_metadata['processing_steps'].append('behavior_start')

            # ä¸ªæ€§åŒ–é€‚é…
            adapted_output = self.adaptive_output.comprehensive_adaptation(
                semantic_result.enhanced_fragments,
                user_profile,
                cognitive_context
            )

            # LLMæµå¼ç”Ÿæˆå“åº”
            if self.llm_provider and self._llm_initialized:
                try:
                    if yield_steps:
                        yield "ğŸ¤– ç”Ÿæˆå±‚ï¼šå¼€å§‹AIå†…å®¹ç”Ÿæˆ..."

                    # æ„å»ºå¯¹è¯æ¶ˆæ¯
                    messages = await self._build_chat_messages(
                        query, user_profile, context_profile,
                        relevant_memories, semantic_result, adapted_output
                    )

                    # æµå¼è°ƒç”¨LLMç”Ÿæˆå“åº”
                    full_response = ""
                    async for chunk in self.llm_provider.chat_stream(messages):
                        full_response += chunk
                        yield chunk  # å®æ—¶è¾“å‡ºå†…å®¹å—

                    # æ›´æ–°é€‚é…è¾“å‡º
                    adapted_output.content = full_response
                    adapted_output.metadata = adapted_output.metadata or {}
                    adapted_output.metadata.update({
                        "llm_generated": True,
                        "llm_model": self.llm_config.model_name,
                        "streaming": True
                    })

                    processing_metadata['processing_steps'].append(
                        'llm_streamed')

                except Exception as e:
                    if yield_steps:
                        yield f"\nâŒ LLMç”Ÿæˆå¤±è´¥: {e}\n"
                    processing_metadata['llm_error'] = str(e)
            else:
                # å›é€€åˆ°é€‚é…å†…å®¹
                yield adapted_output.content

            processing_metadata['processing_steps'].append('behavior_complete')

            # ==================== åç»­å¤„ç† ====================
            if yield_steps:
                yield "\nğŸ¯ å¤„ç†å®Œæˆï¼Œä¿å­˜è®°å¿†å’Œæ›´æ–°ç”¨æˆ·ç”»åƒ..."

            # å¼‚æ­¥ä¿å­˜è®°å¿†ï¼ˆä¸é˜»å¡æµå¼è¾“å‡ºï¼‰
            if self.memory_system:
                asyncio.create_task(self._async_save_memories(
                    query, adapted_output.content, user_id, context,
                    processing_metadata, user_profile, context_profile,
                    semantic_result, relevant_memories
                ))

            # å¼‚æ­¥æ›´æ–°ç”¨æˆ·ç”»åƒ
            asyncio.create_task(self._async_update_user_profile(
                user_id, query, adapted_output, processing_metadata
            ))

        except Exception as e:
            processing_metadata['error'] = str(e)
            processing_metadata['processing_steps'].append('error')
            if yield_steps:
                yield f"\nâŒ ç³»ç»Ÿå¤„ç†å¤±è´¥: {e}\n"

    async def _async_save_memories(self, query: str, response_content: str,
                                   user_id: str, context: Dict[str, Any],
                                   processing_metadata: Dict[str, Any],
                                   user_profile, context_profile,
                                   semantic_result, relevant_memories):
        """å¼‚æ­¥ä¿å­˜è®°å¿†ï¼Œä¸é˜»å¡ä¸»æµç¨‹"""
        try:
            user_context = create_user_context(
                user_id, context.get('session_id'))

            # ä¿å­˜ç”¨æˆ·æŸ¥è¯¢
            query_memory = create_memory_entry(
                content=f"ç”¨æˆ·æŸ¥è¯¢: {query}",
                memory_type=MemoryType.EPISODIC,
                user_context=user_context,
                metadata={
                    "type": "user_query",
                    "context_profile": context_profile.to_dict() if context_profile and hasattr(context_profile, 'to_dict') else {},
                    "processing_metadata": processing_metadata
                }
            )
            await self.memory_system.add_memory(query_memory)

            # ä¿å­˜ç³»ç»Ÿå“åº”
            response_memory = create_memory_entry(
                content=f"ç³»ç»Ÿå“åº”: {response_content}",
                memory_type=MemoryType.EPISODIC,
                user_context=user_context,
                metadata={
                    "type": "system_response",
                    "streaming": True,
                    "activated_memories_count": len(relevant_memories)
                }
            )
            await self.memory_system.add_memory(response_memory)

            # ä¿å­˜é‡è¦çš„è¯­ä¹‰å¢å¼ºç»“æœ
            if semantic_result and hasattr(semantic_result, 'enhanced_fragments'):
                # åªä¿å­˜å‰3ä¸ªé‡è¦ç‰‡æ®µ
                for fragment in semantic_result.enhanced_fragments[:3]:
                    semantic_memory = create_memory_entry(
                        content=f"è¯­ä¹‰å¢å¼º: {fragment.content if hasattr(fragment, 'content') else str(fragment)}",
                        memory_type=MemoryType.SEMANTIC,
                        user_context=user_context,
                        metadata={
                            "type": "semantic_enhancement",
                            "query": query,
                            "confidence": getattr(fragment, 'confidence', 0.8)
                        }
                    )
                    await self.memory_system.add_memory(semantic_memory)

        except Exception as e:
            logger.error(f"å¼‚æ­¥ä¿å­˜è®°å¿†å¤±è´¥: {e}")

    async def _async_update_user_profile(self, user_id: str, query: str,
                                         adapted_output, processing_metadata: Dict[str, Any]):
        """å¼‚æ­¥æ›´æ–°ç”¨æˆ·ç”»åƒï¼Œä¸é˜»å¡ä¸»æµç¨‹"""
        try:
            interaction_data = {
                'query': query,
                'response_quality': self._assess_response_quality_from_output(adapted_output),
                'cognitive_load': adapted_output.cognitive_load,
                'processing_time': self._calculate_processing_time(processing_metadata),
                'streaming': True
            }

            self.user_modeler.update_user_profile(user_id, interaction_data)

        except Exception as e:
            logger.error(f"å¼‚æ­¥æ›´æ–°ç”¨æˆ·ç”»åƒå¤±è´¥: {e}")

    def _assess_response_quality_from_output(self, adapted_output) -> float:
        """ä»é€‚é…è¾“å‡ºè¯„ä¼°å“åº”è´¨é‡"""
        try:
            # åŸºäºå†…å®¹é•¿åº¦ã€é€‚é…ç½®ä¿¡åº¦ç­‰è¯„ä¼°
            content_length = len(
                adapted_output.content) if adapted_output.content else 0
            adaptation_confidence = adapted_output.adaptation_confidence

            # ç®€å•çš„è´¨é‡è¯„ä¼°ç®—æ³•
            length_score = min(1.0, content_length / 500)  # 500å­—ç¬¦ä¸ºæ»¡åˆ†
            quality_score = (length_score * 0.6 + adaptation_confidence * 0.4)

            return max(0.1, min(1.0, quality_score))
        except:
            return 0.7  # é»˜è®¤è´¨é‡åˆ†æ•°

    def _extract_initial_user_data(self, query: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """ä»æŸ¥è¯¢å’Œä¸Šä¸‹æ–‡ä¸­æå–åˆå§‹ç”¨æˆ·æ•°æ®"""
        # TODO: å®ç°æ›´æ™ºèƒ½çš„ç”¨æˆ·æ•°æ®æå–
        return {
            'initial_query': query,
            'context': context,
            'inferred_domains': self._infer_domains_from_query(query),
            'cognitive_style': 'linear'  # é»˜è®¤å€¼
        }

    def _infer_domains_from_query(self, query: str) -> List[str]:
        """ä»æŸ¥è¯¢ä¸­æ¨æ–­æ¶‰åŠçš„é¢†åŸŸ"""
        # TODO: å®ç°åŸºäºNLPçš„é¢†åŸŸè¯†åˆ«
        return ['general']

    def _update_user_profile(self, user_id: str, query: str, response: SystemResponse):
        """æ›´æ–°ç”¨æˆ·ç”»åƒ"""
        interaction_data = {
            'query': query,
            'response_quality': self._assess_response_quality(response),
            'cognitive_load': response.adaptation_info.cognitive_load,
            'processing_time': self._calculate_processing_time(response.processing_metadata),
            'domains': self._extract_domains_from_response(response)
        }

        self.user_modeler.update_user_profile(user_id, interaction_data)

    def _assess_response_quality(self, response: SystemResponse) -> float:
        """è¯„ä¼°å“åº”è´¨é‡"""
        # TODO: å®ç°å“åº”è´¨é‡è¯„ä¼°ç®—æ³•
        return 0.8

    def _calculate_processing_time(self, metadata: Dict[str, Any]) -> float:
        """è®¡ç®—å¤„ç†æ—¶é—´"""
        # TODO: å®ç°å¤„ç†æ—¶é—´è®¡ç®—
        return 1.0

    def _extract_domains_from_response(self, response: SystemResponse) -> List[str]:
        """ä»å“åº”ä¸­æå–æ¶‰åŠçš„é¢†åŸŸ"""
        # TODO: åˆ†æå“åº”å†…å®¹æ¶‰åŠçš„çŸ¥è¯†é¢†åŸŸ
        return ['general']

    def _get_timestamp(self) -> str:
        """è·å–æ—¶é—´æˆ³"""
        import datetime
        return datetime.datetime.now().isoformat()

    def _convert_memory_to_fragment(self, memory: MemoryEntry):
        """å°†å¤–éƒ¨è®°å¿†è½¬æ¢ä¸ºå†…éƒ¨è®°å¿†ç‰‡æ®µæ ¼å¼"""
        try:
            # åˆ›å»ºç®€åŒ–çš„è®°å¿†ç‰‡æ®µ
            # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„MemoryFragmentç»“æ„è¿›è¡Œè°ƒæ•´
            fragment = type('MemoryFragment', (), {
                'content': memory.content,
                'confidence': memory.confidence,
                'source': 'external_memory',
                'memory_type': memory.memory_type.value,
                'timestamp': memory.timestamp,
                'metadata': memory.metadata or {}
            })()

            return fragment

        except Exception as e:
            print(f"è½¬æ¢è®°å¿†ç‰‡æ®µå¤±è´¥: {e}")
            return None

    async def _build_chat_messages(self, query: str, user_profile, context_profile,
                                   relevant_memories: List[MemoryEntry],
                                   semantic_result, adapted_output) -> List[ChatMessage]:
        """æ„å»ºLLMå¯¹è¯æ¶ˆæ¯"""
        messages = []

        # ç³»ç»Ÿæ¶ˆæ¯ï¼šè§’è‰²è®¾å®šå’Œè®¤çŸ¥æŒ‡å¯¼
        system_prompt = self._build_system_prompt(
            user_profile, context_profile)
        messages.append(create_chat_message("system", system_prompt))

        # å¦‚æœæœ‰ç›¸å…³è®°å¿†ï¼Œæ·»åŠ è®°å¿†ä¸Šä¸‹æ–‡
        if relevant_memories:
            memory_context = self._build_memory_context(relevant_memories)
            messages.append(create_chat_message(
                "system", f"ç›¸å…³è®°å¿†ä¿¡æ¯:\n{memory_context}"))

        # å¦‚æœæœ‰è¯­ä¹‰å¢å¼ºç»“æœï¼Œæ·»åŠ å¢å¼ºä¿¡æ¯
        if semantic_result and hasattr(semantic_result, 'enhanced_fragments'):
            enhanced_context = self._build_enhanced_context(semantic_result)
            messages.append(create_chat_message(
                "system", f"è¯­ä¹‰å¢å¼ºä¿¡æ¯:\n{enhanced_context}"))

        # ç”¨æˆ·æŸ¥è¯¢
        messages.append(create_chat_message("user", query))

        return messages

    def _build_system_prompt(self, user_profile, context_profile) -> str:
        """æ„å»ºç³»ç»Ÿæç¤º"""
        prompt_parts = [
            "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½çš„è®°å¿†-è®¤çŸ¥ååŒç³»ç»ŸåŠ©æ‰‹ã€‚",
            f"ç”¨æˆ·è®¤çŸ¥ç‰¹å¾: æ€ç»´æ¨¡å¼={user_profile.cognitive.thinking_mode.value}, è®¤çŸ¥å¤æ‚åº¦={user_profile.cognitive.cognitive_complexity}",
            f"ä»»åŠ¡ç‰¹å¾: ç±»å‹={context_profile.task_characteristics.task_type.value}, å¼€æ”¾æ€§={context_profile.task_characteristics.openness_level}",
            "è¯·åŸºäºç”¨æˆ·çš„è®¤çŸ¥ç‰¹å¾å’Œä»»åŠ¡éœ€æ±‚ï¼Œæä¾›ä¸ªæ€§åŒ–çš„å›ç­”ã€‚"
        ]

        # æ ¹æ®ä¸åŒç³»ç»Ÿé…ç½®æ·»åŠ ç‰¹å®šæŒ‡å¯¼
        if 'educational' in str(self.config.get('system_type', '')):
            prompt_parts.append("è¯·é‡‡ç”¨æ•™è‚²å¯¼å‘çš„è§£é‡Šæ–¹å¼ï¼Œæ³¨é‡å¾ªåºæ¸è¿›å’Œæ¦‚å¿µå»ºæ„ã€‚")
        elif 'research' in str(self.config.get('system_type', '')):
            prompt_parts.append("è¯·æä¾›æ·±åº¦åˆ†æå’Œè·¨é¢†åŸŸå…³è”ï¼Œæ”¯æŒå­¦æœ¯ç ”ç©¶éœ€æ±‚ã€‚")

        return "\n".join(prompt_parts)

    def _build_memory_context(self, memories: List[MemoryEntry]) -> str:
        """æ„å»ºè®°å¿†ä¸Šä¸‹æ–‡"""
        memory_texts = []
        for i, memory in enumerate(memories[:5], 1):  # åªå–å‰5ä¸ªæœ€ç›¸å…³çš„è®°å¿†
            memory_texts.append(
                f"{i}. [{memory.memory_type.value}] {memory.content[:200]}...")

        return "\n".join(memory_texts)

    def _build_enhanced_context(self, semantic_result) -> str:
        """æ„å»ºè¯­ä¹‰å¢å¼ºä¸Šä¸‹æ–‡"""
        if not hasattr(semantic_result, 'enhanced_fragments'):
            return ""

        enhanced_texts = []
        for i, fragment in enumerate(semantic_result.enhanced_fragments[:3], 1):
            if hasattr(fragment, 'content'):
                enhanced_texts.append(f"{i}. {fragment.content[:150]}...")
            else:
                enhanced_texts.append(f"{i}. {str(fragment)[:150]}...")

        return "\n".join(enhanced_texts)

    def get_system_status(self) -> Dict[str, Any]:
        """è·å–ç³»ç»ŸçŠ¶æ€"""
        status = {
            'initialized': self.is_initialized,
            'session_count': len(self.session_history),
            'user_count': len(self.user_modeler.user_profiles),
            'config': self.config
        }

        # æ·»åŠ è®°å¿†ç³»ç»ŸçŠ¶æ€
        if self.memory_system:
            status['memory_system'] = self.memory_system.get_system_info()
            status['memory_initialized'] = self._memory_initialized
        else:
            status['memory_system'] = None
            status['memory_initialized'] = False

        # æ·»åŠ LLMçŠ¶æ€
        if self.llm_provider:
            status['llm_provider'] = self.llm_provider.get_model_info()
            status['llm_initialized'] = self._llm_initialized
        else:
            status['llm_provider'] = None
            status['llm_initialized'] = False

        # æ·»åŠ åµŒå…¥æ¨¡å‹çŠ¶æ€
        if self.embedding_provider:
            status['embedding_provider'] = self.embedding_provider.get_model_info()
            status['embedding_initialized'] = self._embedding_initialized
        else:
            status['embedding_provider'] = None
            status['embedding_initialized'] = False

        return status

    def reset_session(self):
        """é‡ç½®ä¼šè¯"""
        self.session_history.clear()

    def export_user_profile(self, user_id: str) -> Optional[Dict[str, Any]]:
        """å¯¼å‡ºç”¨æˆ·ç”»åƒ"""
        profile = self.user_modeler.get_user_profile(user_id)
        if profile:
            return {
                'user_id': profile.user_id,
                'cognitive': {
                    'thinking_mode': profile.cognitive.thinking_mode.value,
                    'cognitive_complexity': profile.cognitive.cognitive_complexity,
                    'abstraction_level': profile.cognitive.abstraction_level,
                    'creativity_tendency': profile.cognitive.creativity_tendency
                },
                'knowledge': {
                    'core_domains': profile.knowledge.core_domains,
                    'edge_domains': profile.knowledge.edge_domains,
                    'knowledge_depth': profile.knowledge.knowledge_depth
                },
                'interaction': {
                    'cognitive_style': profile.interaction.cognitive_style.value,
                    'information_density_preference': profile.interaction.information_density_preference,
                    'processing_speed': profile.interaction.processing_speed
                },
                'created_at': profile.created_at,
                'updated_at': profile.updated_at
            }
        return None

    async def cleanup(self):
        """æ¸…ç†ç³»ç»Ÿèµ„æº"""
        try:
            # æ¸…ç†LLMæä¾›å•†è¿æ¥
            if self.llm_provider and hasattr(self.llm_provider, 'session'):
                if self.llm_provider.session:
                    await self.llm_provider.session.close()

            # æ¸…ç†åµŒå…¥æä¾›å•†è¿æ¥
            if self.embedding_provider and hasattr(self.embedding_provider, 'session'):
                if self.embedding_provider.session:
                    await self.embedding_provider.session.close()

            # æ¸…ç†è®°å¿†ç³»ç»Ÿè¿æ¥
            if self.memory_system and hasattr(self.memory_system, 'cleanup'):
                await self.memory_system.cleanup()

        except Exception as e:
            print(f"æ¸…ç†ç³»ç»Ÿèµ„æºæ—¶å‡ºé”™: {e}")

    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        await self.cleanup()


class SystemFactory:
    """ç³»ç»Ÿå·¥å‚ç±» - ç”¨äºåˆ›å»ºå’Œé…ç½®ç³»ç»Ÿå®ä¾‹"""

    @staticmethod
    def create_default_system(memory_config: Optional[MemorySystemConfig] = None,
                              llm_config: Optional[ModelConfig] = None,
                              embedding_config: Optional[ModelConfig] = None) -> MemoryCognitiveSystem:
        """åˆ›å»ºé»˜è®¤é…ç½®çš„ç³»ç»Ÿ"""
        default_config = {
            'memory_activation': {
                'surface_weight': 0.3,
                'deep_weight': 0.4,
                'meta_weight': 0.3
            },
            'semantic_enhancement': {
                'gap_identification_threshold': 0.5,
                'bridge_confidence_threshold': 0.6
            },
            'adaptive_output': {
                'default_density': 'medium',
                'default_granularity': 'meso'
            }
        }

        return MemoryCognitiveSystem(default_config, memory_config, llm_config, embedding_config)

    @staticmethod
    def create_system_from_config(config_path: str) -> MemoryCognitiveSystem:
        """ä»é…ç½®æ–‡ä»¶åˆ›å»ºç³»ç»Ÿ"""
        # TODO: å®ç°é…ç½®æ–‡ä»¶åŠ è½½
        return SystemFactory.create_default_system()

    @staticmethod
    def create_educational_system(memory_config: Optional[MemorySystemConfig] = None,
                                  llm_config: Optional[ModelConfig] = None,
                                  embedding_config: Optional[ModelConfig] = None) -> MemoryCognitiveSystem:
        """åˆ›å»ºæ•™è‚²åœºæ™¯ç‰¹åŒ–çš„ç³»ç»Ÿ"""
        educational_config = {
            'memory_activation': {
                'surface_weight': 0.2,
                'deep_weight': 0.5,
                'meta_weight': 0.3
            },
            'semantic_enhancement': {
                'gap_identification_threshold': 0.3,  # æ›´ä½é˜ˆå€¼ï¼Œæ›´ç§¯æè¡¥å……
                'bridge_confidence_threshold': 0.5
            },
            'adaptive_output': {
                'default_density': 'low',  # æ•™è‚²åœºæ™¯åå¥½ä½å¯†åº¦
                'default_granularity': 'macro'  # ä»å®è§‚å¼€å§‹
            },
            'collaboration': {
                'enable_dialectical_perspective': True,
                'enable_cognitive_challenge': True
            }
        }

        return MemoryCognitiveSystem(educational_config, memory_config, llm_config, embedding_config)

    @staticmethod
    def create_research_system(memory_config: Optional[MemorySystemConfig] = None,
                               llm_config: Optional[ModelConfig] = None,
                               embedding_config: Optional[ModelConfig] = None) -> MemoryCognitiveSystem:
        """åˆ›å»ºç ”ç©¶åœºæ™¯ç‰¹åŒ–çš„ç³»ç»Ÿ"""
        research_config = {
            'memory_activation': {
                'surface_weight': 0.2,
                'deep_weight': 0.6,
                'meta_weight': 0.2
            },
            'semantic_enhancement': {
                'gap_identification_threshold': 0.4,
                'bridge_confidence_threshold': 0.7
            },
            'adaptive_output': {
                'default_density': 'high',  # ç ”ç©¶åœºæ™¯éœ€è¦é«˜å¯†åº¦ä¿¡æ¯
                'default_granularity': 'micro'  # å…³æ³¨ç»†èŠ‚
            },
            'analogy_reasoning': {
                'enable_creative_associations': True,
                'cross_domain_weight': 0.8
            }
        }

        return MemoryCognitiveSystem(research_config, memory_config, llm_config, embedding_config)
